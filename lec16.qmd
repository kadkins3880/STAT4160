---
title: "Session 16"
---
Below is a complete lecture package for **Session 16 — Classical Baselines** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll train a **lags‑only linear regressor per ticker** and compare it to the **naive** and **seasonal‑naive** baselines from Session 15. You’ll also see a short, optional **ARIMA** demo and log results in a consistent schema for future comparison.

> **Educational use only — not trading advice.**
> Assumes your repo (e.g., `unified-stocks-teamX`) with `data/processed/returns.parquet` and `data/processed/features_v1.parquet` from Sessions 9–10. Cells include safe fallbacks if some files are missing.

---

## Session 16 — Classical baselines (75 min)

### Learning goals

By the end of class, students can:

1. Fit a **per‑ticker** lags‑only linear regressor to predict **next‑day log return** $r_{t+1}$.
2. Evaluate models with **MAE, sMAPE, MASE** using the **rolling‑origin splits** (with embargo) from Session 15.
3. Log results in a **consistent table schema** for per‑ticker and split‑level summaries.
4. Understand **ARIMA** at a glance and its common pitfalls (optional demo).

---

## Agenda (75 min)

* **(10 min)** Slides: where classical models fit; pitfalls with ARIMA; cross‑sectional regressors
* **(10 min)** Slides: results table schema & comparison to baselines
* **(35 min)** **In‑class lab**: train per‑ticker **Linear (lags‑only)** → evaluate across 2 splits → compare to naive/seasonal‑naive → log CSVs
* **(10 min)** Wrap‑up + homework brief
* **(10 min)** Buffer

---

## Slides / talking points

### Why “classical” now?

* Creates a **credible, strong baseline** against naive that’s still transparent.
* Supports **fast iteration** and helps you debug feature definitions before deep models.

### Lags‑only linear regressor

* **Features** at time $t$: `lag1`, `lag2`, `lag3` (i.e., past returns), optionally a few stable stats (`roll_std_20`, `zscore_20`).
* **Target**: `r_1d` (next‑day log return).
* Fit **per ticker** to avoid cross‑sectional leakage for now.

### ARIMA 60‑second pitfall tour

* Stationarity: **fit on returns**, not prices (unless differencing).
* Evaluation: **re‑fit only on train**; generate **one‑step‑ahead** forecasts on val, updating state **without peeking**.
* Over‑differencing & mis‑specified seasonal terms → bad bias.
* Computational cost grows with grid search; keep demo tiny.

### Results table schema (consistent across sessions)

* **Per‑split summary**:
  `split, train_range, val_range, model, macro_mae, macro_smape, macro_mase, micro_mae, micro_smape, micro_mase`
* **Per‑ticker metrics**:
  `split, ticker, n, model, mae, smape, mase`

---

## In‑class lab (35 min, Colab‑friendly)

> Run each block as its **own cell**. Update `REPO_NAME` if needed.

### 0) Setup & data (with fallbacks)

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME  = "unified-stocks-teamX"  # <- change if needed
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, numpy as np, pandas as pd
from pathlib import Path
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
for p in ["data/raw","data/processed","reports","models","scripts","tests"]:
    Path(p).mkdir(parents=True, exist_ok=True)
print("Working dir:", os.getcwd())

# Load returns; if missing, synthesize
rpath = Path("data/processed/returns.parquet")
if rpath.exists():
    returns = pd.read_parquet(rpath)
else:
    rng = np.random.default_rng(0)
    dates = pd.bdate_range("2022-01-03", periods=360)
    rows=[]
    for t in ["AAPL","MSFT","GOOGL","AMZN","NVDA"]:
        eps = rng.normal(0,0.012, size=len(dates)).astype("float32")
        adj = 100*np.exp(np.cumsum(eps))
        df = pd.DataFrame({
            "date": dates, "ticker": t,
            "adj_close": adj.astype("float32"),
            "log_return": np.r_[np.nan, np.diff(np.log(adj))].astype("float32")
        })
        df["r_1d"] = df["log_return"].shift(-1)
        df["weekday"] = df["date"].dt.weekday.astype("int8")
        df["month"]   = df["date"].dt.month.astype("int8")
        rows.append(df)
    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)
    returns["ticker"] = returns["ticker"].astype("category")
    returns.to_parquet(rpath, index=False)

# Load features_v1 or derive minimal lags from returns if missing
fpath = Path("data/processed/features_v1.parquet")
if fpath.exists():
    feats = pd.read_parquet(fpath)
else:
    # Minimal lags derived just from returns
    feats = returns.sort_values(["ticker","date"]).copy()
    for k in [1,2,3]:
        feats[f"lag{k}"] = feats.groupby("ticker")["log_return"].shift(k)
    feats = feats.dropna(subset=["lag1","lag2","lag3","r_1d"]).reset_index(drop=True)

# Harmonize
feats["date"] = pd.to_datetime(feats["date"])
feats["ticker"] = feats["ticker"].astype("category")
feats = feats.sort_values(["ticker","date"]).reset_index(drop=True)
feats.head()
```

### 1) Rolling‑origin date splits (reuse Session 15 logic)

```python
def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    splits=[]; i = train_min-1; n=len(u)
    while True:
        if i>=n: break
        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=n: break
        splits.append((a,b,u[vs],u[ve])); i+=step
    return splits

splits = make_rolling_origin_splits(feats["date"], 252, 63, 63, 5)
len(splits), splits[:2]
```

### 2) Metrics & baselines (from Session 15)

```python
def mae(y, yhat): 
    y = np.asarray(y); yhat = np.asarray(yhat); 
    return float(np.mean(np.abs(y - yhat)))

def smape(y, yhat, eps=1e-8):
    y = np.asarray(y); yhat = np.asarray(yhat)
    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))

def mase(y_true, y_pred, y_train_true, y_train_naive):
    scale = mae(y_train_true, y_train_naive) + 1e-12
    return float(mae(y_true, y_pred)/scale)

def add_baseline_preds(df: pd.DataFrame, seasonality:int=5) -> pd.DataFrame:
    out = df.copy()
    out["yhat_naive"] = out.groupby("ticker")["log_return"].transform(lambda s: s)
    out["yhat_s"] = out.groupby("ticker")["log_return"].transform(lambda s: s.shift(seasonality-1)) if seasonality>1 else out["yhat_naive"]
    return out
```

### 3) **Per‑ticker lags‑only LinearRegression** (fit only on each split’s TRAIN)

```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# Choose features (lags only for in-class lab)
XCOLS = [c for c in ["lag1","lag2","lag3"] if c in feats.columns]
assert XCOLS, "No lag features found. Ensure features_v1 or fallback creation ran."

def fit_predict_lin_lags(train_df, val_df):
    """Fit per-ticker pipeline(StandardScaler, LinearRegression) on TRAIN; predict on VAL."""
    preds=[]
    for tkr, tr in train_df.groupby("ticker"):
        va = val_df[val_df["ticker"]==tkr]
        if len(tr)==0 or len(va)==0: 
            continue
        pipe = Pipeline([("scaler", StandardScaler(with_mean=True, with_std=True)),
                         ("lr", LinearRegression())])
        pipe.fit(tr[XCOLS].values, tr["r_1d"].values)
        yhat = pipe.predict(va[XCOLS].values)
        out = va[["date","ticker","r_1d","log_return"]].copy()
        out["yhat_linlags"] = yhat.astype("float32")
        preds.append(out)
    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame(columns=["date","ticker","r_1d","log_return","yhat_linlags"])
```

### 4) Evaluate **across the first 2 splits**; compare to naive/seasonal‑naive

```python
seasonality = 5
feats_baseline = add_baseline_preds(feats, seasonality=seasonality)

def per_ticker_metrics(df_val_pred, df_train, method_col):
    rows=[]
    for tkr, gv in df_val_pred.groupby("ticker"):
        if method_col not in gv: 
            continue
        gv = gv.dropna(subset=["r_1d", method_col])
        if len(gv)==0: 
            continue
        # TRAIN scale for MASE
        gt = df_train[df_train["ticker"]==tkr].dropna(subset=["r_1d"])
        gt_naive = gt["log_return"] if "yhat_s" not in method_col else gt["log_return"].shift(seasonality-1)
        gt_naive = gt_naive.loc[gt.index]
        rows.append({
            "ticker": tkr,
            "n": int(len(gv)),
            "mae": mae(gv["r_1d"], gv[method_col]),
            "smape": smape(gv["r_1d"], gv[method_col]),
            "mase": mase(gv["r_1d"], gv[method_col], gt["r_1d"], gt_naive),
        })
    return pd.DataFrame(rows)

def summarize_split(feats_frame, sid, split, save_prefix="linlags"):
    a,b,c,d = split
    tr = feats_frame[(feats_frame["date"]>=a)&(feats_frame["date"]<=b)].copy()
    va = feats_frame[(feats_frame["date"]>=c)&(feats_frame["date"]<=d)].copy()
    # Predictions
    val_pred = fit_predict_lin_lags(tr, va)
    # Attach baseline preds on val slice
    va_base = add_baseline_preds(va, seasonality=seasonality)
    val_pred = val_pred.merge(va_base[["date","ticker","yhat_naive","yhat_s"]], on=["date","ticker"], how="left")

    # Per-ticker metrics
    pt_lin = per_ticker_metrics(val_pred, tr, "yhat_linlags"); pt_lin["model"] = "lin_lags"
    pt_nav = per_ticker_metrics(val_pred.rename(columns={"yhat_naive":"yhat_linlags"}), tr, "yhat_linlags"); pt_nav["model"]="naive"
    pt_sea = per_ticker_metrics(val_pred.rename(columns={"yhat_s":"yhat_linlags"}), tr, "yhat_linlags"); pt_sea["model"]=f"s{seasonality}"

    # Save per-ticker
    out_pt = pd.concat([pt_lin.assign(split=sid), pt_nav.assign(split=sid), pt_sea.assign(split=sid)], ignore_index=True)
    out_pt.to_csv(f"reports/{save_prefix}_per_ticker_split{sid}.csv", index=False)

    # Aggregate
    def agg(df):
        if df.empty: 
            return {"macro_mae":np.nan,"macro_smape":np.nan,"macro_mase":np.nan,"micro_mae":np.nan,"micro_smape":np.nan,"micro_mase":np.nan}
        macro = df[["mae","smape","mase"]].mean().to_dict()
        w = df["n"].to_numpy()
        micro = {"micro_mae": float(np.average(df["mae"], weights=w)),
                 "micro_smape": float(np.average(df["smape"], weights=w)),
                 "micro_mase": float(np.average(df["mase"], weights=w))}
        return {f"macro_{k}":float(v) for k,v in macro.items()} | micro

    rows=[]
    for name, pt in [("lin_lags", pt_lin), ("naive", pt_nav), (f"s{seasonality}", pt_sea)]:
        rows.append({"split":sid, "train_range": f"{a.date()}→{b.date()}",
                     "val_range": f"{c.date()}→{d.date()}",
                     "model":name, **agg(pt)})
    return pd.DataFrame(rows)

# Run on first 2 splits in class
summary_frames=[]
for sid, split in enumerate(splits[:2], start=1):
    sf = summarize_split(feats_baseline, sid, split, save_prefix="linlags")
    summary_frames.append(sf)

summary = pd.concat(summary_frames, ignore_index=True)
summary.to_csv("reports/linlags_summary_splits12.csv", index=False)
summary
```

### 5) (Optional) Tiny ARIMA demo on **one ticker** for the first split

```python
# Optional: quick ARIMA(1,0,0) demo predicting r_{t+1} on val for a single ticker
try:
    from statsmodels.tsa.arima.model import ARIMA
    import warnings; warnings.filterwarnings("ignore")
    a,b,c,d = splits[0]
    tkr = feats["ticker"].cat.categories[0]
    tr = feats[(feats["ticker"]==tkr) & (feats["date"]>=a) & (feats["date"]<=b)]
    va = feats[(feats["ticker"]==tkr) & (feats["date"]>=c) & (feats["date"]<=d)]
    # Fit on TRAIN returns only (endog = log_return). Predict one-step ahead for VAL dates.
    model = ARIMA(tr["log_return"].to_numpy(), order=(1,0,0))
    res = model.fit()
    # Forecast length = len(va), one-step-ahead with dynamic=False updates internally
    # (For strict no-peek rolling one-step, loop and append val true values; here we keep demo simple.)
    fc = res.forecast(steps=len(va))
    arima_mae = mae(va["r_1d"], fc)  # compare against next-day return
    float(arima_mae)
except Exception as e:
    print("ARIMA demo skipped:", e)
```

> ⚠️ ARIMA is **optional** and **slow** on large loops. If you try it per ticker/per split, keep the dataset tiny.

---

## Wrap‑up (10 min)

* You trained a **per‑ticker lags‑only linear** model and compared it fairly to **naive** and **seasonal‑naive** using the **same splits** and **MASE scale** (from the train window).
* You logged results in a **stable schema** that you’ll reuse for future models (LSTM / Transformer).
* ARIMA can be illustrative but is often **fragile + slower**; treat it as optional for your project scale.

---

## Homework (due before next session)

**Goal:** 1) Run the linear lags baseline across **all splits**; 2) Write your **first model card** (Quarto) for the classical baseline.

### Part A — CLI script to evaluate Linear‑Lags across *all* splits

```python
# scripts/eval_linlags.py
#!/usr/bin/env python
from __future__ import annotations
import argparse, numpy as np, pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from pathlib import Path

def mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))
def smape(y,yhat,eps=1e-8):
    y = np.asarray(y); yhat = np.asarray(yhat)
    return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))
def mase(y_true, y_pred, y_train_true, y_train_naive):
    return float(mae(y_true, y_pred) / (mae(y_train_true, y_train_naive)+1e-12))

def make_splits(dates, train_min, val_size, step, embargo):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    splits=[]; i=train_min-1; n=len(u)
    while True:
        if i>=n: break
        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=n: break
        splits.append((a,b,u[vs],u[ve])); i+=step
    return splits

def add_baselines(df, seasonality):
    out = df.copy()
    out["yhat_naive"] = out.groupby("ticker")["log_return"].transform(lambda s: s)
    out["yhat_s"] = out.groupby("ticker")["log_return"].transform(lambda s: s.shift(seasonality-1)) if seasonality>1 else out["yhat_naive"]
    return out

def fit_predict_lin(train_df, val_df, xcols):
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    preds=[]
    for tkr, tr in train_df.groupby("ticker"):
        va = val_df[val_df["ticker"]==tkr]
        if len(tr)==0 or len(va)==0: continue
        pipe = Pipeline([("scaler", StandardScaler()), ("lr", LinearRegression())])
        pipe.fit(tr[xcols].values, tr["r_1d"].values)
        yhat = pipe.predict(va[xcols].values)
        out = va[["date","ticker","r_1d","log_return"]].copy()
        out["yhat_linlags"] = yhat
        preds.append(out)
    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--features", default="data/processed/features_v1.parquet")
    ap.add_argument("--seasonality", type=int, default=5)
    ap.add_argument("--train-min", type=int, default=252)
    ap.add_argument("--val-size", type=int, default=63)
    ap.add_argument("--step", type=int, default=63)
    ap.add_argument("--embargo", type=int, default=5)
    ap.add_argument("--xcols", nargs="+", default=["lag1","lag2","lag3"])
    ap.add_argument("--out-summary", default="reports/linlags_summary.csv")
    ap.add_argument("--out-per-ticker", default="reports/linlags_per_ticker_split{sid}.csv")
    args = ap.parse_args()

    df = pd.read_parquet(args.features).sort_values(["ticker","date"]).reset_index(drop=True)
    df["ticker"] = df["ticker"].astype("category")
    splits = make_splits(df["date"], args.train_min, args.val_size, args.step, args.embargo)
    df = add_baselines(df, args.seasonality)

    rows=[]
    for sid, (a,b,c,d) in enumerate(splits, start=1):
        tr = df[(df["date"]>=a)&(df["date"]<=b)]
        va = df[(df["date"]>=c)&(df["date"]<=d)]
        val_pred = fit_predict_lin(tr, va, args.xcols)
        va = va.merge(val_pred[["date","ticker","yhat_linlags"]], on=["date","ticker"], how="left")
        # per-ticker
        pts=[]
        for tkr, gv in va.groupby("ticker"):
            gv = gv.dropna(subset=["r_1d","yhat_linlags"])
            if len(gv)==0: continue
            gt = tr[tr["ticker"]==tkr].dropna(subset=["r_1d"])
            gt_naive = gt["log_return"]  # scale comparator for MASE
            pts.append({"ticker":tkr,"n":int(len(gv)),
                        "mae": mae(gv["r_1d"], gv["yhat_linlags"]),
                        "smape": smape(gv["r_1d"], gv["yhat_linlags"]),
                        "mase": mase(gv["r_1d"], gv["yhat_linlags"], gt["r_1d"], gt_naive)})
        pt = pd.DataFrame(pts)
        Path("reports").mkdir(exist_ok=True)
        pt.assign(split=sid, model="lin_lags").to_csv(args.out_per_ticker.format(sid=sid), index=False)

        # aggregate
        if not pt.empty:
            macro = pt[["mae","smape","mase"]].mean().to_dict()
            w = pt["n"].to_numpy()
            micro = {"micro_mae": float(np.average(pt["mae"], weights=w)),
                     "micro_smape": float(np.average(pt["smape"], weights=w)),
                     "micro_mase": float(np.average(pt["mase"], weights=w))}
        else:
            macro = {"mae":np.nan,"smape":np.nan,"mase":np.nan}
            micro = {"micro_mae":np.nan,"micro_smape":np.nan,"micro_mase":np.nan}
        rows.append({"split":sid,"train_range":f"{a.date()}→{b.date()}","val_range":f"{c.date()}→{d.date()}",
                     "model":"lin_lags", "macro_mae":float(macro["mae"]), "macro_smape":float(macro["smape"]), "macro_mase":float(macro["mase"]),
                     **micro})

    pd.DataFrame(rows).to_csv(args.out_summary, index=False)
    print("Wrote", args.out_summary)

if __name__ == "__main__":
    main()
```

Make executable & run:

```bash
%%bash
chmod +x scripts/eval_linlags.py
python scripts/eval_linlags.py --xcols lag1 lag2 lag3
```

### Part B — Quarto **Model Card** for the Linear‑Lags baseline

Create `docs/model_card_linear.qmd`:

````markdown
---
title: "Model Card — Linear Lags (Per‑Ticker)"
format:
  html:
    theme: cosmo
    toc: true
params:
  model_name: "Linear Lags (per‑ticker)"
  data: "features_v1.parquet"
---

> **Educational use only — not trading advice.** Predicts next‑day log return \(r_{t+1}\) using past lags.

## Overview

- **Model:** Per‑ticker linear regression with features: `lag1`, `lag2`, `lag3`.
- **Data:** `features_v1.parquet` (Session 10).  
- **Splits:** Expanding, quarterly val, 5‑day embargo (Session 15).  
- **Baselines:** Naive and seasonal‑naive \(s=5\).

## Metrics (across splits)

```{python}
import pandas as pd
df = pd.read_csv("reports/linlags_summary.csv")
df
````

## Discussion

* **Assumptions:** Linear relation to recent returns; stationarity at return level.
* **Strengths:** Fast, interpretable, leakage‑resistant with proper splits.
* **Failure modes:** Regime shifts; volatility spikes; nonlinearity.
* **Ethics:** Educational; not suitable for trading.

````

Render (if Quarto is available):
```bash
quarto render docs/model_card_linear.qmd
````

### Part C — Quick test to safeguard results shape

```python
# tests/test_linlags_results.py
import pandas as pd, os

def test_linlags_summary_exists_and_columns():
    assert os.path.exists("reports/linlags_summary.csv")
    df = pd.read_csv("reports/linlags_summary.csv")
    need = {"split","model","macro_mae","micro_mae"}
    assert need.issubset(df.columns)
```

Run:

```bash
%%bash
pytest -q -k linlags_results
```

### Part D — (Optional) Extend features or add Ridge

* Try `--xcols lag1 lag2 lag3 roll_std_20 zscore_20` (if present in `features_v1`).
* Swap `LinearRegression` for `Ridge(alpha=1.0)`; log and compare.

---

## Instructor checklist (before class)

* Verify `features_v1.parquet` has `lag1..lag3` or the fallback cell creates them.
* Dry‑run the 2‑split demo; ensure total runtime < 5–6 minutes.
* Optionally prepare an ARIMA demo on **one** ticker to illustrate pitfalls.

## Emphasize while teaching

* Keep **splits identical** across models for fair comparison.
* **MASE < 1** ⇒ your model beats naive on train‑scale; report macro & micro.
* Linear lags are a **transparent baseline**—use them to validate your entire pipeline.

## Grading (pass/revise)

* `scripts/eval_linlags.py` runs and writes `reports/linlags_summary.csv` + per‑ticker CSVs.
* Model card exists and renders (locally or in CI artifact).
* Tests for results table shape pass.
* Results show a reasonable comparison against naive/seasonal‑naive.

You now have a **solid classical baseline** with a reproducible evaluation and reporting workflow—perfect for benchmarking upcoming neural models.
