---
title: "Session 20 — Multi‑asset training (unified model"
---
Below is a complete lecture package for **Session 20 — Multi‑asset training (unified model)** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll add a **`ticker_id` embedding** (and optional **sector** embedding) to a sequence model so **one model** learns across **all tickers**.

> **Educational use only — not trading advice.**
> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) and availability of `data/processed/features_v1.parquet` (or `features_v1_static.parquet` from Session 17) with columns like `ticker`, `date`, `log_return`, `r_1d`, and some features (`lag1..lag3`, `roll_std_20`, `zscore_20`, …). Cells include **safe fallbacks** so you can run end‑to‑end.

---

## Session 20 — Multi‑asset training (unified model) (75 min)

### Learning goals

By the end of class, students can:

1. Train **one unified model** across **many tickers** instead of one model per ticker.
2. Add a **`ticker_id` embedding** (and optional **sector** embedding) and concatenate it to sequence inputs.
3. Batch **mixed tickers** safely (respecting the same time‑based splits and embargo from Session 15).
4. Log overall **validation metrics** and **per‑ticker** metrics for fair comparison later.

---

## Agenda (75 min)

* **(10 min)** Slides: unified vs per‑asset models; why embeddings; pitfalls
* **(10 min)** Slides: batching mixed tickers; leakage guardrails; embedding size heuristics
* **(35 min)** **In‑class lab**: dataset that returns `ticker_id` → GRU with `nn.Embedding` → train/evaluate → write per‑ticker metrics
* **(10 min)** Wrap‑up + homework brief
* **(10 min)** Buffer / Q\&A

---

## Slide talking points (add to your deck)

### Why unified?

* **Data efficiency:** share statistical strength across assets.
* **Personalization:** learn **asset‑specific biases** via **ID embeddings** (and optional sector embeddings).
* **Simplicity:** one checkpoint, easier hyperparam search.
* **Trade‑off:** can **overfit** to IDs if embedding too large; must compare to **per‑ticker** baselines.

### Embeddings in a regression model

* Treat categorical IDs (ticker, sector) as learnable vectors.
* Concatenate the embedding to **every time step** features:
  $x'_t = [x_t \;\|\; e_{\text{ticker}} \;\|\; e_{\text{sector}}]$.
* Heuristic sizes: $d_\text{ticker} \in [8, 16]$; $d_\text{sector} \in [4, 8]$. Start small.

### Batching mixed tickers without leakage

* **Splits** are **by date** (same as Session 15); **do not** fit scalers or thresholds on validation.
* The ID mapping is just **indices**; it’s **not** a data leak.
* Keep **train‑fit scaler**, reuse on val/test.

---

## In‑class lab (35 min, Colab‑friendly)

> Run each block as its **own cell** in Colab. Replace `REPO_NAME` if needed.

### 0) Setup & device

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME  = "unified-stocks-teamX"   # <- change to your repo name
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, sys, platform, random
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
for p in ["data/processed","models","reports","scripts","tests"]:
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)
print("Working dir:", os.getcwd())

import numpy as np, pandas as pd, torch
print("Torch:", torch.__version__, "| CUDA:", torch.cuda.is_available(), "| Python:", sys.version.split()[0], "| OS:", platform.system())

def seed_everything(seed=2025):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
seed_everything(2025)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
```

### 1) Load features (with safe fallback) and define a split

```python
from pathlib import Path

f_static = Path("data/processed/features_v1_static.parquet")
f_base   = Path("data/processed/features_v1.parquet")

if f_static.exists():
    df = pd.read_parquet(f_static)
elif f_base.exists():
    df = pd.read_parquet(f_base)
else:
    # Fallback from returns
    rpath = Path("data/processed/returns.parquet")
    if not rpath.exists():
        # synthesize a tiny dataset to keep class flowing
        rng = np.random.default_rng(0)
        dates = pd.bdate_range("2022-01-03", periods=340)
        frames=[]
        for t in ["AAPL","MSFT","GOOGL","AMZN","NVDA","META"]:
            eps = rng.normal(0,0.012,size=len(dates)).astype("float32")
            adj = 100*np.exp(np.cumsum(eps))
            g = pd.DataFrame({
                "date": dates, "ticker": t,
                "adj_close": adj.astype("float32"),
                "log_return": np.r_[np.nan, np.diff(np.log(adj))].astype("float32")
            })
            g["r_1d"] = g["log_return"].shift(-1)
            frames.append(g)
        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)
        df["ticker"] = df["ticker"].astype("category")
        df.to_parquet("data/processed/returns.parquet", index=False)
    else:
        df = pd.read_parquet(rpath)
        df = df.sort_values(["ticker","date"]).reset_index(drop=True)
    # ensure minimal features
    for k in [1,2,3]:
        df[f"lag{k}"] = df.groupby("ticker")["log_return"].shift(k)
    df["roll_std_20"]  = df.groupby("ticker")["log_return"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)
    df["zscore_20"]    = (df["log_return"] - df.groupby("ticker")["log_return"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)) / (df["roll_std_20"] + 1e-8)
    df = df.dropna().reset_index(drop=True)

# Harmonize and subset for speed
df["date"] = pd.to_datetime(df["date"])
df["ticker"] = df["ticker"].astype("category")
df = df.sort_values(["ticker","date"]).reset_index(drop=True)
keep = df["ticker"].cat.categories.tolist()[:10]  # up to 10 tickers for class
df = df[df["ticker"].isin(keep)].copy()

# Choose features
CAND = ["log_return","lag1","lag2","lag3","zscore_20","roll_std_20"]
FEATS = [c for c in CAND if c in df.columns]
assert "r_1d" in df.columns and FEATS, "Missing required columns."

def make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    i=train_min-1; out=[]
    while True:
        if i>=len(u): break
        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=len(u): break
        out.append((a,b,u[vs],u[ve])); i+=step
    return out

splits = make_splits(df["date"], 252, 63, 63, 5)
assert splits, "Not enough history for split 1."
a,b,c,d = splits[0]
train_df = df[(df["date"]>=a)&(df["date"]<=b)].copy()
val_df   = df[(df["date"]>=c)&(df["date"]<=d)].copy()
print("Split1 Train:", a.date(), "→", b.date(), "| Val:", c.date(), "→", d.date(),
      "| tickers train:", train_df["ticker"].nunique(), "val:", val_df["ticker"].nunique())
```

### 2) Dataset with **ticker IDs** (+ optional sector)

```python
import json
from torch.utils.data import Dataset, DataLoader

class FeatureScaler:
    def __init__(self): self.mean_=None; self.std_=None
    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self
    def transform(self, X): return (X - self.mean_) / self.std_
    def state_dict(self): return {"mean": self.mean_.tolist(), "std": self.std_.tolist()}
    def load_state_dict(self, d): self.mean_=np.array(d["mean"]); self.std_=np.array(d["std"])

def build_sector_map(frame: pd.DataFrame):
    # Optional: if a 'sector' column exists (from Session 12 scrape), use it; else all 'UNKNOWN'.
    if "sector" in frame.columns:
        sec = frame[["ticker","sector"]].drop_duplicates().copy()
    else:
        sec = frame[["ticker"]].drop_duplicates().copy()
        sec["sector"] = "UNKNOWN"
    sec["sector"] = sec["sector"].astype("category")
    return dict(zip(sec["ticker"].astype(str), sec["sector"].cat.codes.astype(int))), len(sec["sector"].cat.categories)

class WindowedDatasetXID(Dataset):
    """
    Sliding windows per ticker with ID features.
    Returns: X_scaled (T,F), y (scalar), ticker_id (long), sector_id (long)
    """
    def __init__(self, frame: pd.DataFrame, feature_cols, context_len=64,
                 scaler: FeatureScaler|None=None,
                 ticker2id: dict|None=None, sector2id: dict|None=None, n_sectors: int|None=None):
        assert {"ticker","date","r_1d"}.issubset(frame.columns)
        self.feature_cols = list(feature_cols); self.T = int(context_len)
        self.groups, self.index = {}, []

        # Build mappings on TRAIN; reuse on VAL
        if ticker2id is None:
            cats = frame["ticker"].astype("category").cat.categories.tolist()
            self.ticker2id = {t:i for i,t in enumerate(cats)}
        else:
            self.ticker2id = dict(ticker2id)
        sec_map, nsec = build_sector_map(frame)
        if sector2id is None:
            # Freeze sector ids according to sec_map order
            uniq_secs = sorted(set(sec_map.values()))
            self.sector2id = {s:i for i,s in enumerate(uniq_secs)}
            self.n_sectors = len(self.sector2id)
        else:
            self.sector2id = dict(sector2id)
            self.n_sectors = int(n_sectors)

        # Build per-ticker arrays and global window index
        for tkr, g in frame.groupby("ticker"):
            g = g.sort_values("date").reset_index(drop=True)
            X = g[self.feature_cols].to_numpy("float32")
            y = g["r_1d"].to_numpy("float32")
            t_id = self.ticker2id[str(tkr)]
            s_id = self.sector2id.get(build_sector_map(g)[0][str(tkr)], 0) if "sector" in g else 0
            # Valid windows
            for end in range(self.T-1, len(g)):
                if np.isfinite(y[end]):
                    self.index.append((str(tkr), end, t_id, s_id))
            self.groups[str(tkr)] = {"X": X, "y": y}

        # Fit or reuse scaler
        self.scaler = scaler or FeatureScaler().fit(
            np.concatenate([self.groups[t]["X"] for t in self.groups], axis=0)
        )

    def __len__(self): return len(self.index)

    def __getitem__(self, i):
        tkr, end, t_id, s_id = self.index[i]
        g = self.groups[tkr]
        xs = g["X"][end-self.T+1:end+1]
        xs = self.scaler.transform(xs)
        y  = g["y"][end]
        return (torch.from_numpy(xs), torch.tensor(y, dtype=torch.float32),
                torch.tensor(t_id, dtype=torch.long), torch.tensor(s_id, dtype=torch.long))

# Build TRAIN/VAL datasets & loaders (reusing train-fitted scaler and ID maps)
T = 64; BATCH=256; WORKERS=2; PIN=torch.cuda.is_available()

train_ds = WindowedDatasetXID(train_df, FEATS, context_len=T)
val_ds   = WindowedDatasetXID(val_df,   FEATS, context_len=T,
                              scaler=train_ds.scaler,
                              ticker2id=train_ds.ticker2id,
                              sector2id=train_ds.sector2id,
                              n_sectors=train_ds.n_sectors)

from torch.utils.data import DataLoader
def _seed_worker(_):
    ws = torch.initial_seed() % (2**32)
    np.random.seed(ws); random.seed(ws)

g = torch.Generator(); g.manual_seed(42)
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True,
                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),
                          worker_init_fn=_seed_worker, generator=g)
val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False,
                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),
                          worker_init_fn=_seed_worker)

len(train_ds), len(val_ds), next(iter(train_loader))[0].shape, len(train_ds.ticker2id)
```

### 3) **Unified GRU** with `ticker_id` embedding (optional sector)

```python
import torch.nn as nn, torch
from torch.cuda.amp import autocast, GradScaler

class UnifiedGRUWithID(nn.Module):
    def __init__(self, in_features: int, n_tickers: int, d_ticker: int = 12,
                 n_sectors: int | None = None, d_sector: int = 0,
                 hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):
        super().__init__()
        self.tok = nn.Embedding(n_tickers, d_ticker)
        self.sec = nn.Embedding(n_sectors, d_sector) if (n_sectors and d_sector>0) else None
        augmented_in = in_features + d_ticker + (d_sector if self.sec else 0)
        self.gru = nn.GRU(input_size=augmented_in, hidden_size=hidden, num_layers=num_layers,
                          batch_first=True, dropout=dropout if num_layers>1 else 0.0)
        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden, 1))

    def forward(self, x, ticker_ids, sector_ids=None):
        # x: (B,T,F)
        e = self.tok(ticker_ids)                          # (B, d_ticker)
        if self.sec is not None and sector_ids is not None:
            e = torch.cat([e, self.sec(sector_ids)], dim=-1)  # (B, d_ticker+d_sector)
        e = e.unsqueeze(1).expand(-1, x.size(1), -1)      # repeat across time
        x_aug = torch.cat([x, e], dim=-1)                 # (B,T,F+E)
        _, hN = self.gru(x_aug)
        h = hN[-1]                                        # (B, H)
        return self.head(h).squeeze(-1)

def make_model():
    return UnifiedGRUWithID(in_features=len(FEATS),
                            n_tickers=len(train_ds.ticker2id),
                            d_ticker=12,
                            n_sectors=val_ds.n_sectors, d_sector=0,  # set >0 if you have sector
                            hidden=64, num_layers=2, dropout=0.1)

model = make_model().to(device)
sum(p.numel() for p in model.parameters())/1e6, device
```

### 4) Training loop (AMP + early stopping) and evaluation (**per‑ticker** metrics)

```python
from torch.optim import AdamW
import time, math

def mae_t(y, yhat): return torch.mean(torch.abs(y - yhat))
def smape_t(y, yhat, eps=1e-8): return torch.mean(2*torch.abs(y - yhat)/(torch.abs(y)+torch.abs(yhat)+eps))

def train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):
    model.train(); tot=0.0; n=0
    for xb, yb, tid, sid in loader:
        xb = xb.to(device, non_blocking=True).float()
        yb = yb.to(device, non_blocking=True).float()
        tid = tid.to(device, non_blocking=True)
        sid = sid.to(device, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)
        if use_amp and device.type=="cuda":
            with autocast(dtype=torch.float16):
                pred = model(xb, tid, sid)
                loss = mae_t(yb, pred)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
        else:
            pred = model(xb, tid, sid); loss = mae_t(yb, pred); loss.backward(); optimizer.step()
        bs = xb.size(0); tot += loss.item()*bs; n += bs
    return tot/max(n,1)

@torch.no_grad()
def evaluate(model, loader, device, return_preds=False):
    model.eval(); t_mae=t_smape=0.0; n=0
    all_rows=[]
    for xb, yb, tid, sid in loader:
        xb = xb.to(device, non_blocking=True).float()
        yb = yb.to(device, non_blocking=True).float()
        tid = tid.to(device, non_blocking=True)
        sid = sid.to(device, non_blocking=True)
        pred = model(xb, tid, sid)
        bs = xb.size(0); t_mae += mae_t(yb, pred).item()*bs; t_smape += smape_t(yb, pred).item()*bs; n+=bs
        if return_preds:
            all_rows.append((yb.detach().cpu().numpy(), pred.detach().cpu().numpy(), tid.detach().cpu().numpy()))
    out = {"mae": t_mae/max(n,1), "smape": t_smape/max(n,1)}
    if return_preds:
        ys = np.concatenate([r[0] for r in all_rows]); yh = np.concatenate([r[1] for r in all_rows]); tids = np.concatenate([r[2] for r in all_rows])
        out["y_true"] = ys; out["y_pred"] = yh; out["ticker_id"] = tids
    return out

def fit_unified(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):
    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)
    scaler = GradScaler(enabled=(use_amp and device.type=="cuda"))
    best=math.inf; best_ep=-1; ckpt = Path("models/unified_gru_split1.pt")
    hist=[]
    for ep in range(1, epochs+1):
        t0=time.time()
        tr = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)
        val = evaluate(model, val_loader, device)
        dt=time.time()-t0
        hist.append({"epoch":ep,"train_mae":tr,"val_mae":val["mae"],"val_smape":val["smape"],"sec":dt})
        print(f"Epoch {ep:02d}  train_mae={tr:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)")
        if val["mae"] < best - 1e-6:
            best = val["mae"]; best_ep=ep
            torch.save({"model": model.state_dict(),
                        "epoch": ep,
                        "config": {"feats": FEATS, "T": T, "d_ticker": 12}}, ckpt)
        elif ep - best_ep >= patience:
            print(f"Early stopping at epoch {ep} (best {best:.5f} @ {best_ep})"); break
    return hist, best, best_ep, ckpt

model = make_model().to(device)
hist, best, best_ep, ckpt = fit_unified(model, train_loader, val_loader, epochs=10, lr=1e-3, wd=1e-5, patience=3, use_amp=True)
print("Best val_mae:", best, "| epoch:", best_ep, "| saved:", ckpt.exists())
```

#### Per‑ticker metrics & CSVs

```python
# Reload best and compute per-ticker metrics
ckpt = torch.load("models/unified_gru_split1.pt", map_location=device)
model.load_state_dict(ckpt["model"]); model.to(device)

# Evaluate with predictions and map back to ticker symbols
res = evaluate(model, val_loader, device, return_preds=True)
id2ticker = {v:k for k,v in train_ds.ticker2id.items()}
pt_rows=[]
for tid in np.unique(res["ticker_id"]):
    m = res["ticker_id"]==tid
    y = res["y_true"][m]; yhat = res["y_pred"][m]
    if len(y)==0: continue
    pt_rows.append({"ticker": id2ticker[int(tid)], "n": int(len(y)),
                    "mae": float(np.mean(np.abs(y - yhat))),
                    "smape": float(np.mean(2*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+1e-8)))})
per_ticker = pd.DataFrame(pt_rows).sort_values("mae")
per_ticker.to_csv("reports/unified_gru_split1_per_ticker.csv", index=False)

# Overall micro metrics (pooled)
overall = pd.DataFrame([{
    "split": 1, "model": "unified_gru_id", "context": T, "feats": ",".join(FEATS),
    "val_mae": float(np.mean(np.abs(res["y_true"] - res["y_pred"]))),
    "val_smape": float(np.mean(2*np.abs(res["y_true"] - res["y_pred"])/(np.abs(res["y_true"])+np.abs(res["y_pred"])+1e-8))),
    "best_epoch": ckpt.get("epoch", None),
    "params_M": round(sum(p.numel() for p in model.parameters())/1e6, 3)
}])
overall.to_csv("reports/unified_gru_split1_metrics.csv", index=False)
(per_ticker.head(), overall)
```

> **Time check:** With \~8–10 tickers, `T=64`, and 10 epochs, this should finish in a couple of minutes on Colab CPU; faster on GPU.

---

## Wrap‑up (10 min) — key points

* **Unified** models share information across assets and adapt via **ID embeddings**.
* Embedding dims should be **small** (8–16) to avoid overfitting; they act like **bias + style** vectors.
* Keep the **same splits** and **train‑fit scaler** to avoid leakage.
* Always log **per‑ticker** metrics to check that no specific asset collapses.

---

## Homework (due before Session 21)

**Goal:** Compare **unified (ID‑augmented)** vs **per‑ticker** models on **split 1**. Produce a single table `reports/unified_vs_per_ticker_split1.csv` and a short paragraph in your Quarto report.

### Part A — Train per‑ticker GRU baselines and compare

Create **`scripts/compare_unified_vs_per_ticker.py`**:

```python
#!/usr/bin/env python
from __future__ import annotations
import numpy as np, pandas as pd, torch, torch.nn as nn
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler

# ---- Utilities reused from class ----
def make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]
    while True:
        if i>=len(u): break
        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=len(u): break
        out.append((a,b,u[vs],u[ve])); i+=step
    return out

class FeatureScaler:
    def __init__(self): self.mean_=None; self.std_=None
    def fit(self, X): self.mean_=X.mean(0); self.std_=X.std(0)+1e-8; return self
    def transform(self, X): return (X-self.mean_)/self.std_

class WindowedDataset(Dataset):
    def __init__(self, df, feats, T=64, scaler=None):
        self.feats=feats; self.T=T; self.idx=[]; self.X=None; self.y=None
        g=df.sort_values("date").reset_index(drop=True)
        self.X = g[feats].to_numpy("float32"); self.y = g["r_1d"].to_numpy("float32")
        for end in range(T-1, len(g)):
            if np.isfinite(self.y[end]): self.idx.append(end)
        self.scaler = scaler or FeatureScaler().fit(self.X)
    def __len__(self): return len(self.idx)
    def __getitem__(self,i):
        end=self.idx[i]; X=self.scaler.transform(self.X[end-self.T+1:end+1])
        return torch.from_numpy(X), torch.tensor(self.y[end], dtype=torch.float32)

class GRUReg(nn.Module):
    def __init__(self, in_f, h=64, L=2, p=0.1):
        super().__init__()
        self.gru = nn.GRU(in_f, h, num_layers=L, batch_first=True, dropout=p if L>1 else 0.)
        self.head= nn.Sequential(nn.Linear(h,h), nn.ReLU(), nn.Dropout(p), nn.Linear(h,1))
    def forward(self, x):
        _,hN = self.gru(x); return self.head(hN[-1]).squeeze(-1)

def mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))
def smape(y,yhat,eps=1e-8):
    y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))

def train_eval_one_ticker(df_t, df_v, feats, T=64, epochs=10, lr=1e-3, batch=128, device="cpu"):
    tr_ds = WindowedDataset(df_t, feats, T=T); va_ds = WindowedDataset(df_v, feats, T=T, scaler=tr_ds.scaler)
    tr_ld = DataLoader(tr_ds, batch_size=batch, shuffle=True, drop_last=True)
    va_ld = DataLoader(va_ds, batch_size=batch, shuffle=False)
    net = GRUReg(len(feats)).to(device); opt = AdamW(net.parameters(), lr=lr, weight_decay=1e-5)
    scaler = GradScaler(enabled=(device=="cuda"))
    best=1e9
    for ep in range(1, epochs+1):
        net.train()
        for xb,yb in tr_ld:
            xb=xb.to(device).float(); yb=yb.to(device).float()
            opt.zero_grad(set_to_none=True)
            with autocast(enabled=(device=="cuda"), dtype=torch.float16):
                yhat = net(xb); loss = torch.mean(torch.abs(yb - yhat))
            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
        # quick val
        net.eval(); preds=[]; ys=[]
        with torch.no_grad():
            for xb,yb in va_ld:
                xb=xb.to(device).float(); yb=yb.to(device).float()
                preds.append(net(xb).cpu().numpy()); ys.append(yb.cpu().numpy())
        y = np.concatenate(ys); yhat = np.concatenate(preds)
        best = min(best, mae(y,yhat))
    return best, smape(y,yhat)

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    f_static = Path("data/processed/features_v1_static.parquet")
    df = pd.read_parquet(f_static if f_static.exists() else "data/processed/features_v1.parquet").sort_values(["ticker","date"]).reset_index(drop=True)
    CAND = ["log_return","lag1","lag2","lag3","zscore_20","roll_std_20"]
    feats = [c for c in CAND if c in df.columns]; assert "r_1d" in df.columns
    splits = make_splits(df["date"]); a,b,c,d = splits[0]
    tr_all = df[(df["date"]>=a)&(df["date"]<=b)]; va_all = df[(df["date"]>=c)&(df["date"]<=d)]
    tickers = tr_all["ticker"].astype(str).unique().tolist()
    rows=[]
    for t in tickers:
        tr = tr_all[tr_all["ticker"]==t]; va = va_all[va_all["ticker"]==t]
        if len(tr)<100 or len(va)<20: continue
        vmae, vsmape = train_eval_one_ticker(tr, va, feats, T=64, epochs=10, device=device)
        rows.append({"ticker":t, "model":"per_ticker_gru", "val_mae":vmae, "val_smape":vsmape})
    per_ticker = pd.DataFrame(rows)
    # Load unified results written in class
    uni_pt = pd.read_csv("reports/unified_gru_split1_per_ticker.csv").rename(columns={"mae":"val_mae","smape":"val_smape"})
    uni_pt["model"] = "unified_gru_id"
    # Join and compare
    comp = per_ticker.merge(uni_pt[["ticker","model","val_mae","val_smape"]], on="ticker", how="outer", suffixes=("_per","_uni"))
    comp.to_csv("reports/unified_vs_per_ticker_split1.csv", index=False)
    print("Wrote reports/unified_vs_per_ticker_split1.csv")

if __name__ == "__main__":
    main()
```

Run:

```bash
%%bash
chmod +x scripts/compare_unified_vs_per_ticker.py
python scripts/compare_unified_vs_per_ticker.py
```

### Part B — Add a Makefile target and a minimal test

Append to **`Makefile`**:

```make
.PHONY: train-unified compare-unified
train-unified: ## Train unified GRU with ticker embeddings on split 1
\tpython - <<'PY'
from pathlib import Path
import runpy
runpy.run_module('scripts.__init__', run_name='__main__') if Path('scripts/__init__.py').exists() else None
PY

compare-unified: ## Compare unified vs per-ticker baselines
\tpython scripts/compare_unified_vs_per_ticker.py
```

*(If you prefer, replace `train-unified` with a thin wrapper to re-run the in‑class cells as a script.)*

**Test:** ensure per‑ticker comparison file exists and has required columns.

```python
# tests/test_unified_outputs.py
import os, pandas as pd

def test_unified_vs_per_ticker_table():
    assert os.path.exists("reports/unified_vs_per_ticker_split1.csv")
    df = pd.read_csv("reports/unified_vs_per_ticker_split1.csv")
    need = {"ticker","val_mae_per","val_mae_uni"}
    assert need.issubset(df.columns)
```

Run:

```bash
%%bash
pytest -q -k unified_outputs
```

### Part C — Add a short paragraph + table to your Quarto report

In `reports/eda.qmd` (or a new `reports/unified.qmd`), add:

````markdown
## Unified vs Per‑Ticker

We trained a **unified GRU with ticker embeddings** and compared it to **per‑ticker GRUs** on Split 1.

```{python}
import pandas as pd
pd.read_csv("reports/unified_vs_per_ticker_split1.csv").head(10)
````

**Note.** Unified models can outperform when data per asset is limited, but may underperform on idiosyncratic assets. Keep the embedding **small** to limit overfitting.

```

---

## Instructor notes / gotchas

- **Embedding dimension**: start small (8–16). Larger dims can overfit and memorize IDs.  
- **Scaler**: Fit on TRAIN only; reuse on VAL. It prevents target leakage through normalization.  
- **Per‑ticker metrics**: Always report them to avoid hiding a few failing assets inside a good aggregate.  
- **Sector embedding** (optional): If you have `sector` from Session 12, set `d_sector>0` in the model and pass `sector_ids` from the dataset.

---

## Grading (pass/revise)

- Unified model with **ticker embeddings** trains and writes:
  - `models/unified_gru_split1.pt`  
  - `reports/unified_gru_split1_metrics.csv` (overall)  
  - `reports/unified_gru_split1_per_ticker.csv` (per‑ticker)  
- Comparison table `reports/unified_vs_per_ticker_split1.csv` exists.  
- Quarto report updated with the comparison and a short discussion.

---

### Optional extensions (if students finish early)

- **Ablate** embedding size: d=0 (no ID) vs 8 vs 16.  
- Add **dropout** on embeddings or **L2** weight decay on embedding parameters only.  
- Try **concatenating embeddings at the head** (after GRU) instead of at input, and compare.  
- Try **sector embeddings** (`d_sector=4`) if you have sector metadata.  

You now have a single **multi‑asset** model with **tickers encoded as embeddings**—the foundation for Session 21, where you’ll implement attention and a **tiny GPT** on toy data before adapting it to time series.
```
