{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjZLHzdNwBD1r+QIfPIh4L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.flush_and_unmount()           # ignore errors if already unmounted\n","\n","#If cannot remount, simply delete the mounted drive and then remount\n","# rm -rf /content/drive\n"],"metadata":{"id":"H-1K7L9NJHMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3b0cacda","outputId":"fbf639b6-2e71-4487-bc3c-8734d8e63efa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761524225482,"user_tz":300,"elapsed":20409,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Colab cell\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"01f28b78","executionInfo":{"status":"ok","timestamp":1761524226556,"user_tz":300,"elapsed":300,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[],"source":["# Adjust these two for YOUR repo\n","REPO_OWNER = \"ywanglab\"\n","REPO_NAME  = \"STAT4160\"   # e.g., unified-stocks-team1\n","BASE_DIR   = \"/content/drive/MyDrive/dspt25\"\n","CLONE_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n","REPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n","\n","# if on my office computer\n","\n","# REPO_NAME  = \"lectureNotes\"   # e.g., on my office computer\n","# BASE_DIR = r\"E:\\OneDrive - Auburn University Montgomery\\teaching\\AUM\\STAT 4160 Productivity Tools\" # on my office computer\n","# CLONE_DIR  = f\"{BASE_DIR}\\{REPO_NAME}\"\n","\n","import os, pathlib\n","pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"36277e0a","outputId":"3ea86efc-693a-4f2e-ad42-ff9f531f0038","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761524228677,"user_tz":300,"elapsed":3,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n"]}],"source":["import os, subprocess, shutil, pathlib\n","\n","if not pathlib.Path(CLONE_DIR).exists():\n","    !git clone {REPO_URL} {CLONE_DIR}\n","else:\n","    # If the folder exists, just ensure it's a git repo and pull latest\n","    os.chdir(CLONE_DIR)\n","    # !git status\n","    # !git pull --rebase # !git pull --ff-only\n","os.chdir(CLONE_DIR)\n","print(\"Working dir:\", os.getcwd())"]},{"cell_type":"markdown","source":["**Assumptions:** You completed Session 9–12 and have `data/processed/features_v1.parquet` (or `features_v1_ext.parquet`). If a file is missing, the lab provides a small synthetic fallback so tests still run. **Goal today:** Make it **hard to ship bad data** by adding precise, fast tests.\n","\n","------------------------------------------------------------------------\n","\n","## Session 13 — pytest + Data Validation\n","\n","### Learning goals\n","\n","By the end of class, students can:\n","\n","1.  Write **fast, high‑signal tests** for data pipelines (shapes, dtypes, nulls, **no look‑ahead**).\n","2.  Validate a DataFrame with **Pandera** (schema + value checks) or **custom checks** only.\n","3.  Use **logging** effectively and capture logs in tests.\n","4.  Run tests in Colab / locally and prepare for CI in Session 14.\n","\n","------------------------------------------------------------------------\n","\n","## Agenda\n","\n","-    What to test (and not), “data tests” vs unit tests, speed budget\n","-    Pandera schemas & custom checks; tolerance and stability\n","-   Logging basics (`logging`, levels, handlers); testing logs with `caplog`\n","-    **In‑class lab**: add `tests/test_features.py` (+ optional Pandera test), fixtures, config; run & fix\n","-    Wrap‑up + homework briefing\n","\n","------------------------------------------------------------------------\n","\n","\n","### What to test\n","\n","-   **Contract tests** (tests according to a contract bewteen consumer and a service provider) for data:\n","\n","    -   **Schema**: required columns exist; dtypes sane (`ticker` categorical, calendar ints).\n","    -   **Nulls**: no NAs in training‑critical cols.\n","    -   **Semantics**: `r_1d` is **lead** of `log_return`; rolling features computed from **past only**.\n","    -   **Keys**: no duplicate `(ticker, date)`; dates strictly increasing within ticker.\n","\n","-   Keep tests **under \\~5s total** (CI budget). Avoid long recomputations; sample/take head.\n","\n","### Pandera vs custom checks\n","\n","-   **Pandera**: declarative schema; optional dependency; good for **column existence + ranges**.\n","-   **Custom**: essential for **domain logic** (look‑ahead bans, exact rolling formulas).\n","\n","### Logging basics\n","\n","-   Use `logging.getLogger(__name__)`; set level via env (`LOGLEVEL=INFO`).\n","-   Log **counts, ranges, and any data drops** inside build scripts.\n","-   In tests: use `caplog` to assert a warning is emitted for suspicious conditions.\n","\n","------------------------------------------------------------------------\n"],"metadata":{"id":"ZNMB1I80_Tb4"}},{"cell_type":"markdown","source":["### 1) (Optional) Install test‑time helpers (Pandera)"],"metadata":{"id":"Yc3HKM2LA1kv"}},{"cell_type":"code","source":["!pip -q install pytest pandera pyarrow"],"metadata":{"id":"9XsJ2yQrA25G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759768918514,"user_tz":300,"elapsed":5337,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"7f514fe6-f1ee-47ca-c4e7-b9e19ab94288"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/292.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m256.0/292.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# Pandera\n","\n","**Pandera** is a **Python library** used for **data validation and testing of pandas DataFrames**.\n","\n","It helps you **define schemas** that describe what your data *should* look like — for example, the expected column names, data types, ranges, or even statistical properties — and then automatically checks whether a given DataFrame conforms to those expectations.\n","\n","---\n","\n","###  Why Pandera is useful\n","\n","When working with data pipelines (e.g., ETL (Extract, Transform, Load), feature engineering, model training), your data can easily become corrupted or inconsistent.\n","Pandera acts as a **“data contract”** (agreement between different parts of a data system) between pipeline stages.\n","\n","It helps you:\n","\n","* Catch data quality issues early (e.g., missing columns, wrong dtypes, NaNs)\n","* Ensure data consistency across steps\n","* Document dataset expectations\n","* Write **unit tests** for data (like how `pytest` tests code logic)\n","\n","---\n","\n","###  Example\n","\n","```python\n","import pandas as pd\n","import pandera as pa\n","from pandera import Column, Check\n","\n","# Define a schema for a DataFrame\n","class StocksSchema(pa.DataFrameModel):\n","    ticker: pa.typing.Series[str]\n","    date: pa.typing.Series[pd.Timestamp]\n","    adj_close: pa.typing.Series[float] = pa.Field(gt=0)  # must be > 0\n","    volume: pa.typing.Series[int] = pa.Field(ge=0)        # must be ≥ 0\n","    log_return: pa.typing.Series[float]\n","\n","# Create a sample DataFrame\n","df = pd.DataFrame({\n","    \"ticker\": [\"AAPL\", \"AAPL\"],\n","    \"date\": pd.to_datetime([\"2024-01-01\", \"2024-01-02\"]),\n","    \"adj_close\": [189.5, 190.2],\n","    \"volume\": [1000, 1200],\n","    \"log_return\": [0.0, 0.0037],\n","})\n","\n","# Validate it\n","validated = StocksSchema.validate(df)\n","print(\"✅ Data passed validation!\")\n","```\n","\n","If the data violates the schema (say `adj_close` ≤ 0), Pandera will raise a **SchemaError** with a clear message.\n","\n","---\n","\n","###  Other features\n","\n","* **Integration** with `pytest` for automated testing\n","* **Statistical checks** (e.g., mean, std, correlations)\n","* **Lazy validation** (collect all errors before raising)\n","* **Schema inference** (autogenerate schema from sample data)\n","* **Compatibility** with `pandas`, `polars`, `pyarrow`, and `modin`\n","\n","---\n","\n","###  Analogy\n","\n","If **pandas** is for manipulating data,\n","then **pandera** is for *ensuring that the manipulated data makes sense.*\n","\n"],"metadata":{"id":"ffPmwgdQBKCo"}},{"cell_type":"markdown","source":["#  Put a tiny **logging helper** in your repo (used by build scripts & tests)"],"metadata":{"id":"C70L5IRFBs_l"}},{"cell_type":"markdown","source":["```python\n","def setup_logging(name: str = \"dspt\"):\n","```\n","\n","Defines a function that sets up a **logger** named `\"dspt\"` (by default).\n","You can call it like `logger = setup_logging(\"myapp\")` to get your own logger instance.\n","\n","---\n","\n","###  Determine the log level\n","\n","```python\n","level = os.getenv(\"LOGLEVEL\", \"INFO\").upper()\n","```\n","\n","* Reads the environment variable `LOGLEVEL`.\n","* If not set, defaults to `\"INFO\"`.\n","* Converts it to uppercase (`\"debug\"` → `\"DEBUG\"`).\n","\n","Typical levels:\n","`DEBUG < INFO < WARNING < ERROR < CRITICAL`\n","\n","This allows you to change verbosity **without editing code**, e.g.:\n","\n","```bash\n","export LOGLEVEL=DEBUG\n","python run_pipeline.py\n","```\n","\n","---\n","\n","### Get a named logger\n","\n","```python\n","logger = logging.getLogger(name)\n","```\n","\n","This retrieves (or creates) a **logger object** with that name.\n","Named loggers are hierarchical — for example, `\"dspt.submodule\"` inherits from `\"dspt\"`.\n","\n","---\n","\n","###  Add a handler (only once)\n","\n","```python\n","if not logger.handlers:\n","    handler = logging.StreamHandler()\n","    fmt = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\" #% formating, see below for more explanation\n","    handler.setFormatter(logging.Formatter(fmt))\n","    logger.addHandler(handler)\n","```\n","\n","* **Handlers** define *where* logs go (console, file, etc.).\n","  Here it uses `StreamHandler`, which prints to the console (`sys.stderr`).\n","* **Formatter** defines the output format:\n","\n","  ```\n","  2025-10-06 09:45:23,612 | INFO | dspt | Building database...\n","  ```\n","* The `if not logger.handlers` guard prevents adding duplicate handlers if this function is called multiple times (a common pitfall).\n","\n","---\n","\n","###  Set the logging level\n","\n","```python\n","logger.setLevel(level)\n","```\n","\n","Tells the logger which messages to process (e.g., `DEBUG` shows all, `INFO` hides debug details).\n","\n","---\n","\n","###  Return it for use\n","\n","```python\n","return logger\n","```\n","\n","So you can do:\n","\n","```python\n","logger = setup_logging(\"build_db\")\n","logger.info(\"Building prices database...\")\n","logger.debug(f\"Using LOGLEVEL={os.getenv('LOGLEVEL')}\")\n","```\n","\n","---\n","\n","### Typical Output Example\n","\n","```\n","2025-10-06 09:47:22,023 | INFO | build_db | Building prices database...\n","2025-10-06 09:47:22,025 | DEBUG | build_db | Connected to data/prices.db\n","```\n","\n"],"metadata":{"id":"ciuhTZtYCR8n"}},{"cell_type":"markdown","source":["# __future__ annotations\n","\n","```python\n","from __future__ import annotations\n","```\n","\n","It tells Python to **treat all type annotations as strings**, i.e. *not* to evaluate them immediately at runtime.\n","\n","This is called **“postponed evaluation of annotations”**.\n","\n","Normally, when you write something like:\n","\n","```python\n","class Node:\n","    def __init__(self, next: Node | None = None):\n","        self.next = next\n","```\n","\n","Python tries to *evaluate* `Node` right away.\n","But at that point, the class `Node` hasn’t been fully defined yet — so you’d get an error:\n","\n","```\n","NameError: name 'Node' is not defined\n","```\n","\n","---\n","\n","###  Fix using `from __future__ import annotations`\n","\n","```python\n","from __future__ import annotations\n","\n","class Node:\n","    def __init__(self, next: Node | None = None):\n","        self.next = next\n","```\n","\n","Now Python **stores** `\"Node | None\"` as a **string** in the function’s `__annotations__` instead of trying to resolve it immediately.\n","\n","\n","\n","### Why this matters\n","\n"," **Avoids forward-reference issues**\n","\n","```python\n","class A:\n","    def link(self, b: B):  # ← B not yet defined\n","        ...\n","class B: ...\n","```\n","\n","→ Works fine if you’ve imported `annotations`.\n","\n"," **Improves import performance**\n","Since annotations aren’t evaluated at import time, it can slightly speed up module loading.\n","\n"," **Simplifies circular type references**\n","You no longer have to write `'B'` manually (string form) in type hints.\n","\n","---\n","### Since Python 3.11+\n","\n","Starting from **Python 3.11**, this behavior became the **default** (PEP (Python Enhancement Proposal) 563 → PEP 649 resolution).\n","So in Python 3.11+ you generally don’t *need* to import it — but many projects still include it for **compatibility with 3.9–3.10**.\n","\n","\n"],"metadata":{"id":"s6cOFpLCF7CT"}},{"cell_type":"markdown","source":["# **Python’s old-style string formatting syntax** (often called the **printf-style** format).\n","\n","\n","```python\n","\"%(asctime)s\"\n","```\n","\n","---\n","\n","##  General pattern\n","\n","```\n","%(<key>)<type>\n","```\n","\n","is a placeholder that says:\n","\n","> “Look up `<key>` in a dictionary and format its value according to `<type>`.”\n","\n","---\n","\n","###  The `%` sign\n","\n","* Marks the start of a **format specifier**.\n","* It tells Python: “Insert a value here when formatting.”\n","\n","Example:\n","\n","```python\n","\"Hello %s\" % \"World\"\n","# → \"Hello World\"\n","```\n","\n","---\n","\n","###  The parentheses `(asctime)`\n","\n","* When you see parentheses inside the `%`, it means **dictionary-based formatting**.\n","* The string will be formatted with values from a dictionary that has that key.\n","\n","Example:\n","\n","```python\n","\"%(name)s is %(age)d years old\" % {\"name\": \"Alice\", \"age\": 30}\n","# → \"Alice is 30 years old\"\n","```\n","\n","Here:\n","\n","* `name` and `age` are keys in the dictionary.\n","* `s` means format as string.\n","* `d` means format as integer.\n","\n","---\n","\n","###  The `s` at the end\n","\n","That’s the **type specifier** — it tells Python how to render the value.\n","\n","| Specifier | Meaning                       | Example                                 |\n","| --------- | ----------------------------- | --------------------------------------- |\n","| `%s`      | string (any object → `str()`) | `\"Hello %s\" % \"World\"`                  |\n","| `%d`      | integer                       | `\"Count: %d\" % 5`                       |\n","| `%f`      | floating point                | `\"Pi ≈ %.2f\" % 3.14159` → `\"Pi ≈ 3.14\"` |\n","| `%r`      | raw `repr()` string           | `\"%r\" % [1,2,3]` → `\"[1, 2, 3]\"`        |\n","\n","---\n","\n","###  In the logging context\n","\n","The **logging formatter** uses this same printf-style syntax.\n","The `logging` module fills in a dictionary like:\n","\n","```python\n","{\n","  \"asctime\": \"2025-10-06 09:32:47,321\",\n","  \"levelname\": \"INFO\",\n","  \"name\": \"build_db\",\n","  \"message\": \"Building database...\"\n","}\n","```\n","\n","Then applies:\n","\n","```python\n","\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\" % record_dict\n","```\n","\n","Which produces:\n","\n","```\n","2025-10-06 09:32:47,321 | INFO | build_db | Building database...\n","```\n","\n","---\n","\n","### Modern alternative (f-strings)\n","\n","Nowadays, you’d usually use **f-strings** or `.format()` for most Python code:\n","\n","```python\n","name = \"Alice\"\n","age = 30\n","f\"{name} is {age} years old\"\n","```\n","\n","But the **logging module** still uses the older `%()` style for backwards compatibility.\n","\n"],"metadata":{"id":"J85xzEUaHgHM"}},{"cell_type":"markdown","source":["#  **`repr()`**\n","\n","is a built-in function that returns the **official string representation** of an object — that is, a string that, if possible, can be used to **recreate the object** when passed to `eval()`.\n","\n","---\n","\n","###  **Definition**\n","\n","```python\n","repr(object)\n","```\n","\n","Returns a string that represents the object in a way **useful for debugging and development**.\n","\n","\n","* `repr()` is meant for **developers**.\n","* `str()` is meant for **users**.\n","\n","Think of it like this:\n","\n","| Function    | Audience  | Purpose                                       |\n","| ----------- | --------- | --------------------------------------------- |\n","| `repr(obj)` | Developer | Unambiguous, ideally `eval(repr(obj)) == obj` |\n","| `str(obj)`  | User      | Readable, meant for display                   |\n","\n","---\n","\n","###  **Examples**\n","\n","```python\n","x = 'hello\\nworld'\n","print(str(x))   # prints: hello\n","                #         world\n","print(repr(x))  # prints: 'hello\\nworld'\n","```\n","\n","Explanation:\n","\n","* `str()` shows the human-friendly version.\n","* `repr()` shows the *literal representation* (including escape characters).\n","\n","---\n","\n","###  Definig a cutome ``__repr__`\n","\n","You can define how `repr()` behaves for your own classes by implementing the **`__repr__`** method.\n","\n","```python\n","class Point:\n","    def __init__(self, x, y):\n","        self.x, self.y = x, y\n","\n","    def __repr__(self):\n","        return f\"Point({self.x}, {self.y})\"\n","\n","p = Point(2, 3)\n","print(repr(p))  # Output: Point(2, 3)\n","```\n","\n","That output is valid Python — `eval(\"Point(2, 3)\")` would recreate the same object.\n","\n","\n","* `repr()` gives a precise, debug-oriented string form of an object.\n","* Used internally by the interpreter when you type an expression in the REPL:\n","\n","  ```python\n","  >>> [1, 2, 3]\n","  [1, 2, 3]  # actually uses repr()\n","  ```\n","* If an object defines both `__str__` and `__repr__`, `print()` uses `__str__`, and the interactive shell uses `__repr__`.\n","\n"],"metadata":{"id":"MTekA4sPaM4c"}},{"cell_type":"code","source":["# save this file to scripts/logsetup.py\n","from __future__ import annotations\n","import logging, os\n","\n","def setup_logging(name: str = \"dspt\"):\n","    level = os.getenv(\"LOGLEVEL\", \"INFO\").upper()\n","    logger = logging.getLogger(name)\n","    if not logger.handlers:\n","        handler = logging.StreamHandler()\n","        fmt = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n","        handler.setFormatter(logging.Formatter(fmt))\n","        logger.addHandler(handler)\n","    logger.setLevel(level)\n","    return logger"],"metadata":{"id":"3wieXTGkBwO3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create **pytest config** and a fixture (with safe fallback data)"],"metadata":{"id":"BdCOHbg4ISyN"}},{"cell_type":"markdown","source":[" **pytest configuration and fixture setup** for a data-science project.\n","\n","```python\n","from pathlib import Path\n","Path(\"pytest.ini\").write_text(\"\"\"[pytest]\n","addopts = -q\n","testpaths = tests\n","filterwarnings =\n","    ignore::FutureWarning\n","\"\"\")\n","```\n","\n","* `[pytest]` — standard section header for Pytest config.\n","* `addopts = -q` — run tests in **quiet mode** (no verbose test names).\n","* `testpaths = tests` — look for tests inside the `tests/` directory only.\n","* `filterwarnings = ignore::FutureWarning` — suppress annoying warnings from pandas/numpy like:\n","\n","  ```\n","  FutureWarning: The frame.append method is deprecated...\n","  ```\n","\n","# Fixtures\n","\n"," `tests/conftest.py`: This file defines **shared fixtures** for all tests in your suite.\n","Pytest automatically loads `conftest.py` in any directory it finds.\n","\n","---\n","\n","###  The fixture\n","In pytest, a fixture is a reusable setup function that provides data, resources, or state to your test functions.\n","\n","```python\n","@pytest.fixture(scope=\"session\")\n","def features_df():\n","```\n","\n","Defines a reusable **fixture** named `features_df` that tests can depend on.\n","\n","\n","* `scope=\"session\"` means it’s created **once per test session**, shared by all tests → faster test runs.\n","\n","####  Example usage in a test\n","\n","```python\n","def test_columns_exist(features_df):\n","    expected = {\"ticker\", \"date\", \"adj_close\", \"log_return\", \"roll_mean_20\", \"roll_std_20\"}\n","    assert expected.issubset(features_df.columns)\n","```\n","\n","Pytest automatically injects the fixture, no import needed.\n","\n","\n"],"metadata":{"id":"eDC80tpiLdS1"}},{"cell_type":"markdown","source":["```python\n","@pytest.fixture\n","def features_df():\n","    ...\n","```\n","\n","## The decorator “@” syntax\n","\n","The `@` symbol in Python introduces a **decorator** —\n","a special shorthand for wrapping one function (or class) **inside another**.\n","\n","Conceptually, this:\n","\n","```python\n","@decorator\n","def func():\n","    ...\n","```\n","\n","is equivalent to writing:\n","\n","```python\n","def func():\n","    ...\n","func = decorator(func)\n","```\n","\n","###  What a decorator does\n","\n","A **decorator** is itself a function that:\n","\n","* Takes another function (or class) as input.\n","* Returns a *modified* or *enhanced* version of it.\n","\n","So decorators are a clean way to **add behavior** without changing the original function’s core logic.\n","\n","---\n","\n","###  Simple example\n","\n","```python\n","def announce(func):\n","    def wrapper():\n","        print(\"Starting...\")\n","        func()\n","        print(\"Done!\")\n","    return wrapper\n","```\n","\n","Now apply it:\n","\n","```python\n","@announce\n","def greet():\n","    print(\"Hello!\")\n","\n","greet()\n","```\n","\n","**Output:**\n","\n","```\n","Starting...\n","Hello!\n","Done!\n","```\n","\n","What happens internally:\n","\n","1. `greet` is passed into `announce`.\n","2. `announce` returns the `wrapper` function.\n","3. The name `greet` now refers to `wrapper`.\n","\n","---\n","\n","\n","---\n","\n","###  Other common decorators\n","\n","| Decorator                       | Meaning / Use                                                                    |\n","| ------------------------------- | -------------------------------------------------------------------------------- |\n","| `@staticmethod`                 | Define a static method inside a class                                            |\n","| `@classmethod`                  | Define a method that receives the class (`cls`) instead of the instance (`self`) |\n","| `@property`                     | Turn a method into an attribute-like property                                    |\n","| `@dataclass`                    | Auto-generate class boilerplate (`__init__`, `__repr__`, etc.)                   |\n","| `@lru_cache`                    | Cache function results for performance                                           |\n","| `@app.route(...)`               | Flask/Django-style URL binding                                                   |\n","| `@pytest.mark.parametrize(...)` | Run one test with multiple inputs                                                |\n","\n","\n"],"metadata":{"id":"8dlQMAglPY1Q"}},{"cell_type":"markdown","source":["# Shifting a Time Series\n","\n","```python\n","df[\"log_return\"].shift(-1)\n","```\n","\n","\n","`Series.shift(n)` **moves** the data *downward* or *upward* by `n` rows, filling the empty spots with `NaN`.\n","\n","| Direction                                | Argument | Meaning             |\n","| ---------------------------------------- | -------- | ------------------- |\n","| Downward (later rows get earlier values) | `n > 0`  | “look back” (lag)   |\n","| Upward (earlier rows get later values)   | `n < 0`  | “look ahead” (lead) |\n","\n","---\n","\n","###  Example\n","\n","Suppose you have:\n","\n","| index | log_return |\n","| :---- | ---------: |\n","| 0     |       0.01 |\n","| 1     |       0.02 |\n","| 2     |       0.03 |\n","\n","Then:\n","\n","```python\n","df[\"log_return\"].shift(1)\n","```\n","\n","→ moves values **down 1 row**:\n","\n","| index | shifted |\n","| :---- | ------: |\n","| 0     |     NaN |\n","| 1     |    0.01 |\n","| 2     |    0.02 |\n","\n","and\n","\n","```python\n","df[\"log_return\"].shift(-1)\n","```\n","\n","→ moves values **up 1 row**:\n","\n","| index | shifted |\n","| :---- | ------: |\n","| 0     |    0.02 |\n","| 1     |    0.03 |\n","| 2     |     NaN |\n","\n"],"metadata":{"id":"dgWws1GINfFu"}},{"cell_type":"code","source":["# pytest.ini\n","from pathlib import Path\n","Path(\"pytest.ini\").write_text(\"\"\"[pytest]\n","addopts = -q\n","testpaths = tests\n","filterwarnings =\n","    ignore::FutureWarning\n","\"\"\")\n","\n","# tests/conftest.py\n","from pathlib import Path\n","import pandas as pd, numpy as np, pytest\n","\n","def _synth_features():\n","    # minimal synthetic features for 3 tickers, 60 days\n","    rng = np.random.default_rng(0)\n","    dates = pd.bdate_range(\"2023-01-02\", periods=60)\n","    frames=[]\n","    for t in [\"AAPL\",\"MSFT\",\"GOOGL\"]:\n","        ret = rng.normal(0, 0.01, size=len(dates)).astype(\"float32\")\n","        adj = 100 * np.exp(np.cumsum(ret))\n","        df = pd.DataFrame({\n","            \"date\": dates,\n","            \"ticker\": t,\n","            \"adj_close\": adj.astype(\"float32\"),\n","            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n","        })\n","        # next-day label\n","        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n","        # rolling\n","        df[\"roll_mean_20\"] = df[\"log_return\"].rolling(20, min_periods=20).mean()\n","        df[\"roll_std_20\"]  = df[\"log_return\"].rolling(20, min_periods=20).std()\n","        df[\"zscore_20\"]    = (df[\"log_return\"]-df[\"roll_mean_20\"])/(df[\"roll_std_20\"]+1e-8)\n","        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n","        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n","        frames.append(df)\n","    out = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n","    out[\"ticker\"] = out[\"ticker\"].astype(\"category\")\n","    return out\n","\n","@pytest.fixture(scope=\"session\")\n","def features_df():\n","    p = Path(\"data/processed/features_v1.parquet\")\n","    if p.exists():\n","        df = pd.read_parquet(p)\n","        # Ensure expected minimal cols exist (compute light ones if missing)\n","        if \"weekday\" not in df: df[\"weekday\"] = pd.to_datetime(df[\"date\"]).dt.weekday.astype(\"int8\")\n","        if \"month\" not in df:   df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month.astype(\"int8\")\n","        return df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    # fallback\n","    return _synth_features().sort_values([\"ticker\",\"date\"]).reset_index(drop=True)"],"metadata":{"id":"E6RXMuG3IUEr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  4) **High‑value tests**: shapes, nulls, look‑ahead ban"],"metadata":{"id":"bCWWA37MRcdt"}},{"cell_type":"markdown","source":["### Check nulls\n","```\n","na = features_df[crit].isna().sum().to_dict()\n","```\n","\n","`.sum()`\tsums True values → counts the number of NaNs per column\n","`.to_dict()`\tconverts the Series into a dictionary, e.g. `{\"log_return\": 0, \"r_1d\": 0}`"],"metadata":{"id":"UXRPcNUYXLed"}},{"cell_type":"markdown","source":["###  `@pytest.mark.parametrize(\"W\", [20])`\n","\n","This is a **pytest parameterization decorator**.\n","\n","It means the test will run once for each value of `W` in the list.\n","Here only `W = 20`, but you could add more (e.g. `[5, 10, 20, 50]`).\n","\n","Purpose: easily test multiple rolling-window sizes without rewriting the test.\n","\n"],"metadata":{"id":"3OU9vrI1ZJru"}},{"cell_type":"code","source":["# tests/test_features.py\n","import numpy as np, pandas as pd\n","import pytest\n","\n","REQUIRED_COLS = [\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]\n","\n","def test_required_columns_present(features_df):\n","    missing = [c for c in REQUIRED_COLS if c not in features_df.columns]\n","    assert not missing, f\"Missing required columns: {missing}\"\n","\n","def test_key_no_duplicates(features_df):\n","    dup = features_df[[\"ticker\",\"date\"]].duplicated().sum()\n","    assert dup == 0, f\"Found {dup} duplicate (ticker,date) rows\"\n","\n","def test_sorted_within_ticker(features_df):\n","    for tkr, g in features_df.groupby(\"ticker\"):\n","        assert g[\"date\"].is_monotonic_increasing, f\"Dates not sorted for {tkr}\"\n","\n","def test_nulls_in_critical_columns(features_df):\n","    crit = [\"log_return\",\"r_1d\"]\n","    na = features_df[crit].isna().sum().to_dict()\n","    assert all(v == 0 for v in na.values()), f\"NAs in critical cols: {na}\"\n","\n","def test_calendar_dtypes(features_df):\n","    assert str(features_df[\"weekday\"].dtype) in (\"int8\",\"Int8\"), \"weekday should be compact int\"\n","    assert str(features_df[\"month\"].dtype)   in (\"int8\",\"Int8\"), \"month should be compact int\"\n","\n","def test_ticker_is_categorical(features_df):\n","    # allow object if reading from some parquet engines, but prefer category\n","    assert features_df[\"ticker\"].dtype.name in (\"category\",\"CategoricalDtype\",\"object\")\n","\n","def test_r1d_is_lead_of_log_return(features_df):\n","    for tkr, g in features_df.groupby(\"ticker\"):\n","        # r_1d at t equals log_return at t+1\n","        assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"Lead/lag mismatch for {tkr}\"\n","\n","@pytest.mark.parametrize(\"W\", [20])\n","def test_rolling_mean_matches_definition(features_df, W):\n","    if f\"roll_mean_{W}\" not in features_df.columns:\n","        pytest.skip(f\"roll_mean_{W} not present\")\n","    for tkr, g in features_df.groupby(\"ticker\"):\n","        s = g[\"log_return\"]\n","        rm = s.rolling(W, min_periods=W).mean()\n","        # compare only where defined\n","        mask = ~rm.isna()  # ignore teh first `W-1` rows (containing `NaN`)\n","        diff = (g[f\"roll_mean_{W}\"][mask] - rm[mask]).abs().max()\n","        assert float(diff) <= 1e-7, f\"roll_mean_{W} mismatch for {tkr} (max diff {diff})\""],"metadata":{"id":"o-HWP4l2RgHZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Skip Directive for pytest"],"metadata":{"id":"z9EifaseZine"}},{"cell_type":"markdown","source":["**skip directive** for pytest, and it’s very useful for conditionally skipping tests (or entire test modules) when a dependency isn’t available.\n","\n","```python\n","pytest.skip(\"pandera not installed\", allow_module_level=True)\n","```\n","\n","The function `pytest.skip(reason)` immediately **stops execution of the current test** (or module, if allowed)\n","and marks it as **“skipped”** in the test results rather than as a failure.\n","\n","So in your pytest report you’ll see something like:\n","\n","```\n","SKIPPED [1] test_schema_validation.py:5: pandera not installed\n","```\n","The message\n","\n","```python\n","\"pandera not installed\"\n","```\n","\n","is the **reason** displayed in the test summary —\n","so you or other developers immediately know why the test was skipped.\n","\n","---\n","\n","### The keyword `allow_module_level=True`\n","\n","Normally, you can only call `pytest.skip()` **inside** a test function or fixture.\n","If you call it **at the top level** of a module (before any tests run), pytest would raise an error.\n","\n","Setting `allow_module_level=True` tells pytest:\n","\n","> “It’s okay to skip all tests in this entire module right now.”\n","\n","So it’s used for *module-wide conditional skipping.*\n","\n","---\n","\n","##  Typical usage pattern\n","\n","You’d often see this near the top of a test file:\n","\n","```python\n","import pytest\n","\n","try:\n","    import pandera as pa\n","except ImportError:\n","    pytest.skip(\"pandera not installed\", allow_module_level=True)\n","```\n","\n","This means:\n","\n","* If `pandera` is not available in the environment,\n","* Pytest will skip the **entire test file**,\n","* Instead of crashing or failing with an `ImportError`.\n","\n","---\n","\n","###  Example test file\n","\n","```python\n","# tests/test_schema_validation.py\n","import pytest\n","\n","try:\n","    import pandera as pa\n","except ImportError:\n","    pytest.skip(\"pandera not installed\", allow_module_level=True)\n","\n","def test_schema(features_df):\n","    # this test only runs if pandera is available\n","    ...\n","```\n","\n","Output when Pandera missing:\n","\n","```\n","collected 1 item / 1 skipped\n","============================= short test summary info =============================\n","SKIPPED [1] test_schema_validation.py:5: pandera not installed\n","```\n","\n"],"metadata":{"id":"GrRKRrYXaNmO"}},{"cell_type":"markdown","source":["# Pandera Column Validation\n","`nullable`, `coerce`, and `Check.str_length(...)`\n","are key parts of how **Pandera’s `Column`** validation works.\n","The schema defines the **expected properties** of one column:\n","\n","```python\n","Column(dtype, nullable=..., coerce=..., checks=...)\n","```\n","\n","where:\n","\n","* `dtype` is the expected data type (e.g. `pa.Float`, `pa.String`, `pa.DateTime`)\n","* the other arguments control **how** the column is validated and possibly converted.\n","\n","---\n","\n","##  `nullable`\n","\n","> Whether the column is allowed to contain `NaN` (missing) values.\n","\n","| Setting          | Behavior                                      |\n","| ---------------- | --------------------------------------------- |\n","| `nullable=False` | Column must have **no nulls** (`NaN`, `None`) |\n","| `nullable=True`  | Missing values are allowed                    |\n","\n","Example:\n","\n","```python\n","Column(pa.Float, nullable=False)\n","```\n","\n","fails if any value in that column is missing.\n","\n","```python\n","Column(pa.String, nullable=True)\n","```\n","\n","passes even if some rows have `NaN` or `None`.\n","\n","---\n","\n","##  `coerce`\n","\n","**Meaning:**\n","\n","> Whether Pandera should **automatically convert** the column’s data type\n","> to match the declared type before validating.\n","\n","| Setting                  | Behavior                                            |\n","| ------------------------ | --------------------------------------------------- |\n","| `coerce=True`            | Try to cast column values to the specified dtype    |\n","| `coerce=False` (default) | Expect the column to already have the correct dtype |\n","\n","Example:\n","\n","```python\n","Column(pa.String, coerce=True)\n","```\n","\n","If your data looks like:\n","\n","```python\n","ticker\n","0   AAPL\n","1   MSFT\n","2   1234   ← numeric type, but still okay\n","```\n","\n","Pandera will cast that numeric `1234` to string `\"1234\"` automatically before checking.\n","\n","---\n","\n","##  `Check.str_length(1, 12)`\n","\n","**Meaning:**\n","\n","> Validate that each string’s length lies between 1 and 12 characters.\n","\n","It’s a **built-in Pandera check** specialized for string data.\n","\n","Example:\n","\n","```python\n","Check.str_length(min_value=1, max_value=12)\n","```\n","\n","* Passes for `\"AAPL\"` (length 4)\n","* Fails for `\"\"` (length 0)\n","* Fails for `\"VERYLONGTICKERSYMBOL\"` (length > 12)\n","\n","\n","---\n","\n","###  other common `Check` helpers\n","\n","| Check                      | Description                 |\n","| -------------------------- | --------------------------- |\n","| `Check.in_range(min, max)` | numeric range               |\n","| `Check.isin([...])`        | membership in a list or set |\n","| `Check.less_than(x)`       | all values < x              |\n","| `Check.greater_than(x)`    | all values > x              |\n","| `Check.str_matches(regex)` | regex match for strings     |\n","\n","\n"],"metadata":{"id":"YhyIUOUpx_MS"}},{"cell_type":"code","source":["# tests/test_schema_pandera.py\n","import pytest, pandas as pd, numpy as np\n","try:\n","    import pandera.pandas as pa\n","    from pandera import Column, Check, DataFrameSchema\n","except Exception:\n","    pytest.skip(\"pandera not installed\", allow_module_level=True)\n","\n","schema = pa.DataFrameSchema({\n","    \"date\":     Column(pa.DateTime, nullable=False),\n","    \"ticker\":   Column(pa.String,  nullable=False, coerce=True, checks=Check.str_length(1, 12)),\n","    \"log_return\": Column(pa.Float, nullable=False,\n","                         checks=Check(lambda s: np.isfinite(s).all(), error=\"log_return must be finite\")),\n","    \"r_1d\":       Column(pa.Float, nullable=False,\n","                         checks=Check(lambda s: np.isfinite(s).all(), error=\"r_1d must be finite\")),\n","    \"weekday\":  Column(pa.Int8, checks=Check.isin(range(7))),                 # 0..6\n","    \"month\":    Column(pa.Int8, checks=Check.in_range(1, 12, inclusive=\"both\")),\n","})\n","\n","def test_schema_validate(features_df):\n","    # Cast ticker to string for schema validation; categorical is ok → string\n","    df = features_df.copy()\n","    df[\"ticker\"] = df[\"ticker\"].astype(str)\n","    schema.validate(df[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]])"],"metadata":{"id":"reU_Ei4nZeFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6) Duplicates"],"metadata":{"id":"Ye0cr6TkzJQw"}},{"cell_type":"markdown","source":["\n","```python\n","df[[\"ticker\",\"date\"]].duplicated()\n","```\n","\n","This returns a **Boolean Series** where each row is `True` if that combination of `ticker` and `date`\n","has appeared **before** in the DataFrame.\n","\n","For example:\n","\n","| ticker | date       | duplicated                    |\n","| :----- | :--------- | :---------------------------- |\n","| AAPL   | 2024-01-02 | False                         |\n","| AAPL   | 2024-01-03 | False                         |\n","| AAPL   | 2024-01-03 | True  ← duplicate of previous |\n","| MSFT   | 2024-01-02 | False                         |\n","\n","---\n","\n","### Important details:\n","\n","* `duplicated()` marks **the second and later occurrences** of duplicates as `True`.\n","* The first appearance of a given `(ticker, date)` pair remains `False`.\n","* You can change that with `keep=False` to mark **all** duplicates, e.g.:\n","\n","  ```python\n","  df[[\"ticker\",\"date\"]].duplicated(keep=False)\n","  ```\n","\n"],"metadata":{"id":"JEj6OUBj0olU"}},{"cell_type":"markdown","source":["##  Example DataFrame\n","\n","```python\n","import pandas as pd\n","\n","df = pd.DataFrame({\n","    \"ticker\": [\"AAPL\", \"AAPL\", \"AAPL\", \"MSFT\", \"MSFT\", \"GOOGL\"],\n","    \"date\":   [\"2024-01-02\", \"2024-01-02\", \"2024-01-03\", \"2024-01-02\", \"2024-01-02\", \"2024-01-02\"],\n","    \"adj_close\": [189.5, 189.5, 190.2, 320.0, 320.0, 135.7]\n","})\n","\n","print(df)\n","```\n","\n","Output:\n","\n","```\n","  ticker        date  adj_close\n","0   AAPL  2024-01-02      189.5\n","1   AAPL  2024-01-02      189.5\n","2   AAPL  2024-01-03      190.2\n","3   MSFT  2024-01-02      320.0\n","4   MSFT  2024-01-02      320.0\n","5  GOOGL  2024-01-02      135.7\n","```\n","\n","\n","```python\n","mask = df[[\"ticker\", \"date\"]].duplicated()\n","print(mask)\n","```\n","\n","Output:\n","\n","```\n","0    False\n","1     True\n","2    False\n","3    False\n","4     True\n","5    False\n","dtype: bool\n","```\n","\n","### Explanation:\n","\n","* Row **1** is `True` because `(\"AAPL\", \"2024-01-02\")` already appeared at row 0.\n","* Row **4** is `True` because `(\"MSFT\", \"2024-01-02\")` already appeared at row 3.\n","\n","So `.duplicated()` flags the **second and later occurrences** of duplicates.\n","\n","```python\n","df[[\"ticker\",\"date\"]].duplicated().sum()\n","```\n","\n","Output:\n","\n","```\n","2\n","```\n","\n","\n","\n","## Seeing the duplicate rows themselves\n","\n","```python\n","dupes = df[df[[\"ticker\",\"date\"]].duplicated(keep=False)]\n","print(dupes)\n","```\n","\n","Output:\n","\n","```\n","  ticker        date  adj_close\n","0   AAPL  2024-01-02      189.5\n","1   AAPL  2024-01-02      189.5\n","3   MSFT  2024-01-02      320.0\n","4   MSFT  2024-01-02      320.0\n","```\n","\n","Here `keep=False` marks **all** duplicates (both first and subsequent ones).\n","\n","---\n","\n","##  Dropping duplicates (optional)\n","\n","```python\n","clean = df.drop_duplicates(subset=[\"ticker\",\"date\"], keep=\"first\")\n","print(clean)\n","```\n","\n","Output:\n","\n","```\n","  ticker        date  adj_close\n","0   AAPL  2024-01-02      189.5\n","2   AAPL  2024-01-03      190.2\n","3   MSFT  2024-01-02      320.0\n","5  GOOGL  2024-01-02      135.7\n","```\n","\n","\n"],"metadata":{"id":"ELIjOUE92MrD"}},{"cell_type":"markdown","source":["# Pytest Caplog\n","```python\n","assert any(\"duplicate\" in rec.message for rec in caplog.records)\n","```\n","\n","\n","##   what `caplog` is\n","\n","`caplog` looks like an argment, but it is a special **pytest fixture** that *captures log messages* emitted during a test.\n","It lets you inspect what your code sent to `logging` — e.g., warnings, errors, debug messages.\n","\n","Before the test starts, you set:\n","\n","```python\n","caplog.set_level(logging.WARNING)\n","```\n","\n","→ tells pytest: “Capture all log records at level WARNING or higher.”\n","\n","So, if  `check_for_duplicates(df)` function uses something like:\n","\n","```python\n","logger.warning(\"Found duplicate ticker-date pairs!\")\n","```\n","\n","then that message gets stored in `caplog.records`.\n","\n","\n","\n","| Part                         | Meaning                                                              |\n","| ---------------------------- | -------------------------------------------------------------------- |\n","| `caplog.records`             | a list of captured `LogRecord` objects emitted during this test      |\n","| `rec.message`                | the text of each log message (after formatting)                      |\n","| `\"duplicate\" in rec.message` | check whether the substring `\"duplicate\"` appears in the log message |\n","| `any(...)`                   | returns `True` if *at least one* log record contains that substring  |\n","\n","So this line asserts:\n","\n","> “At least one WARNING message was logged that contains the word ‘duplicate’.”\n","\n","---\n","\n","###  Example of what it’s testing\n","\n","If inside `check_for_duplicates` you have:\n","\n","```python\n","logger.warning(\"Found 1 duplicate rows in key columns.\")\n","```\n","\n","then during the test:\n","\n","```python\n","caplog.records[0].message == \"Found 1 duplicate rows in key columns.\"\n","```\n","\n","and the assertion passes because `\"duplicate\"` is in that string.\n","\n","---\n","\n","###  If no message was logged\n","\n","Then `caplog.records` would be empty,\n","`any(\"duplicate\" in rec.message for rec in caplog.records)` would be `False`,\n","and pytest would fail the test with:\n","\n","```\n","AssertionError: assert False\n","```\n","\n"],"metadata":{"id":"t4yhVZaU4z0X"}},{"cell_type":"code","source":["# save to tests/test_logging.py\n","import logging, pandas as pd, numpy as np, pytest\n","from scripts.logsetup import setup_logging\n","\n","def check_for_duplicates(df, logger=None):\n","    logger = logger or setup_logging(\"dspt\")\n","    dups = df[[\"ticker\",\"date\"]].duplicated().sum()\n","    if dups > 0:\n","        logger.warning(\"Found %d duplicate (ticker,date) rows\", dups)\n","    return dups\n","\n","def test_duplicate_warning(caplog):\n","    caplog.set_level(logging.WARNING)\n","    df = pd.DataFrame({\"ticker\":[\"AAPL\",\"AAPL\"], \"date\":pd.to_datetime([\"2024-01-02\",\"2024-01-02\"])})\n","    dups = check_for_duplicates(df)\n","    assert dups == 1\n","    assert any(\"duplicate\" in rec.message for rec in caplog.records)"],"metadata":{"id":"DB8OszhezLgv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pytest   tests/test_logging.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGyfx3CI2Cp8","executionInfo":{"status":"ok","timestamp":1759777617036,"user_tz":300,"elapsed":3128,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"f819dc83-9f62-4622-e704-be6eb6550490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n","\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.61s\u001b[0m\u001b[0m\n"]}]},{"cell_type":"markdown","source":["# Homework (due before Session 14)\n","\n","**Goal:** Create a **Health Check** notebook that prints key diagnostics and is easy to include in your Quarto report.\n","\n","### Part A — Build a reusable **health** module\n"],"metadata":{"id":"--hBlff17FFv"}},{"cell_type":"markdown","source":["### Python datetime\n","\n","```python\n","pd.to_datetime(df[\"date\"]).min().date()\n","```\n","\n","\n","###  `.date()`\n","\n","Converts the pandas `Timestamp` (or `datetime`) to a **plain Python `datetime.date`** object:\n","\n","```\n","datetime.date(2024, 1, 2)\n","```\n","\n","This strips the time portion (keeps only year–month–day).\n","\n","---\n","\n","##  Example\n","\n","```python\n","import pandas as pd\n","\n","df = pd.DataFrame({\n","    \"date\": [\"2024-01-03\", \"2024-01-05\", \"2024-01-02\"]\n","})\n","\n","start_date = pd.to_datetime(df[\"date\"]).min().date()\n","print(start_date)\n","```\n","\n","Output:\n","\n","```\n","2024-01-02\n","```\n","\n","and the type is:\n","\n","```python\n","type(start_date)\n","# datetime.date\n","```\n","\n"],"metadata":{"id":"3L2WLY1J8bzt"}},{"cell_type":"markdown","source":["### Conunt the number of distinct non-NA vlaues\n","```python\n","df[\"ticker\"].nunique()\n","```\n","\n","The method `.nunique()` means:\n","\n","> “Count the number of **distinct (unique)** non-NA values in this Series.”\n","\n","It’s shorthand for:\n","\n","```python\n","len(df[\"ticker\"].dropna().unique())\n","```\n","\n","---\n","\n","###  Example\n","\n","```python\n","import pandas as pd\n","\n","df = pd.DataFrame({\n","    \"ticker\": [\"AAPL\", \"MSFT\", \"AAPL\", \"GOOGL\", \"MSFT\", None]\n","})\n","\n","print(df[\"ticker\"].nunique())\n","```\n","\n","Output:\n","\n","```\n","3\n","```\n","\n","---\n","\n","## Notes\n","\n","* `NaN` / `None` values are **ignored by default**.\n","* If you want to **include** NaN in the count, you can pass `dropna=False`:\n","\n","```python\n","df[\"ticker\"].nunique(dropna=False)\n","```\n","\n","→ would return `4` in the example above (`AAPL`, `MSFT`, `GOOGL`, and NaN).\n","\n"],"metadata":{"id":"yghFitCJ9GKi"}},{"cell_type":"markdown","source":["\n","```python\n","np.nanmin(s)\n","```\n","\n","\n","`np.nanmin()` returns the **smallest finite value** in `s`, **ignoring NaN** values.\n","\n","Equivalent logic:\n","\n","```python\n","np.min(s[~np.isnan(s)])\n","```\n","\n","So it “skips over” any missing data.\n","\n","\n","\n","##  Example\n","\n","```python\n","import numpy as np\n","\n","s = np.array([3.5, np.nan, 2.1, 5.0])\n","\n","print(np.nanmin(s))\n","```\n","\n","**Output:**\n","\n","```\n","2.1\n","```\n","\n","\n","##  Comparison with similar functions\n","\n","| Function       | Behavior with NaN          | Example result for `[3.5, np.nan, 2.1]` |\n","| -------------- | -------------------------- | --------------------------------------- |\n","| `np.min()`     | **Fails** (returns NaN)    | `nan`                                   |\n","| `np.nanmin()`  | **Ignores NaN**            | `2.1`                                   |\n","| `np.nanmax()`  | Ignores NaN, finds maximum | `3.5`                                   |\n","| `np.nanmean()` | Ignores NaN, computes mean | `2.8`                                   |\n","\n","\n","\n","##  Caution\n","\n","If *all* elements are `NaN`, then:\n","\n","```python\n","np.nanmin([np.nan, np.nan])\n","```\n","\n","raises:\n","\n","```\n","ValueError: All-NaN slice encountered\n","```\n","\n"],"metadata":{"id":"ttDO4vACCDSI"}},{"cell_type":"markdown","source":["\n","\n","##  `if h.get(\"nulls\"):`\n","\n","* `h` is  a **dictionary** holding various statistics or summaries, like:\n","\n","  ```python\n","  h = {\n","      \"rows\": 3000,\n","      \"cols\": 8,\n","      \"nulls\": {\"log_return\": 5, \"r_1d\": 3, \"volume\": 0}\n","  }\n","  ```\n","* `h.get(\"nulls\")` tries to access the key `\"nulls\"`.\n","\n","  * If it exists and is **truthy** (not empty), the code inside runs.\n","  * If `\"nulls\"` is missing or an empty dict `{}`, the `if` block is skipped.\n","\n","\n","---\n","# Build lines using Python\n","\n","##  `lines += [\"\", \"## Top Null Counts\", \"\"]`\n","\n","This adds three new strings to a list called `lines`, which is accumulating lines for a Markdown file.\n","\n","It’s equivalent to appending:\n","\n","```markdown\n","(blank line)\n","## Top Null Counts\n","(blank line)\n","```\n","\n","\n","\n","---\n","\n","##  `lines += [f\"- **{k}**: {v}\" for k,v in h[\"nulls\"].items()]`\n","\n","This is a **list comprehension** that builds a bulleted list of null counts.\n","\n","\n","| Variable            | Description                                        |\n","| ------------------- | -------------------------------------------------- |\n","| `k`                 | column name                                        |\n","| `v`                 | number of missing (null) values in that column     |\n","| `f\"- **{k}**: {v}\"` | formatted Markdown line like `- **log_return**: 5` |\n","\n","Example result:\n","\n","```python\n","[\"- **log_return**: 5\", \"- **r_1d**: 3\", \"- **volume**: 0\"]\n","```\n","\n","Then `lines += ...` appends those lines to the existing list.\n","\n","---\n","\n","##  So in Markdown, the generated section would look like:\n","\n","```\n","## Top Null Counts\n","\n","- **log_return**: 5\n","- **r_1d**: 3\n","- **volume**: 0\n","```\n","\n","\n","\n"],"metadata":{"id":"HBlSKUftDa1N"}},{"cell_type":"code","source":["# save to scripts/health.py\n","from __future__ import annotations\n","import pandas as pd, numpy as np, json\n","from pathlib import Path\n","\n","def df_health(df: pd.DataFrame) -> dict:\n","    out = {}\n","    out[\"rows\"] = int(len(df))\n","    out[\"cols\"] = int(df.shape[1])\n","    out[\"date_min\"] = str(pd.to_datetime(df[\"date\"]).min().date())\n","    out[\"date_max\"] = str(pd.to_datetime(df[\"date\"]).max().date())\n","    out[\"tickers\"]  = int(df[\"ticker\"].nunique())\n","    # Null counts (top 10)\n","    na = df.isna().sum().sort_values(ascending=False)\n","    out[\"nulls\"] = na[na>0].head(10).to_dict()\n","    # Duplicates\n","    out[\"dup_key_rows\"] = int(df[[\"ticker\",\"date\"]].duplicated().sum())\n","    # Example numeric ranges for core cols\n","    for c in [x for x in [\"log_return\",\"r_1d\",\"roll_std_20\"] if x in df.columns]:\n","        s = pd.to_numeric(df[c], errors=\"coerce\")\n","        out[f\"{c}_min\"] = float(np.nanmin(s))\n","        out[f\"{c}_max\"] = float(np.nanmax(s))\n","    return out\n","\n","def write_health_report(in_parquet=\"data/processed/features_v1.parquet\",\n","                        out_json=\"reports/health.json\", out_md=\"reports/health.md\"):\n","    p = Path(in_parquet)\n","    if not p.exists():\n","        raise SystemExit(f\"Missing {in_parquet}.\")\n","    df = pd.read_parquet(p)\n","    h = df_health(df)\n","    Path(out_json).write_text(json.dumps(h, indent=2))\n","    # Render a small Markdown summary\n","    lines = [\n","        \"# Data Health Summary\",\n","        \"\",\n","        f\"- Rows: **{h['rows']}**; Cols: **{h['cols']}**; Tickers: **{h['tickers']}**\",\n","        f\"- Date range: **{h['date_min']} → {h['date_max']}**\",\n","        f\"- Duplicate (ticker,date) rows: **{h['dup_key_rows']}**\",\n","    ]\n","    if h.get(\"nulls\"):\n","        lines += [\"\", \"## Top Null Counts\", \"\"]\n","        lines += [f\"- **{k}**: {v}\" for k,v in h[\"nulls\"].items()]\n","    Path(out_md).write_text(\"\\n\".join(lines))\n","    print(\"Wrote\", out_json, \"and\", out_md)"],"metadata":{"id":"h9J_Hop662fI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python scripts/health.py"],"metadata":{"id":"C4vXnXKj7bAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part B — **Health Check notebook** (`reports/health.ipynb`)\n","\n","Create a new notebook `reports/health.ipynb` with **two cells**:\n","\n","**Cell 1 (setup):**"],"metadata":{"id":"X8utBsgWEVmk"}},{"cell_type":"code","source":["# !pip install --upgrade ipython #use the modern replacement: importlib.reload. However this may cause problem in Colab."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"63jgK5AMFZKx","executionInfo":{"status":"ok","timestamp":1759780907940,"user_tz":300,"elapsed":14857,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"72dfdf93-ac56-446e-dae6-b3dae4e2c8de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (7.34.0)\n","Collecting ipython\n","  Downloading ipython-9.6.0-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython) (4.4.2)\n","Collecting ipython-pygments-lexers (from ipython)\n","  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n","Collecting jedi>=0.16 (from ipython)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.9.0)\n","Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython) (3.0.52)\n","Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (2.19.2)\n","Collecting stack_data (from ipython)\n","  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n","Collecting traitlets>=5.13.0 (from ipython)\n","  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython) (0.8.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.14)\n","Collecting executing>=1.2.0 (from stack_data->ipython)\n","  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n","Collecting asttokens>=2.1.0 (from stack_data->ipython)\n","  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n","Collecting pure-eval (from stack_data->ipython)\n","  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n","Downloading ipython-9.6.0-py3-none-any.whl (616 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n","Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n","Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n","Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n","Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n","Installing collected packages: pure-eval, traitlets, jedi, ipython-pygments-lexers, executing, asttokens, stack_data, ipython\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.7.1\n","    Uninstalling traitlets-5.7.1:\n","      Successfully uninstalled traitlets-5.7.1\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 7.34.0\n","    Uninstalling ipython-7.34.0:\n","      Successfully uninstalled ipython-7.34.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asttokens-3.0.0 executing-2.2.1 ipython-9.6.0 ipython-pygments-lexers-1.1.1 jedi-0.19.2 pure-eval-0.2.3 stack_data-0.6.3 traitlets-5.14.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","traitlets"]},"id":"db321fd5fe4a4323a232eb4fa966460d"}},"metadata":{}}]},{"cell_type":"markdown","source":["\n","`%load_ext autoreload` is an **IPython magic command** that tells the notebook to automatically reload your Python modules (like `scripts.health`) whenever you edit them, so you don’t have to restart the kernel.\n","\n","* `%autoreload 2` means *always reload everything except excluded modules*.\n","* But the extension’s implementation (in `/usr/local/lib/python3.12/dist-packages/IPython/extensions/autoreload.py`) still calls:\n","\n","  ```python\n","  from imp import reload\n","  ```\n","\n","  which fails, since `imp` was **completely removed** in Python 3.12 (it was deprecated since 3.4).\n","\n","---\n","\n","##  Fix / Workarounds\n","\n","### Option 1 — upgrade ipython\n","\n","Newer IPython versions (≥ 8.26) already fixed this.\n","So first try upgrading IPython:\n","\n","```bash\n","!pip install --upgrade ipython\n","```\n","\n","Then restart your runtime and re-run:\n","\n","```python\n","%load_ext autoreload\n","%autoreload 2\n","```\n","\n","Works on recent versions that import `reload` from `importlib` instead of `imp`.\n","\n","---\n","\n","### Option 2 — Use the modern replacement: `importlib.reload`\n","\n","If you can’t upgrade IPython (e.g., fixed Colab image), skip the magic and use the Python API directly:\n","\n","```python\n","from importlib import reload\n","import scripts.health\n","\n","reload(scripts.health)\n","from scripts.health import write_health_report\n","write_health_report()\n","```\n","\n","This manually reloads the module after you edit it.\n","\n"],"metadata":{"id":"7swwM9MrIvbM"}},{"cell_type":"code","source":["# The following cell will work only one time in Colab after upgrading ipython. If run this cell twice, it will crash and freeze.\n","\n","# %load_ext autoreload\n","# %autoreload 2\n","# from scripts.health import write_health_report\n","# write_health_report()  # writes reports/health.json and reports/health.md"],"metadata":{"id":"DRfcwnm0Elri"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cell 2 (display in notebook):**"],"metadata":{"id":"Wunx7aC4Ek3q"}},{"cell_type":"code","source":["# Manual reload is safe.\n","from importlib import reload\n","import scripts.health\n","\n","reload(scripts.health)\n","from scripts.health import write_health_report\n","write_health_report()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_mAJBSrJZkA","executionInfo":{"status":"ok","timestamp":1759781445414,"user_tz":300,"elapsed":6287,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"1cce4fb3-83ce-4c2c-af6c-bda7a55e4e33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote reports/health.json and reports/health.md\n"]}]},{"cell_type":"code","source":["from pathlib import Path\n","print(Path(\"reports/health.md\").read_text())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQHiCf82Jvda","executionInfo":{"status":"ok","timestamp":1759781525625,"user_tz":300,"elapsed":38,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"d9a13176-0af7-4650-a0b8-0da695244d0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# Data Health Summary\n","\n","- Rows: **3975**; Cols: **18**; Tickers: **25**\n","- Date range: **2020-01-29 → 2020-09-07**\n","- Duplicate (ticker,date) rows: **0**\n"]}]},{"cell_type":"markdown","source":["### Part C — Include health output in your **Quarto report**\n","\n","In `reports/eda.qmd`, add a section:\n","\n","````markdown\n","## Data Health (auto-generated)\n","\n","```{python}\n","from pathlib import Path\n","print(Path(\"reports/health.md\").read_text())\n","```\n","````"],"metadata":{"id":"baSP1FxNKsj7"}},{"cell_type":"markdown","source":["### Part D — Add a **Makefile** target and a quick test\n","\n","**Makefile append:** (when copy to Makefile, use four spaces to indent, and then the code below will turn it into the correct tab in Colab. This is just a workaround in Colab).\n","```bash\n",".PHONY: health test\n","health: ## Generate health.json and health.md from the current features parquet\n","  python scripts/health.py\n","\n","pytest:\n","\tpytest -q\n","\n","test: pytest\n","```\n","\n"],"metadata":{"id":"PLYY2beoLZzX"}},{"cell_type":"markdown","source":["# 1) `.PHONY`\n","\n","Use **`.PHONY`**  to tell `make` that certain targets are *not files*, just commands.\n","Why? If a file named `test` or `pytest` exists, `make` would think the target is already “up to date” and skip running it. Marking targets as phony forces them to run.\n","\n","```make\n",".PHONY: health test pytest\n","```\n","\n","# 2) `test: pytest`\n","\n","In a Makefile, the syntax is:\n","\n","```\n","target: prerequisites\n","[TAB] recipe...\n","```\n","\n","So in:\n","\n","```make\n","test: pytest\n","```\n","\n","* `test` is the **target**.\n","* `pytest` is a **prerequisite** (a dependency target).\n","* Putting `pytest` **on the same line after the colon** means: “before building `test`, build `pytest`.”\n","\n","When you run `make test`, `make` first ensures the `pytest` target has run successfully; if `pytest` has its own recipe, that recipe runs. If `pytest` is also phony, it runs every time.\n","\n","Example with recipes:\n","\n","```make\n",".PHONY: test pytest\n","\n","pytest:\n","\tpytest -q\n","\n","test: pytest\n","\t@echo \"All tests completed\"\n","```\n","\n","Here, `make test` will:\n","\n","1. Run `pytest -q` (because `test` depends on `pytest`)\n","2. Then echo “All tests completed”.\n","\n","You can list **multiple prerequisites**:\n","\n","```make\n","test: lint unit integration\n","```\n","\n","…and `make` will run `lint`, `unit`, and `integration` (in some order, unless you add ordering constraints) before `test`.\n","\n","\n"],"metadata":{"id":"XJ-xhlFQStzH"}},{"cell_type":"code","source":["%%bash\n","\n","# BACK UP FIRST\n","cp Makefile Makefile.bak\n","# Replace lines that BEGIN with 4 spaces by a single tab\n","perl -i -pe 's/^\\h{4}(?=\\S)/\\t/' Makefile\n","cat Makefile"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JN2io5u7Med8","executionInfo":{"status":"ok","timestamp":1759783153626,"user_tz":300,"elapsed":34,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"a4bfd601-668a-4dea-e3b9-9c3988a56a5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# Makefile — unified-stocks\n","SHELL := /bin/bash\n",".SHELLFLAGS := -eu -o pipefail -c\n",".ONESHELL:\n","\n","\n","PY := python\n","QUARTO := quarto\n","\n","START ?= 2020-01-01\n","END   ?= 2025-08-01\n","ROLL  ?= 30\n","\n","DATA_RAW := data/raw/prices.csv\n","FEATS    := data/processed/features.parquet\n","REPORT   := docs/reports/eda.html\n","\n","# Default target\n",".DEFAULT_GOAL := help\n","\n",".PHONY: help all clean clobber qa report backup\n","\n","help: ## Show help for each target\n","\t@awk 'BEGIN {FS = \":.*##\"; printf \"Available targets:\\n\"} /^[a-zA-Z0-9_\\-]+:.*##/ {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' $(MAKEFILE_LIST)\n","\n","# all: $(DATA_RAW) $(FEATS) report backup ## Run the full pipeline and back up artifacts\n","all: $(DATA_RAW) $(FEATS) report train backup\n","\n","$(DATA_RAW): scripts/get_prices.py tickers_25.csv\n","\t$(PY) scripts/get_prices.py --tickers tickers_25.csv --start $(START) --end $(END) --out $(DATA_RAW)\n","\n","$(FEATS): scripts/build_features.py $(DATA_RAW) scripts/qa_csv.sh\n","\t# Basic QA first\n","\tscripts/qa_csv.sh $(DATA_RAW)\n","\t$(PY) scripts/build_features.py --input $(DATA_RAW) --out $(FEATS) --roll $(ROLL)\n","\n","# --- add after FEATS definition, near other targets ---\n","\n","TRAIN_METRICS := reports/baseline_metrics.json\n","\n",".PHONY: train\n","train: $(TRAIN_METRICS) ## Train toy baseline and write metrics\n","\n","$(TRAIN_METRICS): scripts/train_baseline.py $(FEATS)\n","\t$(PY) scripts/train_baseline.py --features $(FEATS) --out-metrics $(TRAIN_METRICS)\n","\n","report: $(REPORT) ## Render Quarto EDA to docs1/\n","$(REPORT): reports/eda.qmd _quarto.yml docs1/style.css\n","\t$(QUARTO) render reports/eda.qmd -P symbol:AAPL -P start_date=$(START) -P end_date=$(END) -P rolling=$(ROLL) --output-dir docs1/\n","\t@test -f $(REPORT) || (echo \"Report not generated.\" && exit 1)\n","\n","backup: ## Rsync selected artifacts to backups/<timestamp>/\n","\t./scripts/backup.sh\n","\n","clean: ## Remove intermediate artifacts (safe)\n","\trm -rf data/interim\n","\trm -rf data/processed/*.parquet || true\n","\n","clobber: clean ## Remove generated reports and backups (dangerous)\n","\trm -rf docs/reports || true\n","\trm -rf backups || true\n","\n","DB := data/prices.db\n","\n",".PHONY: db sql-report\n","db: ## Build/refresh SQLite database from CSVs\n","\tpython scripts/build_db.py --db $(DB) --tickers tickers_25.csv --prices data/raw/prices.csv\n","\n","sql-report: db ## Generate a simple SQL-driven CSV summary\n","\t$(PY) - <<-'PY'\n","\timport pandas as pd, sqlite3, os\n","\tcon = sqlite3.connect(\"data/prices.db\")\n","\tdf = pd.read_sql_query(\"\"\"\n","\tSELECT m.sector,\n","\t       COUNT(*) AS n_obs,\n","\t       AVG(ABS(p.log_return)) AS mean_abs_return\n","\tFROM prices p\n","\tJOIN meta m ON p.ticker = m.ticker\n","\tGROUP BY m.sector\n","\tORDER BY n_obs DESC;\n","\t\"\"\", con)\n","\tos.makedirs(\"reports\", exist_ok=True)\n","\tdf.to_csv(\"reports/sql_sector_summary.csv\", index=False)\n","\tprint(df.head())\n","\tcon.close()\n","\tPY\n","\n",".PHONY: prices-parquet returns-parquet\n","prices-parquet:  ## Clean raw prices and save processed Parquet(s)\n","\tpython - <<'PY'\n","\timport pandas as pd, glob, pathlib, numpy as np, re, json\n","\tfrom pathlib import Path\n","\t# (Paste the functions from the lab: standardize_columns, clean_prices, join_meta)\n","\t# Then read raw -> clean -> write parquet as in the lab\n","\tPY\n","\n","returns-parquet: ## Build returns.parquet with r_1d + calendar features\n","\tpython - <<'PY'\n","\timport pandas as pd, numpy as np\n","\tp=\"data/processed/prices.parquet\"; r=pd.read_parquet(p).sort_values([\"ticker\",\"date\"])\n","\tr[\"log_return\"]=r.groupby(\"ticker\")[\"adj_close\"].apply(lambda s: np.log(s/s.shift(1))).reset_index(level=0, drop=True)\n","\tr[\"r_1d\"]=r.groupby(\"ticker\")[\"log_return\"].shift(-1)\n","\tr[\"weekday\"]=r[\"date\"].dt.weekday.astype(\"int8\"); r[\"month\"]=r[\"date\"].dt.month.astype(\"int8\")\n","\tr[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]].to_parquet(\"data/processed/returns.parquet\", compression=\"zstd\", index=False)\n","\tprint(\"Wrote data/processed/returns.parquet\")\n","\tPY\n","\n",".PHONY: health test\n","health: ## Generate health.json and health.md from the current features parquet\n","\tpython scripts/health.py\n","\n","pytest:\n","\tpytest tests/test_logging.py -q\n","test: pytest\n","\n","\n"]}]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","make health\n","make pytest"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1Vzqm_RMjkL","executionInfo":{"status":"ok","timestamp":1759783160263,"user_tz":300,"elapsed":3212,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"6d358a37-17b6-47b2-d226-fbd3f2def132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["python scripts/health.py\n","pytest tests/test_logging.py -q\n",".                                                                        [100%]\n"]}]},{"cell_type":"markdown","source":["**Test that health files exist:**\n","\n"],"metadata":{"id":"5i1Tg016LlT8"}},{"cell_type":"code","source":["# save to tests/test_health_outputs.py\n","import os, json\n","\n","def test_health_files_exist():\n","    assert os.path.exists(\"reports/health.json\")\n","    assert os.path.exists(\"reports/health.md\")\n","    # json is valid\n","    import json\n","    json.load(open(\"reports/health.json\"))"],"metadata":{"id":"1ddnsz4ELoqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pytest tests/test_health_outputs.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOxL5rcuQehS","executionInfo":{"status":"ok","timestamp":1759783328281,"user_tz":300,"elapsed":1828,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"c728bca5-2b7a-4f84-ece9-e6d9dfd52aa3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m.\u001b[0m\u001b[32m                                                                        [100%]\u001b[0m\n","\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[0m\n"]}]},{"cell_type":"markdown","source":["\n","##  **The `-k` flag in pytest**\n","\n","`-k` lets you **run only tests whose names (or test node IDs) match a keyword expression**.\n","\n","###  Basic usage\n","\n","```bash\n","pytest -k \"duplicate\"\n","```\n","\n","Runs all tests whose names *contain* the substring `\"duplicate\"` —\n","for example:\n","\n","* `test_duplicate_warning`\n","* `test_find_duplicates`\n","\n","and skips the rest.\n","\n"],"metadata":{"id":"Z1rRd5K_db09"}},{"cell_type":"code","source":["%%bash\n","make health\n","pytest -q -k health"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rvy7uETQRGbE","executionInfo":{"status":"ok","timestamp":1759783461354,"user_tz":300,"elapsed":3203,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"2238182a-3473-454d-8c3a-e3540530a5a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["python scripts/health.py\n",".                                                                        [100%]\n"]}]}]}