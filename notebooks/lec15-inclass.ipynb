{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDObNvxWU6Nl09QQjoQtpq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.flush_and_unmount()           # ignore errors if already unmounted\n","\n","#If cannot remount, simply delete the mounted drive and then remount\n","# rm -rf /content/drive\n"],"metadata":{"id":"H-1K7L9NJHMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3b0cacda","outputId":"03b844f9-7390-4aef-ff4b-4da963dd916a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760379285800,"user_tz":300,"elapsed":6537,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Colab cell\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"01f28b78","executionInfo":{"status":"ok","timestamp":1760379289775,"user_tz":300,"elapsed":279,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[],"source":["# Adjust these two for YOUR repo\n","REPO_OWNER = \"ywanglab\"\n","REPO_NAME  = \"STAT4160\"   # e.g., unified-stocks-team1\n","BASE_DIR   = \"/content/drive/MyDrive/dspt25\"\n","CLONE_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n","REPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n","\n","# if on my office computer\n","\n","# REPO_NAME  = \"lectureNotes\"   # e.g., on my office computer\n","# BASE_DIR = r\"E:\\OneDrive - Auburn University Montgomery\\teaching\\AUM\\STAT 4160 Productivity Tools\" # on my office computer\n","# CLONE_DIR  = f\"{BASE_DIR}\\{REPO_NAME}\"\n","\n","import os, pathlib\n","pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"36277e0a","outputId":"ba9984fb-0957-492f-9bdc-2c42475c194c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760379291381,"user_tz":300,"elapsed":7,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n"]}],"source":["import os, subprocess, shutil, pathlib\n","\n","if not pathlib.Path(CLONE_DIR).exists():\n","    !git clone {REPO_URL} {CLONE_DIR}\n","else:\n","    # If the folder exists, just ensure it's a git repo and pull latest\n","    os.chdir(CLONE_DIR)\n","    # !git status\n","    # !git pull --rebase # !git pull --ff-only\n","os.chdir(CLONE_DIR)\n","print(\"Working dir:\", os.getcwd())"]},{"cell_type":"markdown","source":["## Session 15 — Framing & Metrics\n","\n","### Learning goals\n","\n","By the end of class, students can:\n","\n","1.  Specify **forecast horizon** $H$, **step** (stride), and choose between **expanding** vs **sliding** rolling‑origin evaluation with an **embargo** gap.\n","2.  Implement a **date‑based splitter** that yields `(train_idx, val_idx)` for all tickers at once.\n","3.  Compute **MAE**, **sMAPE**, **MASE** (with a proper **training‑window scale**), and aggregate **per‑ticker** and **across tickers** (macro vs micro/weighted).\n","4.  Produce a tidy CSV of baseline results to serve as your course’s ground truth.\n","\n","------------------------------------------------------------------------\n","\n","## Agenda\n","\n","-    forecasting setup — horizon $H$, step, rolling‑origin (expanding vs sliding), embargo\n","-    metrics — MAE, sMAPE, MASE; aggregation across tickers (macro vs micro/weighted)\n","-    **In‑class lab**: implement a date‑based splitter → compute naive & seasonal‑naive baselines → MAE/sMAPE/MASE per split/ticker → save reports\n","-    Wrap‑up & homework brief\n","-    Buffer\n","\n","------------------------------------------------------------------------\n","\n","\n","\n","### Framing the forecast\n","\n","-   **Target:** next‑day log return $r_{t+1}$ (you built this as `r_1d`).\n","\n","-   **Horizon** $H$: 1 business day.\n","\n","-   **Step (stride):** how far the **origin** moves forward each split (e.g., 63 trading days ≈ a quarter).\n","\n","-   **Rolling‑origin schemes**\n","\n","    -   **Expanding:** train start fixed; **train grows** over time.\n","    -   **Sliding (rolling):** fixed‑length train window **slides** forward.\n","\n","-   **Embargo:** small **gap** (e.g., 5 days) between train end and validation start to avoid adjacency leakage.\n","\n","### Metrics (scalar, easy to compare)\n","\n","-   **MAE:** $\\frac{1}{n}\\sum |y - \\hat{y}|$ — robust & interpretable.\n","\n","-   **sMAPE:** $\\frac{2}{n}\\sum \\frac{|y - \\hat{y}|}{(|y| + |\\hat{y}| + \\epsilon)}$ — scale‑free, safe for near‑zero returns with $\\epsilon$.\n","\n","-   **MASE (Mean Abs. Scaled Error) :** $\\text{MASE}=\\frac{\\text{MAE}_\\text{model}}{\\text{MAE}_\\text{naive (train)}}$ — \\<1 means better than naive.\n","\n","    -   For seasonality $s$, the **naive comparator** predicts $y_{t+1} \\approx y_{t+1-s}$ (we’ll use $s=5$ for day‑of‑week seasonality on business days).\n","    -   **Scale** is computed on the **training window only**, per ticker.\n","\n","**MASE (Mean Absolute Scaled Error)** is a scale-free accuracy metric used to evaluate forecasting models.\n","\n","$$\n","\\text{MASE} = \\frac{\\text{MAE}_{\\text{model}}}{\\text{MAE}_{\\text{naive (train)}}}\n","$$\n","\n","\n","* **(\\text{MAE}_{\\text{model}})** — the **mean absolute error** of your model’s forecasts, computed over the test (or validation) set:\n","$$\n","  \\text{MAE}*{\\text{model}} = \\frac{1}{n} \\sum*{t=1}^n |y_t - \\hat{y}_t|\n","$$\n","\n","* **(\\text{MAE}_{\\text{naive (train)}})** — the mean absolute error of a **naive forecast** (usually a “no-change” or “random walk” model) computed on the **training data**, often defined as:\n","  [\n","  \\text{MAE}*{\\text{naive (train)}} = \\frac{1}{T-1} \\sum*{t=2}^{T} |y_t - y_{t-1}|\n","  ]\n","  where (y_t) is the training series and (T) is its length.\n","\n","---\n","\n","### Interpretation\n","\n","* **MASE = 1** → your model performs *equally* to the naive model.\n","* **MASE < 1** → your model performs *better* (lower error) than the naive baseline.\n","* **MASE > 1** → your model performs *worse* than the naive baseline.\n","\n","---\n","\n","### Why MASE is useful\n","\n","* It is **scale-independent** (unlike RMSE or MAE), so it can be compared across series with different units or magnitudes.\n","* It remains **well-defined** even when data have seasonality or non-stationary variance (with proper scaling).\n","* It provides an **intuitive benchmark**: if MASE < 1, your model beats the simplest possible forecast.\n","\n","\n","\n","### Aggregation across tickers\n","\n","-   **Per‑ticker metrics** first → then aggregate.\n","-   **Macro average:** mean of per‑ticker metrics (each ticker equal weight).\n","-   **Micro/weighted:** pool all rows (or weight tickers by sample count); for MAE, pooled MAE equals sample‑count weighted average of per‑ticker MAEs.\n","\n","------------------------------------------------------------------------\n","\n","\n","\n","## MAPE — *Mean Absolute Percentage Error*\n","\n","###  Definition\n","\n","$$\n","\\text{MAPE} = \\frac{100%}{n} \\sum_{t=1}^{n}\n","\\left| \\frac{y_t - \\hat{y}_t}{y_t} \\right|\n","$$\n","\n","Where:\n","\n","* $ y_t $ = true (actual) value at time ( t )\n","* $ \\hat{y}_t $ = predicted (forecasted) value\n","* $ n $ = number of observations\n","\n","\n","* Lower is better.\n","* Example: MAPE = 5% → on average, predictions are off by 5%.\n","\n","---\n","\n","###  Example\n","\n","| Time | Actual (y_t) | Forecast (\\hat{y}_t) | Absolute % Error |\n","| ---- | ------------ | -------------------- | ---------------- |\n","| 1    | 100          | 105                  | 5%               |\n","| 2    | 200          | 180                  | 10%              |\n","| 3    | 150          | 155                  | 3.33%            |\n","\n","[\n","\\text{MAPE} = \\frac{(5 + 10 + 3.33)}{3} = 6.11%\n","]\n","\n","---\n","\n","###  Limitations of MAPE\n","\n","1. **Division by zero problem:**\n","   If any ( y_t = 0 ), MAPE is undefined (division by zero).\n","2. **Asymmetry:**\n","   Over-predictions and under-predictions are penalized **unequally** in percentage terms.\n","   (e.g., predicting 50 instead of 100 = 50% error, but predicting 200 instead of 100 = 100% error.)\n","3. **Biased for small actual values:**\n","   When actual values are near zero, MAPE can explode to very large values.\n","\n","---\n","\n","## sMAPE — *Symmetric Mean Absolute Percentage Error*\n","\n","To fix MAPE’s asymmetry, people use **sMAPE**, defined as:\n","\n","$$\n","\\text{sMAPE} =\n","\\frac{100%}{n} \\sum_{t=1}^{n}\n","\\frac{|\\hat{y}_t - y_t|}{(|y_t| + |\\hat{y}_t|)/2}\n","$$\n","\n","or equivalently:\n","$$\n","\\text{sMAPE} =\n","\\frac{200%}{n} \\sum_{t=1}^{n}\n","\\frac{|\\hat{y}_t - y_t|}{|y_t| + |\\hat{y}_t|}\n","$$\n","\n","\n","\n","* The denominator uses the *average of actual and predicted* values —\n","  making the metric **symmetric** (over- and under-prediction penalized equally).\n","* Scale is still percentage-based.\n","\n"," **Interpretation**\n","\n","* 0% = perfect forecast\n","* 100% = prediction is completely off (in typical scaling)\n","\n","---\n","\n","### Example (same data)\n","\n","| Time | Actual (y_t) | Forecast (\\hat{y}_t) | sMAPE component |    |                       |\n","| ---- | ------------ | -------------------- | --------------- | -- | --------------------- |\n","| 1    | 100          | 105                  | ( 2×            | 5  | /(100+105) = 4.88% )  |\n","| 2    | 200          | 180                  | ( 2×            | 20 | /(200+180) = 10.53% ) |\n","| 3    | 150          | 155                  | ( 2×            | 5  | /(150+155) = 3.27% )  |\n","$$\n","\\text{sMAPE} = (4.88 + 10.53 + 3.27)/3 = 6.23%\n","$$\n","\n","\n","\n","---\n","\n","## TL;DR\n","\n","| Metric    | Meaning                                                                           | Key Idea                                             |\n","| --------- | --------------------------------------------------------------------------------- | ---------------------------------------------------- |\n","| **MAPE**  | Mean Absolute Percentage Error                                                    | Average % deviation from actual values               |\n","| **sMAPE** | Symmetric MAPE                                                                    | Same idea, but symmetric and avoids division by zero |\n","| **Tip**   | Use sMAPE for production forecasting metrics — it’s fairer and numerically safer. |                                                      |\n","\n","---\n","\n","\n","\n","\n","## In‑class lab (35 min, Colab‑friendly)\n"],"metadata":{"id":"6hg3hWIddt3w"}},{"cell_type":"markdown","source":["\n","```python\n","np.r_[np.nan, np.diff(np.log(adj))]\n","```\n","\n","### In words:\n","\n","> “Take the **logarithm** of the array `adj`, compute its **first differences**, and then **prepend a NaN** so that the output has the same length as the original series.”\n","\n","Assume you have:\n","\n","```python\n","import numpy as np\n","adj = np.array([100, 105, 102, 110], dtype=float)\n","```\n","\n","\n","```python\n","np.log(adj)\n","# → [4.60517, 4.65400, 4.62497, 4.70048]\n","```\n","\n","This is common in finance — we often use **log prices** because:\n","\n","* differences of logs approximate **returns**,\n","* and logs make multiplicative changes additive.\n","\n","---\n","\n","###  `np.diff(np.log(adj))`\n","\n","Computes the **difference between consecutive elements**.\n","\n","```python\n","np.diff(np.log(adj))\n","# → [0.04883, -0.02903, 0.07551]\n","```\n","\n","Mathematically:\n","$$\n","\\text{diff}_t = \\log(\\text{adj}_t) - \\log(\\text{adj}_{t-1})\n","$$\n","which equals:\n","$$\n","\\log\\left(\\frac{\\text{adj}_t}{\\text{adj}_{t-1}}\\right)\n","$$\n","→ this is the **log return** between days (t-1) and (t).\n","\n","---\n","\n","###  `np.r_[np.nan, ...]`\n","\n","`np.r_[]` concatenates arrays row-wise.\n","\n","Here, you’re prepending a single `np.nan` (Not-a-Number) value before the differences:\n","\n","```python\n","np.r_[np.nan, np.diff(np.log(adj))]\n","# → [nan, 0.04883, -0.02903, 0.07551]\n","```\n","\n","This aligns the array with your original data length.\n","Since there’s no previous day to compute a return for the **first** element, it’s set to `NaN`.\n","\n","\n","\n","##  Equivalent longer version\n","\n","```python\n","log_prices = np.log(adj)\n","diffs = np.diff(log_prices)\n","log_returns = np.insert(diffs, 0, np.nan)\n","```\n","\n","\n","\n"],"metadata":{"id":"2xi8N4W-zM5a"}},{"cell_type":"code","source":["import os, pathlib, numpy as np, pandas as pd\n","from pathlib import Path\n","\n","# Load returns or create a tiny fallback\n","rpath = Path(\"data/processed/returns.parquet\")\n","if rpath.exists():\n","    returns = pd.read_parquet(rpath)\n","else:\n","    # Fallback synthetic returns for 5 tickers, 320 business days\n","    rng = np.random.default_rng(0)\n","    dates = pd.bdate_range(\"2022-01-03\", periods=320)\n","    frames=[]\n","    for tkr in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n","        eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n","        adj = 100*np.exp(np.cumsum(eps))\n","        df = pd.DataFrame({\n","            \"date\": dates,\n","            \"ticker\": tkr,\n","            \"adj_close\": adj.astype(\"float32\"),\n","            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n","        })\n","        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n","        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n","        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n","        frames.append(df)\n","    returns = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n","    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n","    returns.to_parquet(rpath, index=False)\n","\n","# Standardize\n","returns[\"date\"] = pd.to_datetime(returns[\"date\"])\n","returns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n","returns.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"FpduOHMkhUDt","executionInfo":{"status":"ok","timestamp":1760379327327,"user_tz":300,"elapsed":1376,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"6ae865ac-b1e9-486f-9840-fd0e05e50fcb"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        date ticker  log_return      r_1d  weekday  month\n","0 2020-01-01   AAPL         NaN  0.002987        2      1\n","1 2020-01-02   AAPL    0.002987 -0.002741        3      1\n","2 2020-01-03   AAPL   -0.002741 -0.008906        4      1\n","3 2020-01-06   AAPL   -0.008906 -0.004547        0      1\n","4 2020-01-07   AAPL   -0.004547 -0.009916        1      1"],"text/html":["\n","  <div id=\"df-1b35e1e1-85fd-4c65-9600-169c1457eb39\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>ticker</th>\n","      <th>log_return</th>\n","      <th>r_1d</th>\n","      <th>weekday</th>\n","      <th>month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-01-01</td>\n","      <td>AAPL</td>\n","      <td>NaN</td>\n","      <td>0.002987</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2020-01-02</td>\n","      <td>AAPL</td>\n","      <td>0.002987</td>\n","      <td>-0.002741</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2020-01-03</td>\n","      <td>AAPL</td>\n","      <td>-0.002741</td>\n","      <td>-0.008906</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2020-01-06</td>\n","      <td>AAPL</td>\n","      <td>-0.008906</td>\n","      <td>-0.004547</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2020-01-07</td>\n","      <td>AAPL</td>\n","      <td>-0.004547</td>\n","      <td>-0.009916</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b35e1e1-85fd-4c65-9600-169c1457eb39')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1b35e1e1-85fd-4c65-9600-169c1457eb39 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1b35e1e1-85fd-4c65-9600-169c1457eb39');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-993121e8-e559-49c1-9d2a-a14826d1d263\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-993121e8-e559-49c1-9d2a-a14826d1d263')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-993121e8-e559-49c1-9d2a-a14826d1d263 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"returns","summary":"{\n  \"name\": \"returns\",\n  \"rows\": 4500,\n  \"fields\": [\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-01 00:00:00\",\n        \"max\": \"2020-09-08 00:00:00\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"2020-01-28 00:00:00\",\n          \"2020-02-28 00:00:00\",\n          \"2020-08-03 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"INTC\",\n          \"ORCL\",\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"log_return\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4444,\n        \"samples\": [\n          -0.0007366866921074688,\n          0.020137274637818336,\n          0.01050915103405714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"r_1d\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4444,\n        \"samples\": [\n          -0.0007366866921074688,\n          0.020137274637818336,\n          0.01050915103405714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weekday\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"int8\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          8,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["len(returns[\"date\"].unique() ), returns['date'].min(), returns['date'].max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3OHDpJu3TfG","executionInfo":{"status":"ok","timestamp":1760387846186,"user_tz":300,"elapsed":17,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"71c69024-bf1f-4ef8-d094-5b6932fe373c"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(180, Timestamp('2020-01-01 00:00:00'), Timestamp('2020-09-08 00:00:00'))"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### 1) Rolling‑origin date splitter (expanding windows + embargo)"],"metadata":{"id":"r2wPuuyE1FML"}},{"cell_type":"code","source":["import numpy as np, pandas as pd\n","\n","def make_rolling_origin_splits(dates: pd.Series,\n","                               train_min=252,   # ~1y of trading days\n","                               val_size=63,     # ~1 quarter\n","                               step=63,\n","                               embargo=5):\n","    \"\"\"Return a list of (train_start, train_end, val_start, val_end) date tuples.\"\"\"\n","    u = np.array(sorted(pd.to_datetime(dates.unique())))\n","    n = len(u)\n","    splits=[]\n","    i = train_min - 1\n","    while True:\n","        if i >= n: break\n","        tr_start, tr_end = u[0], u[i] # note the start does not change, expanding training window\n","        vs_idx = i + embargo + 1\n","        ve_idx = vs_idx + val_size - 1\n","        if ve_idx >= n: break\n","        splits.append((tr_start, tr_end, u[vs_idx], u[ve_idx]))\n","        i += step\n","    return splits\n","\n","def splits_to_indices(df, split):\n","    \"\"\"Map a date split to index arrays for the full multi-ticker frame.\"\"\"\n","    a,b,c,d = split\n","    tr_idx = df.index[(df[\"date\"]>=a) & (df[\"date\"]<=b)].to_numpy()\n","    va_idx = df.index[(df[\"date\"]>=c) & (df[\"date\"]<=d)].to_numpy()\n","    # sanity: embargo => last train date < first val date\n","    assert b < c\n","    return tr_idx, va_idx\n","\n","splits = make_rolling_origin_splits(returns[\"date\"], train_min= 100, val_size=21, step=21, embargo=5)\n","len(splits), splits[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ji1vZcoqz8om","executionInfo":{"status":"ok","timestamp":1760388083923,"user_tz":300,"elapsed":18,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"f3ef1517-9665-480c-d138-91e02c44d12d"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3,\n"," [(Timestamp('2020-01-01 00:00:00'),\n","   Timestamp('2020-05-19 00:00:00'),\n","   Timestamp('2020-05-27 00:00:00'),\n","   Timestamp('2020-06-24 00:00:00')),\n","  (Timestamp('2020-01-01 00:00:00'),\n","   Timestamp('2020-06-17 00:00:00'),\n","   Timestamp('2020-06-25 00:00:00'),\n","   Timestamp('2020-07-23 00:00:00')),\n","  (Timestamp('2020-01-01 00:00:00'),\n","   Timestamp('2020-07-16 00:00:00'),\n","   Timestamp('2020-07-24 00:00:00'),\n","   Timestamp('2020-08-21 00:00:00'))])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["### 2) Metrics & baseline predictors (naive and seasonal‑naive)"],"metadata":{"id":"4Pl5L1-F1Fan"}},{"cell_type":"code","source":["from typing import Dict, Tuple\n","\n","def mae(y, yhat):\n","    y = np.asarray(y); yhat = np.asarray(yhat);\n","    return float(np.mean(np.abs(y - yhat)))\n","\n","def smape(y, yhat, eps=1e-8):\n","    y = np.asarray(y); yhat = np.asarray(yhat)\n","    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n","\n","def mase(y_true, y_pred, y_train_true, y_train_naive):\n","    # Scale = MAE of comparator (naive) on TRAIN only; add tiny epsilon\n","    scale = mae(y_train_true, y_train_naive) + 1e-12\n","    return float(mae(y_true, y_pred) / scale)\n","\n","def add_baseline_preds(df: pd.DataFrame, seasonality:int=5) -> pd.DataFrame:\n","    \"\"\"\n","    For each ticker:\n","      - naive predicts r_{t+1} ≈ log_return_t (s=1)\n","      - seasonal naive (s) predicts r_{t+1} ≈ log_return_{t+1-s}  => shift(s-1)\n","    Adds columns: yhat_naive, yhat_s{s}\n","    \"\"\"\n","    out = df.copy()\n","    # out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)  # s=1  this is not a naive predict\n","    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].shift(1)\n","    if seasonality <= 1:\n","        out[\"yhat_s\"] = out[\"yhat_naive\"]\n","    else:\n","        out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1))\n","    return out"],"metadata":{"id":"Q-9JZYX81Hv9","executionInfo":{"status":"ok","timestamp":1760387148581,"user_tz":300,"elapsed":3,"user":{"displayName":"David W.","userId":"14633192575819064045"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Apply a (possibly custom) function to a column (or multiple columns).\n","\n","\n","##  1. **Using `.apply()`**\n","\n","### **(a) On a single column**\n","\n","```python\n","df[\"col2\"] = df[\"col1\"].apply(myfunc)\n","```\n","\n","* Applies `myfunc(x)` to each element in column `\"col1\"`.\n","* Example:\n","\n","  ```python\n","  df[\"sqrt\"] = df[\"x\"].apply(lambda x: np.sqrt(x))\n","  ```\n","\n","---\n","\n","### **(b) On the whole DataFrame**\n","\n","```python\n","df.apply(myfunc)\n","```\n","\n","* Passes **each column (Series)** to `myfunc` by default.\n","* Example: sum each column\n","\n","  ```python\n","  df.apply(np.sum)\n","  ```\n","* To apply row-wise instead:\n","\n","  ```python\n","  df.apply(myfunc, axis=1)\n","  ```\n","\n","---\n","\n","##  2. **Using `.map()`** (for Series only)\n","\n","```python\n","df[\"col2\"] = df[\"col1\"].map(myfunc)\n","```\n","\n","* Works like `.apply()` but only for **Series**.\n","* Slightly faster for element-wise transformations.\n","* Can also take a **dict** or **Series** for mapping values:\n","\n","  ```python\n","  df[\"col2\"] = df[\"col1\"].map({\"A\": 1, \"B\": 2})\n","  ```\n","\n","---\n","\n","## 3. **Using `.transform()`**\n","\n","```python\n","df[\"col2\"] = df.groupby(\"group\")[\"x\"].transform(myfunc)\n","```\n","\n","* Similar to `.apply()`, but:\n","\n","  * Returns a **Series with the same index** (so it aligns with the original DataFrame).\n","  * Useful for creating **new columns** after group-wise operations.\n","* Example: standardize each group\n","\n","  ```python\n","  df[\"z\"] = df.groupby(\"ticker\")[\"x\"].transform(lambda s: (s - s.mean()) / s.std())\n","  ```\n","\n","---\n","\n","##  4. **Using `.agg()` (aggregation)**\n","\n","```python\n","df.groupby(\"ticker\")[\"x\"].agg(myfunc)\n","```\n","\n","* Used when you want **a single summary value** per group.\n","* Example:\n","\n","  ```python\n","  df.groupby(\"ticker\")[\"volume\"].agg([\"mean\", \"std\"])\n","  ```\n","* For custom aggregators:\n","\n","  ```python\n","  df.groupby(\"ticker\")[\"x\"].agg(lambda s: np.median(s[s > 0]))\n","  ```\n","\n","---\n","\n","##  5. **Using vectorized NumPy / pandas operations**\n","\n","If your function supports **vectorization**, apply it directly without loops:\n","\n","```python\n","df[\"log_return\"] = np.log(df[\"adj_close\"] / df[\"adj_close\"].shift(1))\n","```\n","\n","* Fastest and most efficient.\n","* Always prefer this when possible.\n","\n","---\n","\n","##  6. **Using `.assign()` for chaining**\n","\n","When working in a **method chain**:\n","\n","```python\n","df = (\n","    df.assign(\n","        log_return = lambda d: np.log(d[\"adj_close\"] / d[\"adj_close\"].shift(1)),\n","        zscore = lambda d: (d[\"x\"] - d[\"x\"].mean()) / d[\"x\"].std()\n","    )\n",")\n","```\n","\n","* Keeps transformations declarative and chainable.\n","\n","---\n","\n","###  Summary Table\n","\n","| Method         | Works on                    | Returns same shape? | Group-aware? | Typical Use                    |\n","| :------------- | :-------------------------- | :------------------ | :----------- | :----------------------------- |\n","| `.map()`       | Series                      | ✅                   | ❌            | elementwise mapping            |\n","| `.apply()`     | Series/DataFrame            | ❌                   | ❌            | flexible elementwise / rowwise |\n","| `.transform()` | Series (often with groupby) | ✅                   | ✅            | feature engineering per group  |\n","| `.agg()`       | Series/DataFrame (grouped)  | ❌                   | ✅            | aggregation / summarization    |\n","| Vectorized ops | Series/DataFrame            | ✅                   | ❌            | fastest arithmetic ops         |\n","| `.assign()`    | DataFrame                   | ✅                   | ❌            | method chaining                |\n","\n","---\n","\n"],"metadata":{"id":"jD8n1OETB-B8"}},{"cell_type":"markdown","source":[" `.agg()` (or `.aggregate()`) **can assign new column names** when you:\n","\n","* (a) pass a dictionary mapping old column names → new (function) names, or\n","* (b) use a list of named tuples (or lambda aliases), or\n","* (c) use keyword arguments (new in pandas ≥1.3).\n","\n","---\n","\n","###  Example DataFrame\n","\n","```python\n","import pandas as pd\n","import numpy as np\n","\n","# Simulate daily log returns for 3 tickers\n","np.random.seed(0)\n","out = pd.DataFrame({\n","    \"ticker\": np.repeat([\"AAPL\", \"MSFT\", \"TSLA\"], 5),\n","    \"date\": pd.date_range(\"2025-01-01\", periods=5).tolist() * 3,\n","    \"adj_close\": np.random.uniform(90, 110, 15) #[90, 100)\n","})\n","```\n","\n","---\n","\n","###  Vectorized operation (fastest)\n","\n","Compute log returns directly:\n","\n","```python\n","out[\"log_return\"] = np.log(out[\"adj_close\"] / out.groupby(\"ticker\")[\"adj_close\"].shift(1))\n","```\n","\n"," Vectorized, efficient, same shape as original DataFrame.\n","\n","---\n","\n","###  `.map()` — elementwise on a single column\n","\n","Add a column with absolute log returns:\n","\n","```python\n","out[\"abs_return\"] = out[\"log_return\"].map(lambda x: abs(x) if pd.notna(x) else np.nan)\n","```\n","\n"," Works per element.\n","\n","---\n","\n","###  `.apply()` — apply function to each column or row\n","\n","Suppose we want a column “rank_within_row” that ranks values across columns per row:\n","\n","```python\n","out[\"rank_within_row\"] = out[[\"adj_close\", \"abs_return\"]].apply(\n","    lambda row: row.rank().iloc[0], axis=1\n",")\n","```\n"," Row-wise apply (axis=1).\n","\n","---\n","\n","###  `.transform()` — group-wise feature creation\n","\n","Compute **z-scores** within each ticker:\n","\n","```python\n","out[\"zscore\"] = out.groupby(\"ticker\")[\"log_return\"].transform(\n","    lambda s: (s - s.mean()) / s.std()\n",")\n","```\n"," Returns same shape as original — can assign directly as new column.\n","\n","---\n","\n","###  `.agg()` — grouped aggregation (with new column names)\n","\n","Compute **mean and std** of log returns per ticker:\n","\n","```python\n","summary = (\n","    out.groupby(\"ticker\")[\"log_return\"]\n","       .agg(mean_return=(\"mean\"), std_return=(\"std\"))\n","       .reset_index()\n",")\n","```\n"," Renames columns automatically (`mean_return`, `std_return`).\n","\n","You can also use a dict:\n","\n","```python\n","summary = out.groupby(\"ticker\").agg({\n","    \"log_return\": [\"mean\", \"std\"],\n","    \"adj_close\": \"last\"\n","})\n","summary.columns = [\"mean_return\", \"std_return\", \"last_price\"]\n","summary = summary.reset_index()\n","```\n","\n"," Multi-level columns flattened and renamed.\n","\n","---\n","\n","###  Result (illustrative)\n","\n","| ticker | mean_return | std_return | last_price |\n","| :----- | ----------: | ---------: | ---------: |\n","| AAPL   |      0.0021 |     0.0183 |      101.7 |\n","| MSFT   |     -0.0044 |     0.0145 |       99.8 |\n","| TSLA   |      0.0062 |     0.0210 |      108.2 |\n","\n","---\n","\n","###  Key takeaway\n","\n","| Method         | Best for                     | Shape      | Example                                     |\n","| :------------- | :--------------------------- | :--------- | :------------------------------------------ |\n","| Vectorized     | Fast math ops                | same       | $\\log(\\text{price}*t / \\text{price}*{t-1})$ |\n","| `.map()`       | Elementwise (Series)         | same       | `abs_return = log_return.map(abs)`          |\n","| `.apply()`     | Row/column-wise custom logic | may change | rank rows                                   |\n","| `.transform()` | Group-wise new features      | same       | z-score per ticker                          |\n","| `.agg()`       | Group summaries              | reduced    | mean/std by ticker                          |\n","\n","\n"],"metadata":{"id":"TzzZzsV8DImy"}},{"cell_type":"markdown","source":["### 3) Evaluate baselines across **first 2 splits** (fast in class)"],"metadata":{"id":"u2VwD3TpOx6J"}},{"cell_type":"code","source":["# Precompute predictions over the entire frame once (safe: uses only past values via shift)\n","seasonality = 5  # business-day weekly\n","preds_all = add_baseline_preds(returns, seasonality=seasonality)\n","\n","def per_ticker_metrics(df_val, df_train, method=\"naive\") -> pd.DataFrame:\n","    \"\"\"\n","    Compute per-ticker MAE, sMAPE, MASE for the chosen method ('naive' or 's').\n","    MASE scale uses TRAIN window and the same comparator as method.\n","    \"\"\"\n","    rows=[]\n","    col = \"yhat_naive\" if method==\"naive\" else \"yhat_s\"\n","    for tkr, g in df_val.groupby(\"ticker\"): #.groupby(\"ticker\", observed=True) to suppress the warning, see the explanations below.\n","        gv = g.dropna(subset=[\"r_1d\", col])\n","        if len(gv)==0:\n","            continue\n","        # TRAIN scale (per ticker)\n","        gt = df_train[df_train[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n","        if method==\"naive\":\n","            gt_pred = gt[\"log_return\"]  # s=1\n","        else:\n","            gt_pred = gt[\"log_return\"].shift(seasonality-1)\n","        gt_clean = gt.dropna(subset=[\"r_1d\"]).copy()\n","        gt_pred = gt_pred.loc[gt_clean.index]  #align index\n","        gt_clean = gt_clean.dropna(subset=[\"r_1d\"])\n","        # Align indices\n","        y_tr = gt_clean[\"r_1d\"].to_numpy()\n","        yhat_tr_naive = gt_pred.to_numpy()\n","        # VAL metrics\n","        y = gv[\"r_1d\"].to_numpy()\n","        yhat = gv[col].to_numpy()\n","        rows.append({\n","            \"ticker\": tkr,\n","            \"n\": int(len(y)),\n","            \"mae\": mae(y,yhat),\n","            \"smape\": smape(y,yhat),\n","            \"mase\": mase(y, yhat, y_tr, yhat_tr_naive),\n","        })\n","    return pd.DataFrame(rows)\n","\n","def aggregate_across_tickers(per_ticker_df: pd.DataFrame) -> Dict[str,float]:\n","    if per_ticker_df.empty:\n","        return {\"macro_mae\":np.nan,\"macro_smape\":np.nan,\"macro_mase\":np.nan,\n","                \"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n","    # Macro = unweighted mean across tickers\n","    macro = per_ticker_df[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n","    # Micro/weighted by n (pooled)\n","    w = per_ticker_df[\"n\"].to_numpy()\n","    micro = {\n","        \"micro_mae\": float(np.average(per_ticker_df[\"mae\"], weights=w)), #weighted avg\n","        \"micro_smape\": float(np.average(per_ticker_df[\"smape\"], weights=w)),\n","        \"micro_mase\": float(np.average(per_ticker_df[\"mase\"], weights=w)),\n","    }\n","    return {f\"macro_{k}\": float(v) for k,v in macro.items()} | micro  # `|` combine two dictionarys, see explanations below\n","\n","# Run on 2 splits in class; you can expand later\n","import pathlib, json\n","pathlib.Path(\"reports\").mkdir(exist_ok=True)\n","rows=[]\n","for sid, split in enumerate(splits[:2], start=1):\n","    a,b,c,d = split\n","    tr_idx, va_idx = splits_to_indices(returns, split)\n","    tr = preds_all.loc[tr_idx].copy()\n","    va = preds_all.loc[va_idx].copy()\n","    # Per-ticker metrics for two baselines\n","    pt_naive = per_ticker_metrics(va, tr, method=\"naive\")\n","    pt_s     = per_ticker_metrics(va, tr, method=\"s\")\n","    agg_naive = aggregate_across_tickers(pt_naive)\n","    agg_s     = aggregate_across_tickers(pt_s)\n","    # Save per-split, per-ticker\n","    pt_naive.to_csv(f\"reports/baseline_naive_split{sid}.csv\", index=False)\n","    pt_s.to_csv(f\"reports/baseline_s{seasonality}_split{sid}.csv\", index=False)\n","    rows.append({\n","        \"split\": sid,\n","        \"train_range\": f\"{a.date()}→{b.date()}\",\n","        \"val_range\": f\"{c.date()}→{d.date()}\",\n","        \"method\": \"naive\", **agg_naive  # **agg_naive: dictionary expansion (see below for explanations)\n","    })\n","    rows.append({\n","        \"split\": sid,\n","        \"train_range\": f\"{a.date()}→{b.date()}\",\n","        \"val_range\": f\"{c.date()}→{d.date()}\",\n","        \"method\": f\"s{seasonality}\", **agg_s\n","    })\n","\n","summary = pd.DataFrame(rows)\n","summary.to_csv(\"reports/baselines_rollingorigin_summary.csv\", index=False)\n","summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"XS8rmTPkOKaJ","executionInfo":{"status":"ok","timestamp":1760388445562,"user_tz":300,"elapsed":615,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"7f5c3ef3-3c58-4c2b-b7ce-9833cfea6fad"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1897108748.py:24: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)  # s=1\n","/tmp/ipython-input-1897108748.py:28: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1))\n","/tmp/ipython-input-3190888044.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in df_val.groupby(\"ticker\"):\n","/tmp/ipython-input-3190888044.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in df_val.groupby(\"ticker\"):\n","/tmp/ipython-input-3190888044.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in df_val.groupby(\"ticker\"):\n","/tmp/ipython-input-3190888044.py:12: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n","  for tkr, g in df_val.groupby(\"ticker\"):\n"]},{"output_type":"execute_result","data":{"text/plain":["   split            train_range              val_range method  macro_mae  \\\n","0      1  2020-01-01→2020-05-19  2020-05-27→2020-06-24  naive   0.011265   \n","1      1  2020-01-01→2020-05-19  2020-05-27→2020-06-24     s5   0.011337   \n","2      2  2020-01-01→2020-06-17  2020-06-25→2020-07-23  naive   0.011894   \n","3      2  2020-01-01→2020-06-17  2020-06-25→2020-07-23     s5   0.011226   \n","\n","   macro_smape  macro_mase  micro_mae  micro_smape  micro_mase  \n","0     1.482772         NaN   0.011265     1.482772         NaN  \n","1     1.494447         NaN   0.011337     1.494447         NaN  \n","2     1.491683         NaN   0.011894     1.491683         NaN  \n","3     1.436448         NaN   0.011226     1.436448         NaN  "],"text/html":["\n","  <div id=\"df-98ddc9f6-385d-4899-ac22-9e7c54541544\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>split</th>\n","      <th>train_range</th>\n","      <th>val_range</th>\n","      <th>method</th>\n","      <th>macro_mae</th>\n","      <th>macro_smape</th>\n","      <th>macro_mase</th>\n","      <th>micro_mae</th>\n","      <th>micro_smape</th>\n","      <th>micro_mase</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2020-01-01→2020-05-19</td>\n","      <td>2020-05-27→2020-06-24</td>\n","      <td>naive</td>\n","      <td>0.011265</td>\n","      <td>1.482772</td>\n","      <td>NaN</td>\n","      <td>0.011265</td>\n","      <td>1.482772</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2020-01-01→2020-05-19</td>\n","      <td>2020-05-27→2020-06-24</td>\n","      <td>s5</td>\n","      <td>0.011337</td>\n","      <td>1.494447</td>\n","      <td>NaN</td>\n","      <td>0.011337</td>\n","      <td>1.494447</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2020-01-01→2020-06-17</td>\n","      <td>2020-06-25→2020-07-23</td>\n","      <td>naive</td>\n","      <td>0.011894</td>\n","      <td>1.491683</td>\n","      <td>NaN</td>\n","      <td>0.011894</td>\n","      <td>1.491683</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>2020-01-01→2020-06-17</td>\n","      <td>2020-06-25→2020-07-23</td>\n","      <td>s5</td>\n","      <td>0.011226</td>\n","      <td>1.436448</td>\n","      <td>NaN</td>\n","      <td>0.011226</td>\n","      <td>1.436448</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98ddc9f6-385d-4899-ac22-9e7c54541544')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-98ddc9f6-385d-4899-ac22-9e7c54541544 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-98ddc9f6-385d-4899-ac22-9e7c54541544');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-891b2f90-c49c-4679-af16-4a7f78049bf2\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-891b2f90-c49c-4679-af16-4a7f78049bf2')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-891b2f90-c49c-4679-af16-4a7f78049bf2 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_eddd48bc-d6a6-4cdb-b32b-d8c9bca9d10a\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_eddd48bc-d6a6-4cdb-b32b-d8c9bca9d10a button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('summary');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"summary","summary":"{\n  \"name\": \"summary\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_range\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2020-01-01\\u21922020-06-17\",\n          \"2020-01-01\\u21922020-05-19\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_range\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2020-06-25\\u21922020-07-23\",\n          \"2020-05-27\\u21922020-06-24\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"s5\",\n          \"naive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00031232011650195326,\n        \"min\": 0.011225946024060249,\n        \"max\": 0.011893901564180851,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.011336738504469394,\n          0.011225946024060249\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_smape\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027055673156581153,\n        \"min\": 1.4364477586746216,\n        \"max\": 1.4944465637207032,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.4944465637207032,\n          1.4364477586746216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_mase\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"micro_mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00031232011650195326,\n        \"min\": 0.011225946024060249,\n        \"max\": 0.011893901564180851,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"micro_smape\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027055673156581153,\n        \"min\": 1.4364477586746216,\n        \"max\": 1.4944465637207032,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"micro_mase\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["rows"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDRxGACScEEz","executionInfo":{"status":"ok","timestamp":1760390303711,"user_tz":300,"elapsed":23,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"04151b14-ef67-4b2e-b138-feba3d06e2ec"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'split': 1,\n","  'train_range': '2020-01-01→2020-05-19',\n","  'val_range': '2020-05-27→2020-06-24',\n","  'method': 'naive',\n","  'macro_mae': 0.011265403032302857,\n","  'macro_smape': 1.4827719831466675,\n","  'macro_mase': nan,\n","  'micro_mae': 0.011265403032302857,\n","  'micro_smape': 1.4827719831466675,\n","  'micro_mase': nan},\n"," {'split': 1,\n","  'train_range': '2020-01-01→2020-05-19',\n","  'val_range': '2020-05-27→2020-06-24',\n","  'method': 's5',\n","  'macro_mae': 0.011336738504469394,\n","  'macro_smape': 1.4944465637207032,\n","  'macro_mase': nan,\n","  'micro_mae': 0.011336738504469394,\n","  'micro_smape': 1.4944465637207032,\n","  'micro_mase': nan},\n"," {'split': 2,\n","  'train_range': '2020-01-01→2020-06-17',\n","  'val_range': '2020-06-25→2020-07-23',\n","  'method': 'naive',\n","  'macro_mae': 0.011893901564180851,\n","  'macro_smape': 1.491683382987976,\n","  'macro_mase': nan,\n","  'micro_mae': 0.011893901564180851,\n","  'micro_smape': 1.491683382987976,\n","  'micro_mase': nan},\n"," {'split': 2,\n","  'train_range': '2020-01-01→2020-06-17',\n","  'val_range': '2020-06-25→2020-07-23',\n","  'method': 's5',\n","  'macro_mae': 0.011225946024060249,\n","  'macro_smape': 1.4364477586746216,\n","  'macro_mase': nan,\n","  'micro_mae': 0.011225946024060249,\n","  'micro_smape': 1.4364477586746216,\n","  'micro_mase': nan}]"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### 🔍 **What the warning means**\n","\n","Whenever you call something like:\n","\n","```python\n","out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)\n","```\n","\n","or\n","\n","```python\n","for tkr, g in df_val.groupby(\"ticker\"):\n","```\n","\n","pandas internally uses:\n","\n","```python\n","observed=False\n","```\n","\n","by default — meaning:\n","\n","> If your grouping key(s) come from a *categorical column*, pandas includes **all possible category levels**, even those **not present** in the data.\n","\n","However, in a **future pandas version**, the default will become:\n","\n","```python\n","observed=True\n","```\n","\n","meaning:\n","\n","> Only the categories actually *observed* in your data will be included.\n","\n","---\n","\n","### ✅ **How to silence the warning now**\n","\n","You can explicitly set `observed` in all your `groupby()` calls.\n","\n","**Option 1 (keep current behavior — include unobserved categories):**\n","\n","```python\n","out.groupby(\"ticker\", observed=False)\n","```\n","\n","**Option 2 (adopt future behavior — include only observed categories):**\n","\n","```python\n","out.groupby(\"ticker\", observed=True)\n","```\n","\n"],"metadata":{"id":"F3-H6XiVYYx1"}},{"cell_type":"markdown","source":["\n","### ** The `|` operator (dictionary merge)**\n","\n","Since **Python 3.9**, the `|` operator merges dictionaries:\n","\n","```python\n","a | b\n","```\n","\n","is equivalent to:\n","\n","```python\n","{**a, **b}\n","```\n","\n","* Keys from `b` override those in `a` if duplicates exist.\n","* It creates a **new** merged dictionary (does not modify either input).\n","\n","So here:\n","\n","```python\n","{f\"macro_{k}\": float(v) for k,v in macro.items()} | micro\n","```\n","\n","means\n","\n","> “Return one dictionary that contains *all macro metrics* (with prefixed names) and *all micro metrics*.”\n","\n","---\n","\n","###  ** Example result**\n","\n","If\n","\n","```python\n","macro = {\"mae\": 0.014, \"smape\": 0.12, \"mase\": 0.9}\n","micro = {\"micro_mae\": 0.0138, \"micro_smape\": 0.115, \"micro_mase\": 0.92}\n","```\n","\n","then\n","\n","```python\n","{f\"macro_{k}\": float(v) for k,v in macro.items()} | micro\n","```\n","\n","returns\n","\n","```python\n","{\n","  \"macro_mae\": 0.014,\n","  \"macro_smape\": 0.12,\n","  \"macro_mase\": 0.9,\n","  \"micro_mae\": 0.0138,\n","  \"micro_smape\": 0.115,\n","  \"micro_mase\": 0.92\n","}\n","```\n"],"metadata":{"id":"EIFJXTYHehaF"}},{"cell_type":"markdown","source":["\n","\n","The `**` operator in a function call or literal like this is called **dictionary unpacking** (or **dictionary expansion**).\n","\n","It takes all the **key–value pairs** from a dictionary and inserts them as if they were written explicitly.\n","\n","---\n","\n","###  Example\n","\n","If you have:\n","\n","```python\n","agg_naive = {\n","    \"macro_mae\": 0.012,\n","    \"macro_smape\": 0.11,\n","    \"macro_mase\": 0.85\n","}\n","```\n","\n","then writing:\n","\n","```python\n","{\n","    \"split\": 1,\n","    \"method\": \"naive\",\n","    **agg_naive\n","}\n","```\n","\n","is equivalent to:\n","\n","```python\n","{\n","    \"split\": 1,\n","    \"method\": \"naive\",\n","    \"macro_mae\": 0.012,\n","    \"macro_smape\": 0.11,\n","    \"macro_mase\": 0.85\n","}\n","```\n"],"metadata":{"id":"i0jqqm69gx_P"}},{"cell_type":"markdown","source":["### 4) Quick sanity assertions (no overlap; embargo honored)"],"metadata":{"id":"LtkG5RBdVbWZ"}},{"cell_type":"code","source":["def check_no_overlap(df, split):\n","    a,b,c,d = split\n","    assert b < c, f\"Embargo violation: train_end {b} >= val_start {c}\"\n","    tr_idx, va_idx = splits_to_indices(df, split)\n","    assert set(tr_idx).isdisjoint(set(va_idx))\n","    return True\n","\n","all(check_no_overlap(returns, s) for s in splits[:2]), len(summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuS6umxaVWMq","executionInfo":{"status":"ok","timestamp":1760388543995,"user_tz":300,"elapsed":42,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"dfe3e659-cca0-4efa-d703-c41826d07342"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(True, 4)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Wrap‑up\n","\n","-   You now have a **date‑based rolling‑origin splitter** with an **embargo**, and **baseline metrics** that set a credible reference.\n","-   **MASE** uses a **training‑window naive** as scale (per ticker), so you can read “\\<1 is better than naive” at a glance.\n","-   Aggregation: report both **macro** (per‑ticker average) and **micro/weighted** (pooled).\n","\n","------------------------------------------------------------------------\n","\n","## Homework (due before Session 16)\n","\n","**Goal:** Build a small CLI to reproduce these baselines over **all splits**, then generate per‑ticker & aggregated tables.\n","\n","### Part A — Script: `scripts/baselines_eval.py`"],"metadata":{"id":"KSg7dJW-hH44"}},{"cell_type":"code","source":["# save to scripts/baselines_eval.py\n","#!/usr/bin/env python\n","from __future__ import annotations # dont interprete the type hints (see explanation below)\n","import argparse, numpy as np, pandas as pd\n","from pathlib import Path\n","\n","def mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\n","def smape(y,yhat,eps=1e-8):\n","    y = np.asarray(y); yhat = np.asarray(yhat)\n","    return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n","def mase(y_true, y_pred, y_train_true, y_train_naive):\n","    return float(mae(y_true, y_pred) / (mae(y_train_true, y_train_naive)+1e-12))\n","\n","def make_splits(dates, train_min, val_size, step, embargo):\n","    u = np.array(sorted(pd.to_datetime(dates.unique()))); n=len(u); out=[]; i=train_min-1\n","    while True:\n","        if i>=n: break\n","        a,b = u[0], u[i]; vs = i + embargo + 1; ve = vs + val_size - 1\n","        if ve>=n: break\n","        out.append((a,b,u[vs],u[ve])); i += step\n","    return out\n","\n","def add_preds(df, s):\n","    out = df.copy()\n","    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda x: x)\n","    out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda x: x.shift(s-1)) if s>1 else out[\"yhat_naive\"]\n","    return out\n","\n","def per_ticker(df_val, df_train, method, s):\n","    col = \"yhat_naive\" if method==\"naive\" else \"yhat_s\"\n","    rows=[]\n","    for tkr, g in df_val.groupby(\"ticker\"):\n","        gv = g.dropna(subset=[\"r_1d\", col])\n","        if len(gv)==0: continue\n","        gt = df_train[df_train[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n","        gt_pred = gt[\"log_return\"] if method==\"naive\" else gt[\"log_return\"].shift(s-1)\n","        gt_pred = gt_pred.loc[gt.index]\n","        y_tr = gt[\"r_1d\"].to_numpy(); yhat_tr = gt_pred.to_numpy()\n","        y = gv[\"r_1d\"].to_numpy(); yhat = gv[col].to_numpy()\n","        rows.append({\"ticker\":tkr,\"n\":int(len(y)),\n","                     \"mae\": mae(y,yhat),\n","                     \"smape\": smape(y,yhat),\n","                     \"mase\": mase(y,yhat,y_tr,yhat_tr)})\n","    return pd.DataFrame(rows)\n","\n","def agg(pt):\n","    if pt.empty: return {\"macro_mae\":np.nan,\"macro_smape\":np.nan,\"macro_mase\":np.nan,\n","                         \"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n","    macro = pt[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n","    w = pt[\"n\"].to_numpy()\n","    micro = {\n","        \"micro_mae\": float(np.average(pt[\"mae\"], weights=w)),\n","        \"micro_smape\": float(np.average(pt[\"smape\"], weights=w)),\n","        \"micro_mase\": float(np.average(pt[\"mase\"], weights=w)),\n","    }\n","    return {f\"macro_{k}\": float(v) for k,v in macro.items()} | micro\n","\n","def main():\n","    ap = argparse.ArgumentParser()\n","    ap.add_argument(\"--returns\", default=\"data/processed/returns.parquet\")\n","    ap.add_argument(\"--seasonality\", type=int, default=5)\n","    ap.add_argument(\"--train-min\", type=int, default=252)\n","    ap.add_argument(\"--val-size\", type=int, default=63)\n","    ap.add_argument(\"--step\", type=int, default=63)\n","    ap.add_argument(\"--embargo\", type=int, default=5)\n","    ap.add_argument(\"--out-summary\", default=\"reports/baselines_rollingorigin_summary.csv\")\n","    ap.add_argument(\"--out-per-ticker\", default=\"reports/baselines_per_ticker_split{sid}_{method}.csv\")\n","    args = ap.parse_args()\n","\n","    df = pd.read_parquet(args.returns).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n","    splits = make_splits(df[\"date\"], args.train_min, args.val_size, args.step, args.embargo)\n","    pred = add_preds(df, args.seasonality)\n","\n","    rows=[]\n","    for sid, (a,b,c,d) in enumerate(splits, start=1):\n","        tr = pred[(pred[\"date\"]>=a)&(pred[\"date\"]<=b)]\n","        va = pred[(pred[\"date\"]>=c)&(pred[\"date\"]<=d)]\n","        for method in [\"naive\",\"s\"]:\n","            pt = per_ticker(va, tr, method, args.seasonality)\n","            Path(\"reports\").mkdir(exist_ok=True)\n","            pt.to_csv(args.out_per_ticker.format(sid=sid, method=method), index=False)\n","            rows.append({\"split\":sid,\"train_range\":f\"{a.date()}→{b.date()}\",\"val_range\":f\"{c.date()}→{d.date()}\",\n","                         \"method\":\"naive\" if method==\"naive\" else f\"s{args.seasonality}\", **agg(pt)})\n","    pd.DataFrame(rows).to_csv(args.out_summary, index=False)\n","    print(\"Wrote\", args.out_summary, \"and per-ticker CSVs.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"8an33hu_hR_X","executionInfo":{"status":"error","timestamp":1760391671805,"user_tz":300,"elapsed":103,"user":{"displayName":"David W.","userId":"14633192575819064045"}},"outputId":"bb1494fc-af79-4bd8-a726-dd904262eb61"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: colab_kernel_launcher.py [-h] [--returns RETURNS]\n","                                [--seasonality SEASONALITY]\n","                                [--train-min TRAIN_MIN] [--val-size VAL_SIZE]\n","                                [--step STEP] [--embargo EMBARGO]\n","                                [--out-summary OUT_SUMMARY]\n","                                [--out-per-ticker OUT_PER_TICKER]\n","colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e08a4e7c-6650-4e90-8435-36b56d395f4c.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"2","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"markdown","source":["```python\n","from __future__ import annotations\n","```\n","\n","Normally, in Python (before version 3.10), type hints are **evaluated immediately** when the function or class is defined.\n","That means if you refer to a class that hasn’t been defined yet, you’d get an error.\n","\n","Example (without this import):\n","\n","```python\n","class Node:\n","    def __init__(self, next: Node | None):  # ❌ NameError: Node not defined yet\n","        self.next = next\n","```\n","\n","---\n","\n","### 🔹 2. With `from __future__ import annotations`\n","\n","Python **does not evaluate** the type hints immediately.\n","Instead, it stores them as **plain strings** (unevaluated).\n","So forward references like `\"Node\"` are automatically deferred.\n","\n","```python\n","from __future__ import annotations\n","\n","class Node:\n","    def __init__(self, next: Node | None):  # ✅ Works fine\n","        self.next = next\n","```\n","\n","Internally, the annotation is stored as the string `\"Node | None\"`,\n","and Python (or tools like `mypy`, `pylance`, etc.) can later resolve it when needed.\n","\n"],"metadata":{"id":"8ktG52a1ikLa"}}]}