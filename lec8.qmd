---
title: "Session 8 — SQL II: Window Functions & `pandas.read_sql` Workflows"
---

Below is a complete lecture package for **Session 8 --- SQL II: Window Functions & `pandas.read_sql` Workflows** (75 minutes). It includes: a timed agenda, slides/talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. Today you'll compute **lags, leads, rolling stats, and top‑k queries** in SQLite using **window functions**, then pull results into pandas for downstream use.

> **Assumptions:**
>
> -   You're in the same Drive‑mounted repo (e.g., `unified-stocks-teamX`).
> -   You have `data/prices.db` from Session 7. If not, the lab includes a small fallback to build it from `data/raw/prices.csv`.
> -   Educational use only --- not trading advice.

------------------------------------------------------------------------

## Session 8 --- SQL II: Window Functions & `pandas.read_sql` (75 min)

### Learning goals

By the end of class, students can:

1.  Explain and use **window functions**: `LAG`, `LEAD`, `ROW_NUMBER`, and aggregates with `OVER (PARTITION BY … ORDER BY … ROWS …)`.
2.  Compute **rolling means/variance** and **multi‑lag features** per ticker **without leakage**.
3.  Use `WINDOW` named frames to avoid repetition and errors.
4.  Run parameterized SQL from Python with `pandas.read_sql_query`, and optionally register a custom SQL function (e.g., `SQRT`).
5.  Evaluate query performance basics with `EXPLAIN QUERY PLAN`.

------------------------------------------------------------------------

## Agenda (75 min)

-   **(10 min)** Window functions: mental model & syntax (`PARTITION BY`, `ORDER BY`, `ROWS` frames)
-   **(10 min)** Patterns for time series: lags, leads, rolling stats, top‑k per group
-   **(35 min)** **In‑class lab** (Colab): lags/leads → rolling mean/variance → z‑scores → top days per ticker → pull into pandas and save features
-   **(10 min)** Wrap‑up, performance notes (`EXPLAIN QUERY PLAN`), homework briefing
-   **(10 min)** Buffer

------------------------------------------------------------------------

## Slides / talking points

**What's a window?**

-   A **window** lets an aggregate or analytic function see a **row + its neighbors** without collapsing rows.

-   Template:

    ``` sql
    func(expr) OVER (
       PARTITION BY key
       ORDER BY time
       ROWS BETWEEN N PRECEDING AND CURRENT ROW
    )
    ```

-   **`PARTITION BY`** = per‑group window; **`ORDER BY`** = sequence; **`ROWS`** = how many rows to include.

**Window vs GROUP BY**

-   `GROUP BY` returns **one row per group**; `OVER (…)` returns **one row per input row** with extra columns.

**Time‑series patterns**

-   **Lags:** `LAG(x, k)` → previous `k` rows ⇒ features at *t* that use only info ≤ *t*.
-   **Leads:** `LEAD(x, k)` → future `k` rows ⇒ *labels* (don't leak into features).
-   **Rolling stats:** `AVG(x) OVER (… ROWS BETWEEN w-1 PRECEDING AND CURRENT ROW)`; rolling variance via `AVG(x*x) - AVG(x)^2`.
-   **Top‑k per group:** compute `ROW_NUMBER() OVER (PARTITION BY key ORDER BY score DESC)` and filter `WHERE rn<=k`.

**ROWS vs RANGE**

-   Use **`ROWS`** for fixed‑length windows on ordered rows (what we need).
-   **Time‑based windows** (e.g., last 30 calendar days) require different techniques in SQLite (correlated subquery); we'll note but not use today.

------------------------------------------------------------------------

## In‑class lab (35 min)

> Run each block as its own Colab cell. Adjust `REPO_OWNER/REPO_NAME` first.

### 0) Mount Drive, open DB, and ensure it exists

``` python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_OWNER = "YOUR_GITHUB_USERNAME_OR_ORG"     # <- change
REPO_NAME  = "unified-stocks-teamX"            # <- change
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, sqlite3, pandas as pd, numpy as np, math, textwrap
pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)
assert pathlib.Path(REPO_DIR).exists(), "Repo not found; clone it first."
os.chdir(REPO_DIR)
print("Working dir:", os.getcwd())

# Ensure DB exists (fallback: build from CSV)
db_path = pathlib.Path("data/prices.db")
if not db_path.exists():
    print("prices.db not found; attempting minimal build from data/raw/prices.csv …")
    pathlib.Path("data").mkdir(exist_ok=True)
    con = sqlite3.connect(db_path)
    con.execute("PRAGMA foreign_keys = ON;")
    con.executescript("""
    CREATE TABLE IF NOT EXISTS meta (
      ticker TEXT PRIMARY KEY,
      name   TEXT,
      sector TEXT NOT NULL
    );
    CREATE TABLE IF NOT EXISTS prices (
      ticker     TEXT NOT NULL,
      date       TEXT NOT NULL,
      adj_close  REAL NOT NULL CHECK (adj_close >= 0),
      volume     INTEGER NOT NULL CHECK (volume >= 0),
      log_return REAL NOT NULL,
      PRIMARY KEY (ticker,date),
      FOREIGN KEY (ticker) REFERENCES meta(ticker)
    );
    CREATE INDEX IF NOT EXISTS idx_prices_date ON prices(date);
    """)
    # Minimal meta from tickers_25 or from CSV
    if pathlib.Path("tickers_25.csv").exists():
        tks = pd.read_csv("tickers_25.csv")["ticker"].dropna().unique().tolist()
    else:
        raw = pd.read_csv("data/raw/prices.csv")
        tks = raw["ticker"].dropna().unique().tolist()
    meta = pd.DataFrame({"ticker": tks, "name": tks, "sector": ["Unknown"]*len(tks)})
    con.executemany("INSERT OR IGNORE INTO meta(ticker,name,sector) VALUES(?,?,?)",
                    meta.itertuples(index=False, name=None))
    # Load prices.csv if present; otherwise synthesize small sample
    if pathlib.Path("data/raw/prices.csv").exists():
        df = pd.read_csv("data/raw/prices.csv", parse_dates=["date"]).copy()
    else:
        dates = pd.bdate_range("2022-01-03", periods=90)
        rng = np.random.default_rng(7)
        frames=[]
        for t in tks[:5]:
            r = rng.normal(0, 0.01, len(dates))
            price = 100*np.exp(np.cumsum(r))
            vol = rng.integers(1e5, 5e6, len(dates))
            frames.append(pd.DataFrame({"ticker": t, "date": dates,
                                        "adj_close": price, "volume": vol}))
        df = pd.concat(frames, ignore_index=True)
        df["log_return"] = np.log(df["adj_close"]).diff().fillna(0)
    df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
    df = df[["ticker","date","adj_close","volume","log_return"]].drop_duplicates(["ticker","date"])
    con.executemany("INSERT OR REPLACE INTO prices(ticker,date,adj_close,volume,log_return) VALUES(?,?,?,?,?)",
                    df.itertuples(index=False, name=None))
    con.commit()
    con.close()

# Connect and register SQRT (SQLite lacks STDDEV; we’ll compute var and take sqrt)
con = sqlite3.connect(db_path)
con.create_function("SQRT", 1, lambda x: math.sqrt(x) if x is not None and x>=0 else None)
print("SQLite version:", sqlite3.sqlite_version)
```

### 1) LAG & LEAD (no leakage in features)

``` python
import pandas as pd

sql = """
SELECT ticker, date,
       log_return AS r,
       LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1,
       LAG(log_return,2) OVER (PARTITION BY ticker ORDER BY date) AS lag2,
       LEAD(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS r_tplus1  -- label candidate
FROM prices
WHERE date BETWEEN ? AND ?
ORDER BY ticker, date
LIMIT 20;
"""
df = pd.read_sql_query(sql, con, params=["2022-03-01","2022-06-30"])
df.head(10)
```

> **Teaching notes:**
>
> -   `lag1/lag2` are **safe features at time *t*** (depend on ≤ *t-1*).
> -   `r_tplus1` is a **label**; never include in features.

### 2) Named `WINDOW` + rolling mean/variance (20‑row window)

``` python
sql = """
SELECT
  ticker, date, log_return AS r,
  AVG(log_return) OVER w AS roll_mean_20,
  AVG(log_return*log_return) OVER w
    - (AVG(log_return) OVER w)*(AVG(log_return) OVER w) AS roll_var_20
FROM prices
WINDOW w AS (
  PARTITION BY ticker
  ORDER BY date
  ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
)
WHERE date BETWEEN ? AND ?
ORDER BY ticker, date
LIMIT 20;
"""
roll = pd.read_sql_query(sql, con, params=["2022-03-01","2022-06-30"])
roll.head(10)
```

Compute rolling **std** and **z‑score** in pandas (since we registered `SQRT`, we could also do it in SQL; here we'll do it in pandas for clarity):

``` python
roll["roll_std_20"] = (roll["roll_var_20"].clip(lower=0)).pow(0.5)
roll["zscore_20"] = (roll["r"] - roll["roll_mean_20"]) / roll["roll_std_20"].replace(0, pd.NA)
roll.head(5)
```

### 3) Top‑k absolute moves **per ticker** with `ROW_NUMBER`

``` python
sql = """
WITH ranked AS (
  SELECT
    ticker, date, log_return,
    ABS(log_return) AS abs_move,
    ROW_NUMBER() OVER (
      PARTITION BY ticker
      ORDER BY ABS(log_return) DESC
    ) AS rn
  FROM prices
  WHERE date BETWEEN ? AND ?
)
SELECT * FROM ranked WHERE rn <= 3
ORDER BY ticker, rn;
"""
topk = pd.read_sql_query(sql, con, params=["2022-01-01","2025-08-01"])
topk.head(15)
```

### 4) Build a **features** DataFrame directly from SQL and save to Parquet

We'll assemble lags and rolling stats in one query using a named window `w20`:

``` python
sql = """
SELECT
  ticker, date,
  log_return AS r_1d,
  LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1,
  LAG(log_return,2) OVER (PARTITION BY ticker ORDER BY date) AS lag2,
  LAG(log_return,3) OVER (PARTITION BY ticker ORDER BY date) AS lag3,
  AVG(log_return) OVER w20 AS roll_mean_20,
  AVG(log_return*log_return) OVER w20
    - (AVG(log_return) OVER w20)*(AVG(log_return) OVER w20) AS roll_var_20
FROM prices
WINDOW w20 AS (
  PARTITION BY ticker
  ORDER BY date
  ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
)
WHERE date BETWEEN ? AND ?
ORDER BY ticker, date;
"""
features_sql = pd.read_sql_query(sql, con, params=["2019-01-01","2025-08-01"])
features_sql["roll_std_20"] = (features_sql["roll_var_20"].clip(lower=0)).pow(0.5)
features_sql["zscore_20"] = (features_sql["r_1d"] - features_sql["roll_mean_20"]) / features_sql["roll_std_20"].replace(0, pd.NA)

# Drop first 2–3 rows per ticker where lags are null
features_sql = (features_sql
                .sort_values(["ticker","date"])
                .groupby("ticker", group_keys=False)
                .apply(lambda g: g.iloc[3:]))

# Save
pathlib.Path("data/processed").mkdir(parents=True, exist_ok=True)
features_sql.to_parquet("data/processed/features_sql.parquet", index=False)
features_sql.head()
```

### 5) `EXPLAIN QUERY PLAN` sanity & index usage

``` python
plan = pd.read_sql_query("""
EXPLAIN QUERY PLAN
SELECT ticker, date,
       LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1
FROM prices
WHERE date BETWEEN ? AND ?
ORDER BY ticker, date;
""", con, params=["2022-01-01","2025-08-01"])
plan
```

> **Interpretation tip:** You should see use of the `idx_prices_date` or `PRIMARY KEY` (depending on the optimizer). If you often filter by `(ticker, date)`, consider a composite index: `CREATE INDEX IF NOT EXISTS idx_prices_ticker_date ON prices(ticker, date);` (We'll include that in homework.)

### 6) Save lab outputs

``` python
pathlib.Path("reports").mkdir(exist_ok=True)
features_sql.head(100).to_csv("reports/sql_window_demo.csv", index=False)
topk.to_csv("reports/top3_abs_moves_per_ticker.csv", index=False)
print("Wrote reports/sql_window_demo.csv and reports/top3_abs_moves_per_ticker.csv")
con.close()
```

------------------------------------------------------------------------

## Wrap‑up (10 min)

-   Window functions = **per‑row context** (lags, rolling stats, top‑k per group) with **no row collapse**.
-   Prefer **`ROWS BETWEEN N PRECEDING AND CURRENT ROW`** to express true rolling windows.
-   **No leakage**: only use `LAG` for features; `LEAD` is for labels.
-   Use `pandas.read_sql_query` to **push computation into SQL** and bring back tidy frames.
-   Indexes matter; check `EXPLAIN QUERY PLAN`, and add `(ticker, date)` index when filtering by both.

------------------------------------------------------------------------

## Homework (due before Session 9)

**Goal:** Productionize SQL‑side feature engineering and performance basics. You will (A) create a reusable SQL file that defines features using windows, (B) write a small Python runner that writes **`data/processed/features_sql.parquet`**, (C) add a composite index, and (D) produce two small reports.

### Part A --- `sql/features_window.sql` (reusable)

Create `sql/features_window.sql`:

``` sql
-- sql/features_window.sql
-- Rolling features and lags built with window functions.
WITH base AS (
  SELECT
    ticker, date, log_return AS r_1d
  FROM prices
  WHERE date BETWEEN ? AND ?  -- placeholders: start, end
)
SELECT
  ticker, date, r_1d,
  LAG(r_1d,1) OVER (PARTITION BY ticker ORDER BY date) AS lag1,
  LAG(r_1d,2) OVER (PARTITION BY ticker ORDER BY date) AS lag2,
  LAG(r_1d,3) OVER (PARTITION BY ticker ORDER BY date) AS lag3,
  AVG(r_1d) OVER w20 AS roll_mean_20,
  AVG(r_1d*r_1d) OVER w20 - (AVG(r_1d) OVER w20)*(AVG(r_1d) OVER w20) AS roll_var_20
FROM base
WINDOW w20 AS (
  PARTITION BY ticker
  ORDER BY date
  ROWS BETWEEN 19 PRECEDING AND CURRENT ROW
)
ORDER BY ticker, date;
```

### Part B --- Runner: `scripts/build_features_sql.py`

``` python
# scripts/build_features_sql.py
#!/usr/bin/env python
import argparse, sqlite3, pandas as pd, numpy as np, math
from pathlib import Path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--db", default="data/prices.db")
    ap.add_argument("--sqlfile", default="sql/features_window.sql")
    ap.add_argument("--start", default="2019-01-01")
    ap.add_argument("--end",   default="2025-08-01")
    ap.add_argument("--out",   default="data/processed/features_sql.parquet")
    ap.add_argument("--drop-head", type=int, default=3, help="Drop first N rows per ticker (due to lags)")
    args = ap.parse_args()

    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    con = sqlite3.connect(args.db)
    con.create_function("SQRT", 1, lambda x: math.sqrt(x) if x is not None and x>=0 else None)

    sql = Path(args.sqlfile).read_text()
    df = pd.read_sql_query(sql, con, params=[args.start, args.end])

    # Finish std & z-score in pandas
    df["roll_std_20"] = (df["roll_var_20"].clip(lower=0)).pow(0.5)
    df["zscore_20"] = (df["r_1d"] - df["roll_mean_20"]) / df["roll_std_20"].replace(0, pd.NA)

    # Drop first N rows per ticker where lags are NaN
    df = (df.sort_values(["ticker","date"])
            .groupby("ticker", group_keys=False)
            .apply(lambda g: g.iloc[args.drop_head:]))

    df.to_parquet(args.out, index=False)
    print("Wrote", args.out, "rows:", len(df))

if __name__ == "__main__":
    main()
```

Make it executable:

``` python
import os, stat, pathlib
p = pathlib.Path("scripts/build_features_sql.py")
os.chmod(p, os.stat(p).st_mode | stat.S_IEXEC)
print("Ready:", p)
```

### Part C --- Add a composite index and verify plan

1.  Create `sql/add_index_ticker_date.sql`:

``` sql
-- sql/add_index_ticker_date.sql
CREATE INDEX IF NOT EXISTS idx_prices_ticker_date ON prices(ticker, date);
```

2.  Runner to apply it (or just execute once in a notebook):

``` python
import sqlite3, pathlib
con = sqlite3.connect("data/prices.db")
con.executescript(pathlib.Path("sql/add_index_ticker_date.sql").read_text())
con.close()
print("Index created: idx_prices_ticker_date")
```

3.  Capture the query plan to a text file:

``` python
import sqlite3, pandas as pd, pathlib
con = sqlite3.connect("data/prices.db")
plan = pd.read_sql_query("""
EXPLAIN QUERY PLAN
SELECT ticker, date, LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date)
FROM prices
WHERE ticker = ? AND date BETWEEN ? AND ?
ORDER BY date;
""", con, params=["AAPL","2022-01-01","2025-08-01"])
pathlib.Path("reports").mkdir(exist_ok=True)
plan.to_csv("reports/query_plan_lag1.csv", index=False)
con.close()
print("Wrote reports/query_plan_lag1.csv")
```

### Part D --- Produce two small reports

1.  **Top‑k per ticker** (k=5) as CSV:

``` python
import sqlite3, pandas as pd, pathlib
con = sqlite3.connect("data/prices.db")
sql = """
WITH ranked AS (
  SELECT ticker, date, log_return, ABS(log_return) AS abs_move,
         ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY ABS(log_return) DESC) AS rn
  FROM prices
  WHERE date BETWEEN ? AND ?
)
SELECT * FROM ranked WHERE rn <= 5 ORDER BY ticker, rn;
"""
df = pd.read_sql_query(sql, con, params=["2019-01-01","2025-08-01"])
pathlib.Path("reports").mkdir(exist_ok=True)
df.to_csv("reports/top5_abs_moves_per_ticker.csv", index=False)
con.close()
print("Wrote reports/top5_abs_moves_per_ticker.csv")
```

2.  **Features via SQL** saved to Parquet:

``` python
!python scripts/build_features_sql.py --start 2019-01-01 --end 2025-08-01 --out data/processed/features_sql.parquet
```

### Part E --- Makefile targets (optional but recommended)

Append to your `Makefile`:

``` make
DB := data/prices.db
FEATS_SQL := data/processed/features_sql.parquet

.PHONY: features-sql add-index plan
features-sql: $(FEATS_SQL) ## Build features using SQL windows
$(FEATS_SQL): scripts/build_features_sql.py sql/features_window.sql $(DB)
\tpython scripts/build_features_sql.py --db $(DB) --sqlfile sql/features_window.sql --start $(START) --end $(END) --out $(FEATS_SQL)

add-index: ## Create composite (ticker,date) index
\tsqlite3 $(DB) < sql/add_index_ticker_date.sql

plan: ## Save a sample EXPLAIN QUERY PLAN to reports/
\tpython - << 'PY'
import sqlite3, pandas as pd, os
con = sqlite3.connect("data/prices.db")
df = pd.read_sql_query(\"\"\"\nEXPLAIN QUERY PLAN\nSELECT ticker, date, LAG(log_return,1) OVER (PARTITION BY ticker ORDER BY date)\nFROM prices WHERE ticker=? AND date BETWEEN ? AND ? ORDER BY date;\n\"\"\", con, params=["AAPL","2022-01-01","2025-08-01"])
os.makedirs("reports", exist_ok=True)
df.to_csv("reports/query_plan_lag1.csv", index=False)
con.close()
print("Wrote reports/query_plan_lag1.csv")
PY
```

Run:

``` bash
%%bash
set -euo pipefail
cd "/content/drive/MyDrive/dspt25/unified-stocks-teamX"
make add-index
make features-sql
make plan
```

### Submission checklist (pass/revise)

-   `sql/features_window.sql` present; uses `WINDOW w20` and `LAG` for 1--3 lags.
-   `scripts/build_features_sql.py` runs and writes `data/processed/features_sql.parquet`.
-   Composite index created (`idx_prices_ticker_date`).
-   `reports/top5_abs_moves_per_ticker.csv` and `reports/query_plan_lag1.csv` generated.
-   (Optional) `Makefile` updated with `features-sql`, `add-index`, `plan`.

------------------------------------------------------------------------

## Instructor checklist (before class)

-   Test the in‑class lab once in a fresh Colab runtime; ensure SQLite version ≥ 3.25 (window functions).
-   If a student's runtime is older (rare), advise upgrading Colab or running the Python fallback path.
-   Keep one slide showing the difference between **`ROWS BETWEEN 19 PRECEDING AND CURRENT ROW`** and off‑by‑one mistakes.

## Emphasize while teaching

-   **No leakage**: features must come from **`LAG`**, not `LEAD`.
-   Rolling stats with windows are **declarative and fast**; avoid reinventing in pandas unless needed.
-   Use `WINDOW` names to reduce errors and duplication.
-   Check query plans and add indexes purposefully.

**Next up (Session 9):** Finance‑specific evaluation & leakage control --- walk‑forward splits, embargo, and regime‑aware error analysis.
