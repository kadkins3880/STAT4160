---
title: "Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)"
---

Below is a complete lecture package for **Session 15 — Framing & Metrics (Rolling‑Origin Evaluation)** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. You’ll formalize the forecasting problem (horizon/step), implement a **rolling‑origin splitter** (a.k.a. walk‑forward), and evaluate **naive** and **seasonal‑naive** baselines with **MAE, sMAPE, MASE**, aggregated **across tickers** (macro vs micro/weighted).

> **Educational use only — not trading advice.**
> Assumes your repo in Drive (e.g., `unified-stocks-teamX`) and `data/processed/returns.parquet` from Session 9. If missing, the lab creates a small fallback.

---

## Session 15 — Framing & Metrics (75 min)

### Learning goals

By the end of class, students can:

1. Specify **forecast horizon** $H$, **step** (stride), and choose between **expanding** vs **sliding** rolling‑origin evaluation with an **embargo** gap.
2. Implement a **date‑based splitter** that yields `(train_idx, val_idx)` for all tickers at once.
3. Compute **MAE**, **sMAPE**, **MASE** (with a proper **training‑window scale**), and aggregate **per‑ticker** and **across tickers** (macro vs micro/weighted).
4. Produce a tidy CSV of baseline results to serve as your course’s ground truth.

---

## Agenda (75 min)

* **(10 min)** Slides: forecasting setup — horizon $H$, step, rolling‑origin (expanding vs sliding), embargo
* **(10 min)** Slides: metrics — MAE, sMAPE, MASE; aggregation across tickers (macro vs micro/weighted)
* **(35 min)** **In‑class lab**: implement a date‑based splitter → compute naive & seasonal‑naive baselines → MAE/sMAPE/MASE per split/ticker → save reports
* **(10 min)** Wrap‑up & homework brief
* **(10 min)** Buffer

---

## Slides / talking points (add these bullets to your deck)

### Framing the forecast

* **Target:** next‑day log return $r_{t+1}$ (you built this as `r_1d`).
* **Horizon $H$:** 1 business day.
* **Step (stride):** how far the **origin** moves forward each split (e.g., 63 trading days ≈ a quarter).
* **Rolling‑origin schemes**

  * **Expanding:** train start fixed; **train grows** over time.
  * **Sliding (rolling):** fixed‑length train window **slides** forward.
* **Embargo:** small **gap** (e.g., 5 days) between train end and validation start to avoid adjacency leakage.

### Metrics (scalar, easy to compare)

* **MAE:** $\frac{1}{n}\sum |y - \hat{y}|$ — robust & interpretable.
* **sMAPE:** $\frac{2}{n}\sum \frac{|y - \hat{y}|}{(|y| + |\hat{y}| + \epsilon)}$ — scale‑free, safe for near‑zero returns with $\epsilon$.
* **MASE:** $\text{MASE}=\frac{\text{MAE}_\text{model}}{\text{MAE}_\text{naive (train)}}$ — <1 means better than naive.

  * For seasonality $s$, the **naive comparator** predicts $y_{t+1} \approx y_{t+1-s}$ (we’ll use $s=5$ for day‑of‑week seasonality on business days).
  * **Scale** is computed on the **training window only**, per ticker.

### Aggregation across tickers

* **Per‑ticker metrics** first → then aggregate.
* **Macro average:** mean of per‑ticker metrics (each ticker equal weight).
* **Micro/weighted:** pool all rows (or weight tickers by sample count); for MAE, pooled MAE equals sample‑count weighted average of per‑ticker MAEs.

---

## In‑class lab (35 min, Colab‑friendly)

> Run each block as its own cell. Adjust `REPO_NAME` if needed.

### 0) Setup & fallback data

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME  = "unified-stocks-teamX"  # <- change to your repo name
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, numpy as np, pandas as pd
from pathlib import Path
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
for p in ["data/raw","data/processed","reports","scripts","tests"]:
    Path(p).mkdir(parents=True, exist_ok=True)
print("Working dir:", os.getcwd())

# Load returns or create a tiny fallback
rpath = Path("data/processed/returns.parquet")
if rpath.exists():
    returns = pd.read_parquet(rpath)
else:
    # Fallback synthetic returns for 5 tickers, 320 business days
    rng = np.random.default_rng(0)
    dates = pd.bdate_range("2022-01-03", periods=320)
    frames=[]
    for tkr in ["AAPL","MSFT","GOOGL","AMZN","NVDA"]:
        eps = rng.normal(0, 0.012, size=len(dates)).astype("float32")
        adj = 100*np.exp(np.cumsum(eps))
        df = pd.DataFrame({
            "date": dates,
            "ticker": tkr,
            "adj_close": adj.astype("float32"),
            "log_return": np.r_[np.nan, np.diff(np.log(adj))].astype("float32")
        })
        df["r_1d"] = df["log_return"].shift(-1)
        df["weekday"] = df["date"].dt.weekday.astype("int8")
        df["month"]   = df["date"].dt.month.astype("int8")
        frames.append(df)
    returns = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)
    returns["ticker"] = returns["ticker"].astype("category")
    returns.to_parquet(rpath, index=False)

# Standardize
returns["date"] = pd.to_datetime(returns["date"])
returns = returns.sort_values(["ticker","date"]).reset_index(drop=True)
returns["ticker"] = returns["ticker"].astype("category")
returns.head()
```

### 1) Rolling‑origin date splitter (expanding windows + embargo)

```python
import numpy as np, pandas as pd

def make_rolling_origin_splits(dates: pd.Series,
                               train_min=252,   # ~1y of trading days
                               val_size=63,     # ~1 quarter
                               step=63,
                               embargo=5):
    """Return a list of (train_start, train_end, val_start, val_end) date tuples."""
    u = np.array(sorted(pd.to_datetime(dates.unique())))
    n = len(u)
    splits=[]
    i = train_min - 1
    while True:
        if i >= n: break
        tr_start, tr_end = u[0], u[i]
        vs_idx = i + embargo + 1
        ve_idx = vs_idx + val_size - 1
        if ve_idx >= n: break
        splits.append((tr_start, tr_end, u[vs_idx], u[ve_idx]))
        i += step
    return splits

def splits_to_indices(df, split):
    """Map a date split to index arrays for the full multi-ticker frame."""
    a,b,c,d = split
    tr_idx = df.index[(df["date"]>=a) & (df["date"]<=b)].to_numpy()
    va_idx = df.index[(df["date"]>=c) & (df["date"]<=d)].to_numpy()
    # sanity: embargo => last train date < first val date
    assert b < c
    return tr_idx, va_idx

splits = make_rolling_origin_splits(returns["date"], train_min=252, val_size=63, step=63, embargo=5)
len(splits), splits[:2]
```

### 2) Metrics & baseline predictors (naive and seasonal‑naive)

```python
from typing import Dict, Tuple

def mae(y, yhat): 
    y = np.asarray(y); yhat = np.asarray(yhat); 
    return float(np.mean(np.abs(y - yhat)))

def smape(y, yhat, eps=1e-8):
    y = np.asarray(y); yhat = np.asarray(yhat)
    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))

def mase(y_true, y_pred, y_train_true, y_train_naive):
    # Scale = MAE of comparator (naive) on TRAIN only; add tiny epsilon
    scale = mae(y_train_true, y_train_naive) + 1e-12
    return float(mae(y_true, y_pred) / scale)

def add_baseline_preds(df: pd.DataFrame, seasonality:int=5) -> pd.DataFrame:
    """
    For each ticker:
      - naive predicts r_{t+1} ≈ log_return_t (s=1)
      - seasonal naive (s) predicts r_{t+1} ≈ log_return_{t+1-s}  => shift(s-1)
    Adds columns: yhat_naive, yhat_s{s}
    """
    out = df.copy()
    out["yhat_naive"] = out.groupby("ticker")["log_return"].transform(lambda s: s)  # s=1
    if seasonality <= 1:
        out["yhat_s"] = out["yhat_naive"]
    else:
        out["yhat_s"] = out.groupby("ticker")["log_return"].transform(lambda s: s.shift(seasonality-1))
    return out
```

### 3) Evaluate baselines across **first 2 splits** (fast in class)

```python
# Precompute predictions over the entire frame once (safe: uses only past values via shift)
seasonality = 5  # business-day weekly
preds_all = add_baseline_preds(returns, seasonality=seasonality)

def per_ticker_metrics(df_val, df_train, method="naive") -> pd.DataFrame:
    """
    Compute per-ticker MAE, sMAPE, MASE for the chosen method ('naive' or 's').
    MASE scale uses TRAIN window and the same comparator as method.
    """
    rows=[]
    col = "yhat_naive" if method=="naive" else "yhat_s"
    for tkr, g in df_val.groupby("ticker"):
        gv = g.dropna(subset=["r_1d", col])
        if len(gv)==0: 
            continue
        # TRAIN scale (per ticker)
        gt = df_train[df_train["ticker"]==tkr].dropna(subset=["r_1d"])
        if method=="naive":
            gt_pred = gt["log_return"]  # s=1
        else:
            gt_pred = gt["log_return"].shift(seasonality-1)
        gt_clean = gt.dropna(subset=["r_1d"]).copy()
        gt_pred = gt_pred.loc[gt_clean.index]
        gt_clean = gt_clean.dropna(subset=["r_1d"])
        # Align indices
        y_tr = gt_clean["r_1d"].to_numpy()
        yhat_tr_naive = gt_pred.to_numpy()
        # VAL metrics
        y = gv["r_1d"].to_numpy()
        yhat = gv[col].to_numpy()
        rows.append({
            "ticker": tkr,
            "n": int(len(y)),
            "mae": mae(y,yhat),
            "smape": smape(y,yhat),
            "mase": mase(y, yhat, y_tr, yhat_tr_naive),
        })
    return pd.DataFrame(rows)

def aggregate_across_tickers(per_ticker_df: pd.DataFrame) -> Dict[str,float]:
    if per_ticker_df.empty:
        return {"macro_mae":np.nan,"macro_smape":np.nan,"macro_mase":np.nan,
                "micro_mae":np.nan,"micro_smape":np.nan,"micro_mase":np.nan}
    # Macro = unweighted mean across tickers
    macro = per_ticker_df[["mae","smape","mase"]].mean().to_dict()
    # Micro/weighted by n (pooled)
    w = per_ticker_df["n"].to_numpy()
    micro = {
        "micro_mae": float(np.average(per_ticker_df["mae"], weights=w)),
        "micro_smape": float(np.average(per_ticker_df["smape"], weights=w)),
        "micro_mase": float(np.average(per_ticker_df["mase"], weights=w)),
    }
    return {f"macro_{k}": float(v) for k,v in macro.items()} | micro

# Run on 2 splits in class; you can expand later
import pathlib, json
pathlib.Path("reports").mkdir(exist_ok=True)
rows=[]
for sid, split in enumerate(splits[:2], start=1):
    a,b,c,d = split
    tr_idx, va_idx = splits_to_indices(returns, split)
    tr = preds_all.loc[tr_idx].copy()
    va = preds_all.loc[va_idx].copy()
    # Per-ticker metrics for two baselines
    pt_naive = per_ticker_metrics(va, tr, method="naive")
    pt_s     = per_ticker_metrics(va, tr, method="s")
    agg_naive = aggregate_across_tickers(pt_naive)
    agg_s     = aggregate_across_tickers(pt_s)
    # Save per-split, per-ticker
    pt_naive.to_csv(f"reports/baseline_naive_split{sid}.csv", index=False)
    pt_s.to_csv(f"reports/baseline_s{seasonality}_split{sid}.csv", index=False)
    rows.append({
        "split": sid,
        "train_range": f"{a.date()}→{b.date()}",
        "val_range": f"{c.date()}→{d.date()}",
        "method": "naive", **agg_naive
    })
    rows.append({
        "split": sid,
        "train_range": f"{a.date()}→{b.date()}",
        "val_range": f"{c.date()}→{d.date()}",
        "method": f"s{seasonality}", **agg_s
    })

summary = pd.DataFrame(rows)
summary.to_csv("reports/baselines_rollingorigin_summary.csv", index=False)
summary
```

### 4) Quick sanity assertions (no overlap; embargo honored)

```python
def check_no_overlap(df, split):
    a,b,c,d = split
    assert b < c, f"Embargo violation: train_end {b} >= val_start {c}"
    tr_idx, va_idx = splits_to_indices(df, split)
    assert set(tr_idx).isdisjoint(set(va_idx))
    return True

all(check_no_overlap(returns, s) for s in splits[:2]), len(summary)
```

---

## Wrap‑up (10 min)

* You now have a **date‑based rolling‑origin splitter** with an **embargo**, and **baseline metrics** that set a credible reference.
* **MASE** uses a **training‑window naive** as scale (per ticker), so you can read “<1 is better than naive” at a glance.
* Aggregation: report both **macro** (per‑ticker average) and **micro/weighted** (pooled).

---

## Homework (due before Session 16)

**Goal:** Build a small CLI to reproduce these baselines over **all splits**, then generate per‑ticker & aggregated tables.

### Part A — Script: `scripts/baselines_eval.py`

```python
#!/usr/bin/env python
from __future__ import annotations
import argparse, numpy as np, pandas as pd
from pathlib import Path

def mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))
def smape(y,yhat,eps=1e-8):
    y = np.asarray(y); yhat = np.asarray(yhat)
    return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))
def mase(y_true, y_pred, y_train_true, y_train_naive):
    return float(mae(y_true, y_pred) / (mae(y_train_true, y_train_naive)+1e-12))

def make_splits(dates, train_min, val_size, step, embargo):
    u = np.array(sorted(pd.to_datetime(dates.unique()))); n=len(u); out=[]; i=train_min-1
    while True:
        if i>=n: break
        a,b = u[0], u[i]; vs = i + embargo + 1; ve = vs + val_size - 1
        if ve>=n: break
        out.append((a,b,u[vs],u[ve])); i += step
    return out

def add_preds(df, s):
    out = df.copy()
    out["yhat_naive"] = out.groupby("ticker")["log_return"].transform(lambda x: x)
    out["yhat_s"] = out.groupby("ticker")["log_return"].transform(lambda x: x.shift(s-1)) if s>1 else out["yhat_naive"]
    return out

def per_ticker(df_val, df_train, method, s):
    col = "yhat_naive" if method=="naive" else "yhat_s"
    rows=[]
    for tkr, g in df_val.groupby("ticker"):
        gv = g.dropna(subset=["r_1d", col])
        if len(gv)==0: continue
        gt = df_train[df_train["ticker"]==tkr].dropna(subset=["r_1d"])
        gt_pred = gt["log_return"] if method=="naive" else gt["log_return"].shift(s-1)
        gt_pred = gt_pred.loc[gt.index]
        y_tr = gt["r_1d"].to_numpy(); yhat_tr = gt_pred.to_numpy()
        y = gv["r_1d"].to_numpy(); yhat = gv[col].to_numpy()
        rows.append({"ticker":tkr,"n":int(len(y)),
                     "mae": mae(y,yhat),
                     "smape": smape(y,yhat),
                     "mase": mase(y,yhat,y_tr,yhat_tr)})
    return pd.DataFrame(rows)

def agg(pt):
    if pt.empty: return {"macro_mae":np.nan,"macro_smape":np.nan,"macro_mase":np.nan,
                         "micro_mae":np.nan,"micro_smape":np.nan,"micro_mase":np.nan}
    macro = pt[["mae","smape","mase"]].mean().to_dict()
    w = pt["n"].to_numpy()
    micro = {
        "micro_mae": float(np.average(pt["mae"], weights=w)),
        "micro_smape": float(np.average(pt["smape"], weights=w)),
        "micro_mase": float(np.average(pt["mase"], weights=w)),
    }
    return {f"macro_{k}": float(v) for k,v in macro.items()} | micro

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--returns", default="data/processed/returns.parquet")
    ap.add_argument("--seasonality", type=int, default=5)
    ap.add_argument("--train-min", type=int, default=252)
    ap.add_argument("--val-size", type=int, default=63)
    ap.add_argument("--step", type=int, default=63)
    ap.add_argument("--embargo", type=int, default=5)
    ap.add_argument("--out-summary", default="reports/baselines_rollingorigin_summary.csv")
    ap.add_argument("--out-per-ticker", default="reports/baselines_per_ticker_split{sid}_{method}.csv")
    args = ap.parse_args()

    df = pd.read_parquet(args.returns).sort_values(["ticker","date"]).reset_index(drop=True)
    splits = make_splits(df["date"], args.train_min, args.val_size, args.step, args.embargo)
    pred = add_preds(df, args.seasonality)

    rows=[]
    for sid, (a,b,c,d) in enumerate(splits, start=1):
        tr = pred[(pred["date"]>=a)&(pred["date"]<=b)]
        va = pred[(pred["date"]>=c)&(pred["date"]<=d)]
        for method in ["naive","s"]:
            pt = per_ticker(va, tr, method, args.seasonality)
            Path("reports").mkdir(exist_ok=True)
            pt.to_csv(args.out_per_ticker.format(sid=sid, method=method), index=False)
            rows.append({"split":sid,"train_range":f"{a.date()}→{b.date()}","val_range":f"{c.date()}→{d.date()}",
                         "method":"naive" if method=="naive" else f"s{args.seasonality}", **agg(pt)})
    pd.DataFrame(rows).to_csv(args.out_summary, index=False)
    print("Wrote", args.out_summary, "and per-ticker CSVs.")

if __name__ == "__main__":
    main()
```

Make executable & run:

```bash
%%bash
chmod +x scripts/baselines_eval.py
python scripts/baselines_eval.py --seasonality 5
```

### Part B — Plot a tiny, informative results figure

```python
import pandas as pd, matplotlib.pyplot as plt, pathlib
pathlib.Path("docs/figs").mkdir(parents=True, exist_ok=True)

summary = pd.read_csv("reports/baselines_rollingorigin_summary.csv")
plt.figure(figsize=(6,3.5))
for method, g in summary.groupby("method"):
    plt.plot(g["split"], g["micro_mae"], marker="o", label=f"{method} micro MAE")
plt.xlabel("Split"); plt.ylabel("MAE"); plt.title("Baseline MAE across splits")
plt.legend(); plt.tight_layout()
plt.savefig("docs/figs/baselines_mae_splits.png", dpi=200)
"Saved docs/figs/baselines_mae_splits.png"
```

### Part C — Add a quick test to protect the splitter

```python
# tests/test_rolling_splitter.py
import pandas as pd, numpy as np
from datetime import timedelta

def make_splits(dates, train_min, val_size, step, embargo):
    u = np.array(sorted(pd.to_datetime(dates.unique()))); n=len(u); out=[]; i=train_min-1
    while True:
        if i>=n: break
        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=n: break
        out.append((a,b,u[vs],u[ve])); i+=step
    return out

def test_embargo_and_order():
    dates = pd.bdate_range("2024-01-01", periods=400)
    s = make_splits(pd.Series(dates), 252, 63, 63, 5)
    assert all(b < c for (a,b,c,d) in s), "Embargo/order violated"
    # Splits should move forward
    assert len(s) >= 2 and s[1][1] > s[0][1]
```

Run:

```bash
%%bash
pytest -q -k rolling_splitter
```

### Part D — (Optional) Makefile targets

```make
.PHONY: baselines
baselines: ## Evaluate naive & seasonal-naive baselines across all splits
\tpython scripts/baselines_eval.py --seasonality 5
```

---

## Instructor checklist (before class)

* Ensure `returns.parquet` exists or fallback works.
* Be ready to whiteboard **why** the seasonal naïve for daily data uses `s=5`.
* Emphasize **MASE scale from TRAIN** and **macro vs micro** aggregation.

## Emphasize while teaching

* **Define the problem first** (H, step, splits); metrics only make sense after framing.
* **MASE < 1** ⇒ better than naïve; report both macro & micro.
* **Embargo** helps mitigate adjacency leakage; keep it small but nonzero.

## Grading (pass/revise)

* Rolling‑origin splitter implemented and used (train/val ranges printed).
* Reports written: `baselines_rollingorigin_summary.csv` and per‑ticker CSVs per split & method.
* Metrics include **MAE**, **sMAPE**, **MASE**; aggregation includes **macro** and **micro**.
* A test asserts basic splitter properties (no overlap; forward progress).

You now have clear **framing and metrics** for your project. In Session 16, you’ll fit **classical baselines** (e.g., lags‑only linear, ARIMA/ETS quick sketches) and log them in the same results table schema.
