---
title: "Session 23 — Packaging & CLI (Typer)"
---
Below is a complete lecture package for **Session 23 — Packaging & CLI (Typer)** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. You’ll package your repo into a proper Python project using a `src/` layout, add a minimal **Typer** CLI, and wire it to score models over a date range using your existing **features Parquet** and saved checkpoints.

> **Assumptions**
> • Students use Colab (Drive mounted).
> • Repo already has `data/processed/features_v1(.parquet)`, `data/processed/returns.parquet`; and (ideally) `models/gru_split1.pt` and/or `models/tsgpt_split1.pt` from Sessions 19 & 22.
> • If checkpoints are missing, the CLI **falls back** to baselines (`naive`, `lin_lags`).
> • This is **educational**, not trading advice.

---

## Session 23 — Packaging & CLI (Typer) (75 min)

### Learning goals

By the end, students can:

1. Convert a research repo into an **installable package** with a `src/` layout and `pyproject.toml`.
2. Create a **Typer** CLI (`python -m projectname.cli …`) that reads config, loads data, and **scores** a model over a date range.
3. Centralize config in **YAML**, keep paths tidy, and write results to `reports/`.
4. Run a **fresh‑clone** smoke test: `make env && pip install -e . && python -m projectname.cli score …`.

---

## Agenda (75 min)

* **(10 min)** Slides: why package? `src/` layout; `pyproject.toml`; editable installs (`pip install -e .`).
* **(10 min)** Slides: CLI ergonomics with **Typer**; single binary vs `python -m`; structured logging & exit codes.
* **(40 min)** **In‑class lab**: add package skeleton → `pyproject.toml` → config YAML → CLI with `score` and `split-info` → run end‑to‑end.
* **(15 min)** Wrap‑up, Makefile targets, homework brief.

---

## Slides — talking points (paste into deck)

**Why package your project?**

* Import your code anywhere (`import projectname…`) → reliable relative imports, easier testing/CI, cleaner notebooks.
* “`src/` layout” prevents accidental imports from the working dir; forces **installed** code to be used.

**`pyproject.toml` essentials**

* `[build-system]` selects backend (`setuptools`, `hatchling`, …).
* `[project]` declares name, version, dependencies.
* Use **editable install** during research: `pip install -e .[dev]`.

**Typer for CLIs**

* Declarative options & automatic help (`--help`).
* Compose commands: `score`, `split-info`, `show-config`.
* Return **non‑zero** exit codes on errors; don’t hide failures.

**Config in YAML**

* One `config/config.yaml` to centralize paths, features, split params.
* Parse with `yaml.safe_load`; add a tiny typed wrapper.

---

## In‑class Lab (40 min)

> Run each block as its own cell (or file creation step). Update `REPO_NAME` as needed.

### 0) Mount Drive & cd to repo

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME = "unified-stocks-teamX"  # <- change
BASE_DIR  = "/content/drive/MyDrive/dspt25"
REPO_DIR  = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, sys, platform
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
print("Working dir:", os.getcwd(), "| Python:", sys.version.split()[0], "| OS:", platform.system())

# Create standard dirs if missing
for p in ["src/projectname", "src/projectname/data", "src/projectname/features",
          "src/projectname/models", "src/projectname/utils", "config",
          "reports", "models", "tests", "docs/figs"]:
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)
```

### 1) Create `pyproject.toml` (packaging metadata)

> **Tip**: We keep `requirements.txt` for now, but the package will also declare dependencies.

```toml
# pyproject.toml  (place at repo root)
[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "projectname"                # rename if you like; keep it simple & lowercase
version = "0.1.0"
description = "Unified stock forecasting pipeline (teaching)"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pandas>=2.1",
  "numpy>=1.25",
  "pyarrow>=14",
  "typer[all]>=0.9",
  "rich>=13.3",
  "PyYAML>=6.0",
  "scikit-learn>=1.3",
  "torch>=2.1",
]
[project.optional-dependencies]
dev = ["pytest>=7", "pytest-cov>=4"]

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[project.scripts]
# Optional console entrypoint (`projectname`); we also support `python -m projectname.cli`
projectname = "projectname.cli:main"
```

### 2) Add a default config YAML

```yaml
# config/config.yaml
data:
  processed_dir: "data/processed"
  features_file: "data/processed/features_v1.parquet"
  returns_file:  "data/processed/returns.parquet"
  models_dir:    "models"
  reports_dir:   "reports"

eval:
  train_min: 252
  val_size: 63
  embargo: 5
  context: 64

features:
  use: ["log_return", "lag1", "lag2", "lag3", "zscore_20", "roll_std_20"]  # only those present will be used
```

### 3) Package boilerplate

**`src/projectname/__init__.py`**

```python
__all__ = ["config", "utils", "models"]
__version__ = "0.1.0"
```

**`src/projectname/config.py`**

```python
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
import yaml

DEFAULT_CFG = Path("config/config.yaml")

@dataclass
class DataPaths:
    processed_dir: Path
    features_file: Path
    returns_file: Path
    models_dir: Path
    reports_dir: Path

@dataclass
class EvalCfg:
    train_min: int
    val_size: int
    embargo: int
    context: int

@dataclass
class Config:
    data: DataPaths
    eval: EvalCfg
    features_use: list[str]

def load_config(path: str | Path = DEFAULT_CFG) -> Config:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Config not found: {p.resolve()}")
    cfg = yaml.safe_load(p.read_text())
    data = cfg.get("data", {})
    ev   = cfg.get("eval", {})
    feats= cfg.get("features", {}).get("use", [])
    return Config(
        data=DataPaths(
            processed_dir=Path(data["processed_dir"]),
            features_file=Path(data["features_file"]),
            returns_file=Path(data["returns_file"]),
            models_dir=Path(data["models_dir"]),
            reports_dir=Path(data["reports_dir"]),
        ),
        eval=EvalCfg(
            train_min=int(ev["train_min"]),
            val_size=int(ev["val_size"]),
            embargo=int(ev["embargo"]),
            context=int(ev["context"]),
        ),
        features_use=list(feats),
    )
```

**`src/projectname/utils/splits.py`**

```python
from __future__ import annotations
import numpy as np
import pandas as pd

def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    i = train_min - 1
    out = []
    while True:
        if i >= len(u):
            break
        a, b = u[0], u[i]
        vs = i + embargo + 1
        ve = vs + val_size - 1
        if ve >= len(u):
            break
        out.append((a, b, u[vs], u[ve]))
        i += step
    return out
```

**`src/projectname/utils/metrics.py`**

```python
from __future__ import annotations
import numpy as np
import pandas as pd

def mae(y, yhat) -> float:
    y = np.asarray(y); yhat = np.asarray(yhat)
    return float(np.mean(np.abs(y - yhat)))

def smape(y, yhat, eps: float = 1e-8) -> float:
    y = np.asarray(y); yhat = np.asarray(yhat)
    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))

def mase_scale_train(train_df: pd.DataFrame) -> dict[str, float]:
    # per-ticker naive MAE scale on TRAIN only
    scales={}
    for tkr, g in train_df.groupby("ticker"):
        g = g.dropna(subset=["r_1d","log_return"])
        if len(g)==0: continue
        scales[str(tkr)] = mae(g["r_1d"], g["log_return"])
    return scales

def mase(y_true, y_pred, tickers, scale: dict[str, float]) -> float:
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    tickers = np.asarray(tickers).astype(str)
    den = np.array([scale.get(t, np.nan) for t in tickers], dtype=float)
    return float(np.nanmean(np.abs(y_true - y_pred)/(den + 1e-12)))
```

**`src/projectname/models/baselines.py`**

```python
from __future__ import annotations
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

def predict_naive(val_df: pd.DataFrame) -> pd.DataFrame:
    out = val_df[["date","ticker","r_1d","log_return"]].copy()
    out = out.rename(columns={"r_1d":"y_true","log_return":"yhat"})
    out["method"] = "naive"
    return out

def predict_lin_lags(train_df: pd.DataFrame, val_df: pd.DataFrame, feats: list[str]) -> pd.DataFrame:
    preds=[]
    xcols = [c for c in feats if c in val_df.columns]
    for tkr, tr in train_df.groupby("ticker"):
        va = val_df[val_df["ticker"]==tkr]
        if len(tr)==0 or len(va)==0: continue
        pipe = Pipeline([("scaler", StandardScaler()), ("lr", LinearRegression())])
        pipe.fit(tr[xcols].values, tr["r_1d"].values)
        yhat = pipe.predict(va[xcols].values)
        g = va[["date","ticker","r_1d"]].copy()
        g["yhat"] = yhat
        g["method"] = "lin_lags"
        preds.append(g)
    out = pd.concat(preds, ignore_index=True) if preds else pd.DataFrame(columns=["date","ticker","y_true","yhat","method"])
    return out.rename(columns={"r_1d":"y_true"})
```

**`src/projectname/models/torch_infer.py`** (tiny inference wrappers; safe if ckpt missing)

```python
from __future__ import annotations
import json
from pathlib import Path
import numpy as np
import pandas as pd
import torch, torch.nn as nn

# Reuse tiny GRU and TS-GPT definitions (matching Sessions 19 & 22)
class GRURegressor(nn.Module):
    def __init__(self, in_features: int, hidden=64, layers=2, dropout=0.1):
        super().__init__()
        self.gru = nn.GRU(in_features, hidden, num_layers=layers, batch_first=True, dropout=dropout if layers>1 else 0.)
        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden,1))
    def forward(self, x):
        _, hN = self.gru(x)
        return self.head(hN[-1]).squeeze(-1)

class CausalSelfAttention(nn.Module):
    def __init__(self, d_model, n_head, ctx, p=0.0):
        super().__init__(); assert d_model % n_head == 0
        self.nh = n_head; self.dh = d_model//n_head
        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)
        self.proj= nn.Linear(d_model, d_model, bias=False)
        self.ad = nn.Dropout(p); self.rd = nn.Dropout(p)
        self.register_buffer("mask", torch.tril(torch.ones(ctx,ctx)).unsqueeze(0).unsqueeze(0))
    def forward(self, x):
        B,T,C=x.shape; qkv=self.qkv(x); q,k,v=qkv.split(C,dim=2)
        q=q.view(B,T,self.nh,self.dh).transpose(1,2); k=k.view(B,T,self.nh,self.dh).transpose(1,2); v=v.view(B,T,self.nh,self.dh).transpose(1,2)
        att=(q @ k.transpose(-2,-1))/np.sqrt(self.dh); att=att.masked_fill(self.mask[:,:,:T,:T]==0, float("-inf"))
        att=att.softmax(dim=-1); att=self.ad(att); y=att @ v; y=y.transpose(1,2).contiguous().view(B,T,C)
        return self.rd(self.proj(y))

class Block(nn.Module):
    def __init__(self, d_model, n_head, ctx, d_ff, p=0.0):
        super().__init__()
        self.ln1=nn.LayerNorm(d_model); self.att=CausalSelfAttention(d_model,n_head,ctx,p)
        self.ln2=nn.LayerNorm(d_model); self.mlp=nn.Sequential(nn.Linear(d_model,d_ff), nn.GELU(), nn.Dropout(p), nn.Linear(d_ff,d_model), nn.Dropout(p))
    def forward(self,x): x=x+self.att(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x

class TimeSeriesGPT(nn.Module):
    def __init__(self, in_features, ctx=64, d_model=64, n_head=2, n_layer=2, d_ff=128, p=0.0):
        super().__init__(); self.ctx=ctx
        self.proj=nn.Linear(in_features,d_model); self.pos=nn.Embedding(ctx,d_model)
        self.blocks=nn.ModuleList([Block(d_model,n_head,ctx,d_ff,p) for _ in range(n_layer)])
        self.ln=nn.LayerNorm(d_model); self.head=nn.Linear(d_model,1)
    def forward(self,x):
        B,T,F=x.shape; pos=torch.arange(T, device=x.device)
        h=self.proj(x)+self.pos(pos)[None,:,:]
        for blk in self.blocks: h=blk(h)
        h=self.ln(h)
        return self.head(h[:,-1,:]).squeeze(-1)

def _windowize(df: pd.DataFrame, feats: list[str], T: int):
    Xs, ys, tk = [], [], []
    for tkr, g in df.groupby("ticker"):
        g = g.sort_values("date").reset_index(drop=True)
        X = g[feats].to_numpy("float32"); y = g["r_1d"].to_numpy("float32")
        for end in range(T-1, len(g)):
            Xs.append(X[end-T+1:end+1]); ys.append(y[end]); tk.append(str(tkr))
    Xs = np.stack(Xs,0); ys = np.array(ys); tk = np.array(tk)
    return Xs, ys, tk

def predict_with_gru(val_df: pd.DataFrame, feats: list[str], models_dir: Path, T: int=64):
    ckpt = models_dir / "gru_split1.pt"
    if not ckpt.exists():
        raise FileNotFoundError("Missing models/gru_split1.pt (train in Session 19)")

    meta = torch.load(ckpt, map_location="cpu")
    in_f = len(feats)
    net = GRURegressor(in_features=in_f)
    net.load_state_dict(meta["model_state"]); net.eval()
    Xs, ys, tk = _windowize(val_df, feats, T)
    with torch.no_grad():
        yhat = net(torch.from_numpy(Xs).float()).cpu().numpy()
    out = val_df.iloc[T-1:].copy().reset_index(drop=True).loc[:, ["date","ticker"]]
    out = out.assign(y_true=ys, yhat=yhat, method="gru")
    return out

def predict_with_tsgpt(val_df: pd.DataFrame, feats: list[str], models_dir: Path, T: int=64):
    ckpt = models_dir / "tsgpt_split1.pt"
    if not ckpt.exists():
        raise FileNotFoundError("Missing models/tsgpt_split1.pt (train in Session 22)")

    meta = torch.load(ckpt, map_location="cpu")
    in_f = len(feats)
    net = TimeSeriesGPT(in_features=in_f, ctx=T)
    net.load_state_dict(meta["model"]); net.eval()
    Xs, ys, tk = _windowize(val_df, feats, T)
    with torch.no_grad():
        yhat = net(torch.from_numpy(Xs).float()).cpu().numpy()
    out = val_df.iloc[T-1:].copy().reset_index(drop=True).loc[:, ["date","ticker"]]
    out = out.assign(y_true=ys, yhat=yhat, method="tsgpt")
    return out
```

### 4) The CLI (Typer): `src/projectname/cli.py`

```python
from __future__ import annotations
import sys
from datetime import date
from pathlib import Path
from typing import Optional, Literal

import pandas as pd
import typer
from rich import print as rprint
from rich.table import Table

from projectname.config import load_config
from projectname.utils.splits import make_rolling_origin_splits
from projectname.utils.metrics import mae, smape, mase_scale_train, mase
from projectname.models.baselines import predict_naive, predict_lin_lags

# Optional imports for torch models (graceful fallback)
try:
    from projectname.models.torch_infer import predict_with_gru, predict_with_tsgpt
except Exception as e:
    predict_with_gru = predict_with_tsgpt = None  # type: ignore

app = typer.Typer(add_completion=False, help="Project CLI for scoring/evaluation.")

def _load_features(features_file: Path) -> pd.DataFrame:
    df = pd.read_parquet(features_file)
    df["date"] = pd.to_datetime(df["date"])
    df["ticker"] = df["ticker"].astype("category")
    return df.sort_values(["ticker","date"]).reset_index(drop=True)

@app.command()
def split_info(
    config: Path = typer.Option("config/config.yaml", help="Path to YAML config."),
):
    "Print available rolling-origin splits (first 3)."
    cfg = load_config(config)
    df = _load_features(cfg.data.features_file)
    splits = make_rolling_origin_splits(df["date"], cfg.eval.train_min, cfg.eval.val_size, cfg.eval.val_size, cfg.eval.embargo)
    table = Table(title="Rolling-origin splits (first 3)")
    table.add_column("id"); table.add_column("train_start"); table.add_column("train_end"); table.add_column("val_start"); table.add_column("val_end")
    for i,(a,b,c,d) in enumerate(splits[:3], start=1):
        table.add_row(str(i), str(a.date()), str(b.date()), str(c.date()), str(d.date()))
    rprint(table)

@app.command()
def show_config(
    config: Path = typer.Option("config/config.yaml", help="Path to YAML config.")
):
    "Echo resolved config."
    rprint(load_config(config))

@app.command()
def score(
    model: Literal["naive","lin_lags","gru","tsgpt"] = typer.Option("lin_lags", help="Model to score."),
    start: Optional[str] = typer.Option(None, help="Val period start (YYYY-MM-DD). If omitted, use split 1."),
    end: Optional[str]   = typer.Option(None, help="Val period end (YYYY-MM-DD)."),
    split: int = typer.Option(1, help="Use this split if --start/--end not provided."),
    out: Path = typer.Option("reports/cli_score.csv", help="Output CSV path."),
    config: Path = typer.Option("config/config.yaml", help="Path to YAML config."),
):
    """
    Score a model over a date range; write tidy predictions with metrics.
    """
    cfg = load_config(config)
    df = _load_features(cfg.data.features_file)

    # Pick features present in the file
    feat_cols = [c for c in cfg.features_use if c in df.columns]
    if model in ("lin_lags","gru","tsgpt") and not feat_cols:
        typer.echo("No requested features found in features parquet.", err=True)
        raise typer.Exit(code=2)

    # Build split
    if start and end:
        vstart = pd.to_datetime(start); vend = pd.to_datetime(end)
        # Train = everything strictly before val start, respecting train_min
        dates = sorted(df["date"].unique())
        idx = next(i for i,d in enumerate(dates) if d>=vstart)
        if idx < cfg.eval.train_min: 
            typer.echo("Not enough history before val start for train_min.", err=True); raise typer.Exit(code=2)
        tstart = dates[0]; tend = dates[idx-1]
    else:
        splits = make_rolling_origin_splits(df["date"], cfg.eval.train_min, cfg.eval.val_size, cfg.eval.val_size, cfg.eval.embargo)
        try:
            tstart, tend, vstart, vend = splits[split-1]
        except IndexError:
            typer.echo(f"Split {split} not available.", err=True); raise typer.Exit(code=2)

    train_df = df[(df["date"]>=tstart)&(df["date"]<=tend)].copy()
    val_df   = df[(df["date"]>=vstart)&(df["date"]<=vend)].copy()

    if len(val_df)==0 or len(train_df)==0:
        typer.echo("Empty train/val after slicing. Check dates.", err=True); raise typer.Exit(code=2)

    # Produce predictions
    try:
        if model == "naive":
            preds = predict_naive(val_df)
        elif model == "lin_lags":
            preds = predict_lin_lags(train_df, val_df, feat_cols)
        elif model == "gru":
            if predict_with_gru is None:
                typer.echo("Torch not available in this environment.", err=True); raise typer.Exit(code=2)
            preds = predict_with_gru(val_df, feat_cols, cfg.data.models_dir, T=cfg.eval.context)
        elif model == "tsgpt":
            if predict_with_tsgpt is None:
                typer.echo("Torch not available in this environment.", err=True); raise typer.Exit(code=2)
            preds = predict_with_tsgpt(val_df, feat_cols, cfg.data.models_dir, T=cfg.eval.context)
        else:
            raise ValueError(model)
    except FileNotFoundError as e:
        typer.echo(f"{e}. Falling back to lin_lags.", err=True)
        preds = predict_lin_lags(train_df, val_df, feat_cols)

    # Metrics (micro; and MASE using train scale)
    scale = mase_scale_train(train_df)
    y = preds["y_true"]; yhat = preds["yhat"]; tick = preds["ticker"].astype(str)
    m_mae = mae(y,yhat); m_smape = smape(y,yhat); m_mase = mase(y,yhat,tick, scale)

    # Save predictions and print summary
    out.parent.mkdir(parents=True, exist_ok=True)
    preds.to_csv(out, index=False)

    rprint(f"[bold green]Wrote[/bold green] {out}")
    table = Table(title=f"{model} — {str(vstart.date())} → {str(vend.date())}")
    table.add_column("metric"); table.add_column("value")
    table.add_row("MAE", f"{m_mae:.6f}")
    table.add_row("sMAPE", f"{m_smape:.6f}")
    table.add_row("MASE", f"{m_mase:.6f}")
    rprint(table)

def main():
    app()

if __name__ == "__main__":
    main()
```

### 5) Install the package (editable) & quick smoke test

```bash
%%bash
pip install -e ".[dev]" -q
python -m projectname.cli --help | head -n 20
python -m projectname.cli split-info
python -m projectname.cli score --model lin_lags --out reports/cli_linlags.csv
```

You should see a metrics table and a saved CSV under `reports/`.

### 6) Makefile additions

Append to your existing **Makefile**:

```make
.PHONY: install score-lin
install: ## Editable install of the package
\tpip install -e ".[dev]"

score-lin: ## Score lin_lags on split 1, save predictions
\tpython -m projectname.cli score --model lin_lags --out reports/cli_linlags.csv
```

### 7) Minimal test for the CLI

**`tests/test_cli_score.py`**

```python
from typer.testing import CliRunner
from projectname.cli import app
import pandas as pd
from pathlib import Path

runner = CliRunner()

def test_score_lin_lags(tmp_path: Path):
    out = tmp_path/"preds.csv"
    result = runner.invoke(app, ["score", "--model", "lin_lags", "--out", str(out)])
    assert result.exit_code == 0
    assert out.exists()
    df = pd.read_csv(out)
    assert {"date","ticker","y_true","yhat","method"}.issubset(df.columns)
```

Run:

```bash
%%bash
pytest -q -k cli_score
```

---

## Wrap‑up (15 min)

* **Package once, reuse everywhere**: notebooks, scripts, CI.
* **CLI is the user surface**: deterministic, scriptable, easy to test.
* **Centralized config** prevents duplicated paths and parameters.
* Keep the CLI **fast** (no training). Scoring should finish in seconds to a couple minutes.

---

## Homework (due before Session 24)

**Goal:** Prove your repo is **reproducible from a fresh clone** and your CLI works end‑to‑end.

### A. Fresh‑clone smoke test script (Colab cell)

Paste this into a fresh Colab (or a new runtime):

```bash
# --- variables ---
REPO_SSH_OR_HTTPS="<YOUR REPO URL>"   # e.g. https://github.com/you/unified-stocks-teamX.git
REPO_DIR="unified-stocks-teamX"

# --- run ---
rm -rf "$REPO_DIR"
git clone "$REPO_SSH_OR_HTTPS" "$REPO_DIR"
cd "$REPO_DIR"

# env + editable install + report + score
make env              # from Session 2 (installs requirements.txt)
pip install -e ".[dev]"
quarto --version >/dev/null 2>&1 || echo "Quarto optional; skipping render."
python -m projectname.cli split-info
python -m projectname.cli score --model lin_lags --out reports/cli_linlags.csv

# show outputs
python - <<'PY'
import pandas as pd
print(pd.read_csv("reports/cli_linlags.csv").head())
PY
```

**Deliverables:**

1. A short note in README: “Fresh‑clone test (date), environment, and exact commands used.”
2. The file `reports/cli_linlags.csv` committed (small).
3. Optional: run `python -m projectname.cli score --model tsgpt --out reports/cli_tsgpt.csv` if `models/tsgpt_split1.pt` exists, and include that CSV too.

### B. Tiny CI check (optional but recommended)

Add this job to `.github/workflows/ci.yml` after tests:

```yaml
- name: CLI smoke test
  run: |
    pip install -e ".[dev]"
    python -m projectname.cli split-info
    python -m projectname.cli score --model lin_lags --out reports/cli_ci.csv
```

---

## Instructor notes / gotchas

* In Colab, after modifying `pyproject.toml` or code under `src/`, re‑run `pip install -e .` or restart the kernel to pick up changes.
* If **Torch** isn’t available, the CLI still works with `naive`/`lin_lags`.
* If students renamed the package (not `projectname`), update imports and `pyproject.toml` accordingly.
* Keep the CLI’s error messages **actionable** (e.g., “missing models/tsgpt\_split1.pt; falling back to lin\_lags”).

---

## Grading (pass/revise)

* `pip install -e .` succeeds; `import projectname` works.
* `python -m projectname.cli split-info` prints splits.
* `python -m projectname.cli score --model lin_lags` writes `reports/cli_linlags.csv` and prints metrics.
* README includes a **fresh‑clone** reproduction section.
* (Optional) A simple CLI test passes in CI.

This session leaves you with a clean, **installable** project and a **Typer CLI** that others (or future‑you) can run in one command, setting you up for Session 24’s **reproducibility audit** (and optional FastAPI demo).
