---
title: "Session 22 — Adapting GPT to Time Series"
---
Below is a complete lecture package for **Session 22 — Adapting GPT to Time Series** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll **replace token embeddings with a linear projection of real‑valued features**, keep **positional embeddings** and a **causal mask**, and train a **tiny Transformer (GPT‑style)** to predict **next‑day return**.

> **Educational use only — not trading advice.**
> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) and availability of `data/processed/features_v1.parquet` (or `features_v1_static.parquet`) with columns like `ticker`, `date`, `log_return`, `r_1d`, plus features (`lag1..lag3`, `roll_std_20`, `zscore_20`, …). Cells include **safe fallbacks** so the class can run even if files are missing.

---

## Session 22 — Adapting GPT to Time Series (75 min)

### Learning goals

By the end of class, students can:

1. Convert a **window of real‑valued features** into a sequence input for a GPT‑style model.
2. Use **positional embeddings** and **causal self‑attention** for **sequence‑to‑one regression** (predict `r_1d[t]` from `x[≤t]`).
3. Train with **MAE/Huber** loss, **early stopping**, and **AMP** (optional on CUDA).
4. Save reproducible artifacts (checkpoint + metrics CSV) and produce a quick loss plot.

---

## Agenda (75 min)

* **(10 min)** Slides: from tokens to real‑valued features; causal masking & alignment; loss choices
* **(10 min)** Slides: tiny TS‑GPT architecture (feature projector → pos emb → blocks → head)
* **(35 min)** **In‑class lab**: dataset → TS‑GPT → train/validate → save metrics & checkpoint → quick plots
* **(10 min)** Wrap‑up + homework brief
* **(10 min)** Buffer / Q\&A

---

## Slide talking points (drop into your deck)

### 1) From characters to features

* Char‑GPT maps **discrete tokens → embeddings**.
* **TS‑GPT** maps **real‑valued features `x_t∈ℝ^F` → `ℝ^d_model`** via a **linear projection** at each time step:

  $$
  h_t^{(0)} = W_\text{proj} x_t + b + \text{pos\_emb}(t).
  $$

### 2) Labels & alignment (no leakage)

* **Input window:** $X_{t-T+1:t}$ (length $T$).
* **Target:** $y_t = r\_1d[t]$ (next‑day log return), computed **outside** the model and **shifted by −1**.
* **Causal mask** ensures the block at step $t$ cannot see $>t$.

### 3) Loss & scaling

* **Loss:** MAE or **Huber** (robust to outliers in returns); report **MAE** and **sMAPE**.
* **Scaling:** Fit a **feature scaler on TRAIN only**, reuse for VAL/TEST.

### 4) Tiny TS‑GPT config (Colab‑friendly)

* `d_model=64`, `n_head=2`, `n_layer=2`, `d_ff=128`, `ctx=64`, dropout 0.0–0.1, batch 128–256.
* Save: `models/tsgpt_split1.pt` and `reports/tsgpt_split1_metrics.csv`.

---

## In‑class lab (35 min, Colab‑friendly)

> Run each block as its **own cell**. Update `REPO_NAME` if needed.

### 0) Setup & device

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME = "unified-stocks-teamX"  # <- change if needed
BASE_DIR  = "/content/drive/MyDrive/dspt25"
REPO_DIR  = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, sys, platform, random, math, time
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
for p in ["data/processed","models","reports","scripts","tests","docs/figs"]:
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)
print("Working dir:", os.getcwd(), "| Python", sys.version.split()[0], "| OS", platform.system())

import numpy as np, pandas as pd, torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Torch:", torch.__version__, "| CUDA:", torch.cuda.is_available(), "| Device:", device)

def seed_everything(seed=2222):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
seed_everything()
```

### 1) Load features; build split; TRAIN‑fit scaler (with fallbacks)

```python
from pathlib import Path

# Prefer static universe from Session 17
f_static = Path("data/processed/features_v1_static.parquet")
f_base   = Path("data/processed/features_v1.parquet")

if f_static.exists():
    df = pd.read_parquet(f_static)
elif f_base.exists():
    df = pd.read_parquet(f_base)
else:
    # Fallback from returns; add minimal features
    rpath = Path("data/processed/returns.parquet")
    if rpath.exists():
        df = pd.read_parquet(rpath).sort_values(["ticker","date"])
    else:
        # synthesize small dataset
        rng = np.random.default_rng(0)
        dates = pd.bdate_range("2022-01-03", periods=320)
        frames=[]
        for t in ["AAPL","MSFT","GOOGL","AMZN","NVDA","META"]:
            eps = rng.normal(0, 0.012, size=len(dates)).astype("float32")
            adj = 100*np.exp(np.cumsum(eps))
            g = pd.DataFrame({
                "date": dates, "ticker": t,
                "adj_close": adj.astype("float32"),
                "log_return": np.r_[np.nan, np.diff(np.log(adj))].astype("float32")
            })
            g["r_1d"] = g["log_return"].shift(-1)
            frames.append(g)
        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)
    # add basic features
    df = df.sort_values(["ticker","date"]).reset_index(drop=True)
    for k in [1,2,3]:
        df[f"lag{k}"] = df.groupby("ticker")["log_return"].shift(k)
    df["roll_std_20"]  = df.groupby("ticker")["log_return"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)
    df["zscore_20"]    = (df["log_return"] - df.groupby("ticker")["log_return"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)) / (df["roll_std_20"] + 1e-8)
    df = df.dropna().reset_index(drop=True)

# Harmonize and subset for speed
df["date"] = pd.to_datetime(df["date"]); df["ticker"] = df["ticker"].astype("category")
df = df.sort_values(["ticker","date"]).reset_index(drop=True)
keep = df["ticker"].cat.categories.tolist()[:10]  # up to 10 tickers for class
df = df[df["ticker"].isin(keep)].copy()

# Choose features (keep small & causal)
CAND = ["log_return","lag1","lag2","lag3","zscore_20","roll_std_20","weekday","month"]
FEATS = [c for c in CAND if c in df.columns]
assert "r_1d" in df.columns and len(FEATS)>0, "Missing label or features."

def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    i=train_min-1; out=[]
    while True:
        if i>=len(u): break
        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=len(u): break
        out.append((a,b,u[vs],u[ve])); i+=step
    return out

splits = make_rolling_origin_splits(df["date"], 252, 63, 63, 5)
assert splits, "Not enough history."
a,b,c,d = splits[0]
train_df = df[(df["date"]>=a)&(df["date"]<=b)].copy()
val_df   = df[(df["date"]>=c)&(df["date"]<=d)].copy()
print("Split1: train", a.date(), "→", b.date(), "| val", c.date(), "→", d.date(),
      "| train rows:", len(train_df), "| val rows:", len(val_df))
```

### 2) Dataset with **feature projection** targetting `r_1d`

```python
from torch.utils.data import Dataset, DataLoader
import json

class FeatureScaler:
    def __init__(self): self.mean_=None; self.std_=None
    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self
    def transform(self, X): return (X - self.mean_) / self.std_
    def state_dict(self): return {"mean": self.mean_.tolist(), "std": self.std_.tolist()}
    def load_state_dict(self, d): self.mean_=np.array(d["mean"]); self.std_=np.array(d["std"])

class WindowedTS(Dataset):
    """
    Sliding windows per ticker. Each item:
      X_scaled: (T, F), y: scalar r_1d at window end, ticker_id: long
    """
    def __init__(self, frame: pd.DataFrame, feats, T=64, scaler: FeatureScaler|None=None,
                 ticker2id: dict|None=None):
        self.T = int(T); self.feats=list(feats); self.idx=[]; self.groups={}
        # Build ticker ids (frozen on TRAIN)
        if ticker2id is None:
            cats = frame["ticker"].astype("category").cat.categories.tolist()
            self.ticker2id = {t:i for i,t in enumerate(cats)}
        else:
            self.ticker2id = dict(ticker2id)
        # Per‑ticker arrays and index of valid windows
        for tkr, g in frame.groupby("ticker"):
            g = g.sort_values("date").reset_index(drop=True)
            X = g[self.feats].to_numpy("float32")
            y = g["r_1d"].to_numpy("float32")
            tid = self.ticker2id[str(tkr)]
            for end in range(self.T-1, len(g)):
                if np.isfinite(y[end]): self.idx.append((str(tkr), end, tid))
            self.groups[str(tkr)] = {"X": X, "y": y}
        # Fit scaler on TRAIN if not provided
        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.groups[t]["X"] for t in self.groups], 0))

    def __len__(self): return len(self.idx)
    def __getitem__(self, i):
        tkr, end, tid = self.idx[i]
        g = self.groups[tkr]
        X = g["X"][end-self.T+1:end+1]
        X = self.scaler.transform(X)
        y = g["y"][end]
        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), torch.tensor(tid, dtype=torch.long)

# Build TRAIN/VAL datasets & loaders
CTX = 64; BATCH=256; WORKERS=2; PIN=torch.cuda.is_available()

train_ds = WindowedTS(train_df, FEATS, T=CTX)
val_ds   = WindowedTS(val_df,   FEATS, T=CTX, scaler=train_ds.scaler, ticker2id=train_ds.ticker2id)

# Persist scaler (reproducibility)
Path("models").mkdir(exist_ok=True)
Path("models/tsgpt_scaler_split1.json").write_text(json.dumps(train_ds.scaler.state_dict()))

def _seed_worker(_):
    ws = torch.initial_seed() % (2**32)
    np.random.seed(ws); random.seed(ws)

g = torch.Generator(); g.manual_seed(42)
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True,
                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),
                          worker_init_fn=_seed_worker, generator=g)
val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False,
                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),
                          worker_init_fn=_seed_worker)

len(train_ds), len(val_ds), next(iter(train_loader))[0].shape
```

### 3) **Time‑Series GPT** (feature projector + pos emb + Transformer blocks + regression head)

```python
import torch.nn as nn
from torch.nn import functional as F

BLOCK_SIZE = CTX  # maximum context supported

class CausalSelfAttention(nn.Module):
    def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):
        super().__init__()
        assert d_model % n_head == 0
        self.n_head = n_head; self.d_head = d_model // n_head
        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)
        self.proj = nn.Linear(d_model, d_model, bias=False)
        self.attn_drop = nn.Dropout(dropout); self.resid_drop = nn.Dropout(dropout)
        self.register_buffer("mask", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)).unsqueeze(0).unsqueeze(0))
    def forward(self, x):
        B,T,C = x.size()
        qkv = self.qkv(x); q,k,v = qkv.split(C, dim=2)
        q = q.view(B,T,self.n_head,self.d_head).transpose(1,2)
        k = k.view(B,T,self.n_head,self.d_head).transpose(1,2)
        v = v.view(B,T,self.n_head,self.d_head).transpose(1,2)
        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_head)
        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float("-inf"))
        att = F.softmax(att, dim=-1); att = self.attn_drop(att)
        y = att @ v
        y = y.transpose(1,2).contiguous().view(B,T,C)
        return self.resid_drop(self.proj(y))

class Block(nn.Module):
    def __init__(self, d_model: int, n_head: int, d_ff: int, dropout: float = 0.0):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model); self.attn = CausalSelfAttention(d_model, n_head, dropout)
        self.ln2 = nn.LayerNorm(d_model); self.mlp  = nn.Sequential(
            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model), nn.Dropout(dropout)
        )
    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x

class TimeSeriesGPT(nn.Module):
    """
    Real-valued sequence → regression (predict r_1d at last step).
    """
    def __init__(self, in_features: int, d_model=64, n_head=2, n_layer=2, d_ff=128, ctx=64, dropout=0.0,
                 n_tickers: int|None=None, d_ticker: int=0):
        super().__init__()
        self.ctx = ctx
        self.proj = nn.Linear(in_features, d_model)        # FEATURE PROJECTION
        self.pos  = nn.Embedding(ctx, d_model)             # POSITIONAL EMBEDDINGS
        self.id_emb = nn.Embedding(n_tickers, d_ticker) if (n_tickers and d_ticker>0) else None
        augmented = d_model + (d_ticker if self.id_emb else 0)
        self.blocks = nn.ModuleList([Block(augmented, n_head, d_ff, dropout) for _ in range(n_layer)])
        self.ln = nn.LayerNorm(augmented)
        self.head = nn.Linear(augmented, 1)                # REGRESSION HEAD

        self.apply(self._init)
    def _init(self, m):
        if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight); 
        if isinstance(m, nn.Embedding): nn.init.normal_(m.weight, 0.0, 0.02)
    def forward(self, x, ticker_ids=None):
        # x: (B,T,F)
        B,T,F = x.size()
        pos = torch.arange(T, device=x.device)
        h = self.proj(x) + self.pos(pos)[None,:,:]         # (B,T,d_model)
        if self.id_emb is not None and ticker_ids is not None:
            e = self.id_emb(ticker_ids).unsqueeze(1).expand(-1, T, -1)   # (B,T,d_ticker)
            h = torch.cat([h, e], dim=-1)                                 # (B,T,d_model+d_ticker)
        for blk in self.blocks: h = blk(h)
        h = self.ln(h)
        yhat = self.head(h[:,-1,:]).squeeze(-1)            # use last time step’s hidden state
        return yhat

# Instantiate tiny model (no ticker embedding for baseline)
D_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT = 64, 2, 2, 128, 0.0
model = TimeSeriesGPT(in_features=len(FEATS), d_model=D_MODEL, n_head=N_HEAD,
                      n_layer=N_LAYER, d_ff=D_FF, ctx=CTX, dropout=DROPOUT,
                      n_tickers=None, d_ticker=0).to(device)
sum(p.numel() for p in model.parameters())/1e6, device
```

### 4) Train (MAE or Huber), early stop, checkpoint, and metrics

```python
from torch.optim import AdamW
from torch.cuda.amp import autocast, GradScaler

def mae_t(y, yhat): return torch.mean(torch.abs(y - yhat))
def smape_t(y, yhat, eps=1e-8): return torch.mean(2.0*torch.abs(y - yhat)/(torch.abs(y)+torch.abs(yhat)+eps))

USE_HUBER = True
huber_delta = 0.01
huber = torch.nn.HuberLoss(delta=huber_delta, reduction="mean")

def train_one_epoch(model, loader, opt, scaler, device, use_amp=True):
    model.train(); tot=0.0; n=0
    for xb, yb, _ in loader:
        xb = xb.to(device, non_blocking=True).float()
        yb = yb.to(device, non_blocking=True).float()
        opt.zero_grad(set_to_none=True)
        if use_amp and device.type=="cuda":
            with autocast(dtype=torch.float16):
                yhat = model(xb)
                loss = huber(yhat, yb) if USE_HUBER else mae_t(yb, yhat)
            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
        else:
            yhat = model(xb); loss = huber(yhat, yb) if USE_HUBER else mae_t(yb, yhat)
            loss.backward(); opt.step()
        bs = xb.size(0); tot += loss.item()*bs; n += bs
    return tot/max(n,1)

@torch.no_grad()
def evaluate(model, loader, device):
    model.eval(); m_mae=m_smape=0.0; n=0
    for xb, yb, _ in loader:
        xb=xb.to(device, non_blocking=True).float()
        yb=yb.to(device, non_blocking=True).float()
        yhat = model(xb)
        bs=xb.size(0)
        m_mae += mae_t(yb, yhat).item()*bs
        m_smape += smape_t(yb, yhat).item()*bs
        n+=bs
    return {"mae": m_mae/max(n,1), "smape": m_smape/max(n,1)}

def fit(model, train_loader, val_loader, epochs=12, lr=2e-3, wd=1e-5, patience=3, use_amp=True):
    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(0.9,0.99))
    scaler = GradScaler(enabled=(use_amp and device.type=="cuda"))
    best=float("inf"); best_ep=-1; ckpt=Path("models/tsgpt_split1.pt")
    history=[]
    for ep in range(1, epochs+1):
        tr = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)
        val = evaluate(model, val_loader, device)
        history.append({"epoch":ep,"train_mae":tr,"val_mae":val["mae"],"val_smape":val["smape"]})
        print(f"Epoch {ep:02d}  train_mae={tr:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}")
        if val["mae"] < best - 1e-6:
            best = val["mae"]; best_ep=ep
            torch.save({"model": model.state_dict(),
                        "epoch": ep,
                        "config": {"ctx":CTX,"d_model":D_MODEL,"n_head":N_HEAD,"n_layer":N_LAYER,
                                   "d_ff":D_FF,"dropout":DROPOUT,"feats":FEATS,"huber":USE_HUBER}}, ckpt)
        elif ep - best_ep >= patience:
            print(f"Early stopping at epoch {ep} (best {best:.5f} @ {best_ep})")
            break
    return history, best, best_ep, ckpt

history, best, best_ep, ckpt = fit(model, train_loader, val_loader, epochs=10, lr=2e-3, wd=1e-5, patience=3, use_amp=True)
print("Best val_mae:", best, "| epoch:", best_ep, "| saved:", ckpt.exists())

# Save metrics CSV & quick plot data
pd.DataFrame(history).to_csv("reports/tsgpt_train_curve.csv", index=False)
final = evaluate(model, val_loader, device)
pd.DataFrame([{"split":1,"model":"tsgpt","ctx":CTX,"val_mae":final["mae"],"val_smape":final["smape"],
               "params_M": round(sum(p.numel() for p in model.parameters())/1e6, 3),"best_epoch":best_ep}]).to_csv(
    "reports/tsgpt_split1_metrics.csv", index=False)
final
```

### 5) Quick loss plot (optional in‑class visualization)

```python
import matplotlib.pyplot as plt, pandas as pd
cur = pd.read_csv("reports/tsgpt_train_curve.csv")
plt.figure(figsize=(6,3.5))
plt.plot(cur["epoch"], cur["train_mae"], marker="o", label="train MAE")
plt.plot(cur["epoch"], cur["val_mae"], marker="s", label="val MAE")
plt.xlabel("Epoch"); plt.ylabel("MAE"); plt.title("TS-GPT training"); plt.legend(); plt.tight_layout()
plt.savefig("docs/figs/tsgpt_train_curve.png", dpi=160)
"Saved docs/figs/tsgpt_train_curve.png"
```

---

## Wrap‑up (10 min) — emphasize these points

* **Feature projection ≈ token embeddings** for real‑valued inputs; **positional embeddings + causal mask** stay the same.
* Use **TRAIN‑fit scaler** to avoid leakage; keep the **rolling‑origin split** and **embargo** from previous sessions.
* For noisy returns, **Huber/MAE** often stabilize training better than MSE.
* Save checkpoints & metrics; we’ll ablate config choices (context/head/dropout) in the homework.

---

## Homework (due before Session 23)

**Goal:** Run a small ablation on **context length (32 vs 64)**, **dropout (0.0 vs 0.1)**, and **heads (2 vs 4)**, then summarize results in one table and add a short discussion to your Quarto report.

### A. Training script: `scripts/tsgpt_train.py` (configurable TS‑GPT)

```python
#!/usr/bin/env python
from __future__ import annotations
import argparse, json, math, random
from pathlib import Path
import numpy as np, pandas as pd, torch, torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader

# --------- Dataset & scaler (compact) ----------
class FeatureScaler:
    def __init__(self): self.mean_=None; self.std_=None
    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self
    def transform(self, X): return (X - self.mean_) / self.std_

class WindowedTS(Dataset):
    def __init__(self, df, feats, T=64, scaler=None, ticker2id=None):
        self.T=T; self.feats=list(feats); self.idx=[]; self.g={}
        if ticker2id is None:
            cats=df["ticker"].astype("category").cat.categories.tolist()
            self.ticker2id={t:i for i,t in enumerate(cats)}
        else: self.ticker2id=dict(ticker2id)
        for tkr,g in df.groupby("ticker"):
            g=g.sort_values("date").reset_index(drop=True)
            X=g[self.feats].to_numpy("float32"); y=g["r_1d"].to_numpy("float32")
            for end in range(T-1,len(g)):
                if np.isfinite(y[end]): self.idx.append((str(tkr), end, self.ticker2id[str(tkr)]))
            self.g[str(tkr)]={"X":X,"y":y}
        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.g[t]["X"] for t in self.g],0))
    def __len__(self): return len(self.idx)
    def __getitem__(self,i):
        tkr,end,tid=self.idx[i]; g=self.g[tkr]
        X=self.scaler.transform(g["X"][end-self.T+1:end+1]); y=g["y"][end]
        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), torch.tensor(tid, dtype=torch.long)

def make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    i=train_min-1; out=[]
    while True:
        if i>=len(u): break
        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve>=len(u): break
        out.append((a,b,u[vs],u[ve])); i+=step
    return out

# --------- Model ----------
class CausalSelfAttention(nn.Module):
    def __init__(self, d_model, n_head, ctx, p=0.0):
        super().__init__(); assert d_model % n_head == 0
        self.nh=n_head; dh=d_model//n_head; self.dh=dh; self.ctx=ctx
        self.qkv=nn.Linear(d_model,3*d_model,bias=False); self.proj=nn.Linear(d_model,d_model,bias=False)
        self.ad=nn.Dropout(p); self.rd=nn.Dropout(p)
        self.register_buffer("mask", torch.tril(torch.ones(ctx,ctx)).unsqueeze(0).unsqueeze(0))
    def forward(self,x):
        B,T,C=x.shape; qkv=self.qkv(x); q,k,v=qkv.split(C,dim=2)
        q=q.view(B,T,self.nh,self.dh).transpose(1,2); k=k.view(B,T,self.nh,self.dh).transpose(1,2); v=v.view(B,T,self.nh,self.dh).transpose(1,2)
        att=(q @ k.transpose(-2,-1))/math.sqrt(self.dh); att=att.masked_fill(self.mask[:,:,:T,:T]==0, float("-inf"))
        att=att.softmax(dim=-1); att=self.ad(att); y=att @ v; y=y.transpose(1,2).contiguous().view(B,T,C)
        return self.rd(self.proj(y))

class Block(nn.Module):
    def __init__(self, d_model, n_head, ctx, d_ff, p=0.0):
        super().__init__()
        self.ln1=nn.LayerNorm(d_model); self.att=CausalSelfAttention(d_model,n_head,ctx,p)
        self.ln2=nn.LayerNorm(d_model); self.mlp=nn.Sequential(nn.Linear(d_model,d_ff), nn.GELU(), nn.Dropout(p), nn.Linear(d_ff,d_model), nn.Dropout(p))
    def forward(self,x): x=x+self.att(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x

class TimeSeriesGPT(nn.Module):
    def __init__(self, in_f, ctx=64, d_model=64, n_head=2, n_layer=2, d_ff=128, p=0.0, n_tickers=None, d_ticker=0):
        super().__init__(); self.ctx=ctx
        self.proj=nn.Linear(in_f,d_model); self.pos=nn.Embedding(ctx,d_model)
        self.id_emb=nn.Embedding(n_tickers,d_ticker) if (n_tickers and d_ticker>0) else None
        aug=d_model+(d_ticker if self.id_emb else 0)
        self.blocks=nn.ModuleList([Block(aug,n_head,ctx,d_ff,p) for _ in range(n_layer)])
        self.ln=nn.LayerNorm(aug); self.head=nn.Linear(aug,1)
        self.apply(self._init)
    def _init(self,m):
        if isinstance(m,nn.Linear): nn.init.xavier_uniform_(m.weight)
        if isinstance(m,nn.Embedding): nn.init.normal_(m.weight,0.0,0.02)
    def forward(self,x,tid=None):
        B,T,F=x.shape; pos=torch.arange(T, device=x.device)
        h=self.proj(x)+self.pos(pos)[None,:,:]
        if self.id_emb is not None and tid is not None:
            e=self.id_emb(tid).unsqueeze(1).expand(-1,T,-1); h=torch.cat([h,e],dim=-1)
        for blk in self.blocks: h=blk(h)
        h=self.ln(h); return self.head(h[:,-1,:]).squeeze(-1)

def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--features", default="data/processed/features_v1.parquet")
    ap.add_argument("--context", type=int, default=64)
    ap.add_argument("--d_model", type=int, default=64)
    ap.add_argument("--n_head", type=int, default=2)
    ap.add_argument("--n_layer", type=int, default=2)
    ap.add_argument("--d_ff", type=int, default=128)
    ap.add_argument("--dropout", type=float, default=0.0)
    ap.add_argument("--epochs", type=int, default=10)
    ap.add_argument("--batch", type=int, default=256)
    ap.add_argument("--lr", type=float, default=2e-3)
    ap.add_argument("--patience", type=int, default=3)
    ap.add_argument("--tickers", type=int, default=10)
    ap.add_argument("--out", default="reports/tsgpt_metrics.csv")
    args=ap.parse_args()

    # Load data
    f_static = Path("data/processed/features_v1_static.parquet")
    df = pd.read_parquet(f_static) if f_static.exists() else pd.read_parquet(args.features)
    df=df.sort_values(["ticker","date"]).reset_index(drop=True); df["ticker"]=df["ticker"].astype("category")
    cand=["log_return","lag1","lag2","lag3","zscore_20","roll_std_20","weekday","month"]
    feats=[c for c in cand if c in df.columns]; assert "r_1d" in df.columns and feats
    keep=df["ticker"].cat.categories.tolist()[:args.tickers]; df=df[df["ticker"].isin(keep)].copy()

    # Split
    def splits(dates, train_min=252, val=63, step=63, embargo=5):
        u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]
        while True:
            if i>=len(u): break
            a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val-1
            if ve>=len(u): break
            out.append((a,b,u[vs],u[ve])); i+=step
        return out
    a,b,c,d = splits(df["date"])[0]
    tr=df[(df["date"]>=a)&(df["date"]<=b)]; va=df[(df["date"]>=c)&(df["date"]<=d)]

    # Datasets
    tr_ds=WindowedTS(tr, feats, T=args.context); va_ds=WindowedTS(va, feats, T=args.context, scaler=tr_ds.scaler, ticker2id=tr_ds.ticker2id)
    pin=torch.cuda.is_available()
    def _seed(_): ws=torch.initial_seed()%2**32; np.random.seed(ws); random.seed(ws)
    g=torch.Generator(); g.manual_seed(42)
    tr_ld=DataLoader(tr_ds, batch_size=args.batch, shuffle=True, drop_last=True, num_workers=2, pin_memory=pin, worker_init_fn=_seed, generator=g)
    va_ld=DataLoader(va_ds, batch_size=args.batch, shuffle=False, drop_last=False, num_workers=2, pin_memory=pin, worker_init_fn=_seed)

    device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
    net=TimeSeriesGPT(len(feats), ctx=args.context, d_model=args.d_model, n_head=args.n_head, n_layer=args.n_layer, d_ff=args.d_ff, p=args.dropout).to(device)
    opt=torch.optim.AdamW(net.parameters(), lr=args.lr, weight_decay=1e-5, betas=(0.9,0.99))
    huber=torch.nn.HuberLoss(delta=0.01)

    def mae_t(y,yhat): return torch.mean(torch.abs(y - yhat))
    def smape_t(y,yhat,eps=1e-8): return torch.mean(2*torch.abs(y-yhat)/(torch.abs(y)+torch.abs(yhat)+eps))

    best=1e9; best_ep=0
    for ep in range(1, args.epochs+1):
        net.train()
        for xb,yb,_ in tr_ld:
            xb=xb.to(device).float(); yb=yb.to(device).float()
            opt.zero_grad(set_to_none=True)
            yhat=net(xb); loss=huber(yhat,yb); loss.backward()
            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)
            opt.step()
        # val
        net.eval(); m_mae=m_smape=0.0; n=0
        with torch.no_grad():
            for xb,yb,_ in va_ld:
                xb=xb.to(device).float(); yb=yb.to(device).float()
                yhat=net(xb); bs=xb.size(0)
                m_mae += mae_t(yb,yhat).item()*bs; m_smape += smape_t(yb,yhat).item()*bs; n+=bs
        val_mae=m_mae/n; val_smape=m_smape/n
        print(f"Epoch {ep:02d}  val_mae={val_mae:.5f}  val_sMAPE={val_smape:.5f}")
        if val_mae < best-1e-6: best=val_mae; best_ep=ep
        elif ep-best_ep >= args.patience: break

    Path("reports").mkdir(exist_ok=True)
    pd.DataFrame([{"model":"tsgpt","ctx":args.context,"d_model":args.d_model,"n_head":args.n_head,
                   "n_layer":args.n_layer,"dropout":args.dropout,"val_mae":best,"best_epoch":best_ep}]).to_csv(args.out, index=False)
    print("Wrote", args.out)

if __name__ == "__main__":
    main()
```

Run examples:

```bash
%%bash
chmod +x scripts/tsgpt_train.py
python scripts/tsgpt_train.py --context 32 --dropout 0.0 --n_head 2 --n_layer 2 --out reports/tsgpt_run_a.csv
python scripts/tsgpt_train.py --context 64 --dropout 0.1 --n_head 4 --n_layer 2 --out reports/tsgpt_run_b.csv
```

### B. Summarize ablations into one table

```python
import pandas as pd, glob
paths = glob.glob("reports/tsgpt_run_*.csv")
rows = []
for p in paths:
    try:
        r = pd.read_csv(p).iloc[0].to_dict(); r["run"] = p; rows.append(r)
    except Exception as e:
        print("Skip", p, e)
ab = pd.DataFrame(rows).sort_values("val_mae")
ab.to_csv("reports/tsgpt_ablation_summary.csv", index=False)
ab.head(10)
```

### C. Add to Quarto report (one section)

In `reports/eda.qmd` (or `reports/ts_gpt.qmd`), add:

````markdown
## Tiny Transformer for Time Series

**Split 1** results for TS‑GPT (sequence‑to‑one regression).

```{python}
import pandas as pd
print(pd.read_csv("reports/tsgpt_split1_metrics.csv"))
print(pd.read_csv("reports/tsgpt_ablation_summary.csv").sort_values("val_mae").head(8))
````

![TS‑GPT training curve](../docs/figs/tsgpt_train_curve.png)

````

### D. Minimal tests (protect causal mask & window shapes)
```python
# tests/test_tsgpt_mask_and_shapes.py
import torch, pandas as pd
from pathlib import Path

def test_mask_is_causal():
    from scripts.tsgpt_train import TimeSeriesGPT
    m = TimeSeriesGPT(in_f=4, ctx=16, d_model=32, n_head=2, n_layer=1, d_ff=64, p=0.0)
    att = [b.att for b in m.blocks][0]
    M = att.mask[0,0]
    assert torch.all(M.triu(diagonal=1)==0)

def test_window_shape():
    df = pd.read_parquet("data/processed/features_v1.parquet").sort_values(["ticker","date"])
    feats = [c for c in ["log_return","lag1","lag2","lag3"] if c in df.columns]
    assert feats
    from scripts.tsgpt_train import WindowedTS, make_splits
    a,b,c,d = make_splits(df["date"])[0]
    ds = WindowedTS(df[(df["date"]>=a)&(df["date"]<=b)], feats, T=32)
    X, y, tid = ds[0]
    assert X.shape == (32, len(feats))
    assert torch.isfinite(torch.tensor(y)).item() == 1
````

Run:

```bash
%%bash
pytest -q -k tsgpt_mask_and_shapes
```

### E. (Optional) Makefile targets

Append to `Makefile`:

```make
.PHONY: train-tsgpt ablate-tsgpt
train-tsgpt: ## Train TS-GPT on split 1 (tiny config)
\tpython scripts/tsgpt_train.py --context 64 --n_head 2 --n_layer 2 --dropout 0.0 --out reports/tsgpt_run_base.csv

ablate-tsgpt: ## Run two small ablations
\tpython scripts/tsgpt_train.py --context 32 --n_head 2 --dropout 0.0 --out reports/tsgpt_run_a.csv
\tpython scripts/tsgpt_train.py --context 64 --n_head 4 --dropout 0.1 --out reports/tsgpt_run_b.csv
```

---

## Instructor checklist (before class)

* Ensure at least `log_return`, `r_1d`, and a few lags are present, or rely on fallback generation.
* Dry‑run the tiny config on CPU or GPU; confirm val MAE decreases in <5 minutes.
* Prepare one slide mapping “char embedding” → “feature projection”.

## Emphasize while teaching

* **Causality and alignment** are unchanged from char‑GPT; only the **input embedding** changes.
* **Scaling on TRAIN only**; never refit on VAL/TEST.
* Report at least **MAE and sMAPE**; add calibration by regime later if desired.

## Grading (pass/revise)

* `models/tsgpt_split1.pt` and `reports/tsgpt_split1_metrics.csv` exist.
* Students ran at least **two ablations** and wrote `reports/tsgpt_ablation_summary.csv`.
* Tests pass (`test_tsgpt_mask_and_shapes`).
* Quarto report updated with numbers and the training curve.

You now have a **tiny GPT adapted to time series**, ready to be compared to your GRU/LSTM baselines and incorporated into the unified multi‑asset pipeline.
