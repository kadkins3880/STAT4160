---
title: "Session 24 — Reproducibility audit & optional FastAPI"
---
Below is a complete lecture package for **Session 24 — Reproducibility audit & optional FastAPI** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, a lightweight **reproducibility audit script**, and **homework with code** (tagging `v1.0-rc` + a simple changelog generator + optional FastAPI “hello score” service).

> **Assumptions**
>
> * You’ve packaged your repo in Session 23 (editable install works: `pip install -e .`).
> * You have a working CLI (`python -m projectname.cli ...`).
> * Students are on Colab (Drive mounted) or local. We’ll keep everything CPU‑fast.
> * Finance content remains **educational**, not trading advice.

---

## Session 24 — Reproducibility audit & optional FastAPI (75 min)

### Learning goals

By the end, students can:

1. Verify **end‑to‑end** reproducibility from a clean environment using a checklist and an automated script.
2. Create and persist a **run manifest** (commit hash, env, data checksums, seeds).
3. (Optional) Stand up a **FastAPI** “hello score” endpoint that wraps the package scoring logic.

---

## Agenda (75 min)

* **(12 min)** Slides: what “reproducible” means; seeds, lockfiles, data snapshots, deterministic flags.
* **(8 min)** Slides: manifests & checksums; common pitfalls; CI scope (keep fast).
* **(35 min)** **In‑class lab**: run the audit, generate a manifest, compare two runs, file issues.
* **(10 min)** (Optional) FastAPI “hello score” demo.
* **(10 min)** Wrap‑up & homework brief.

---

## Slide notes (paste into your deck)

### What “reproducible” means for this course

* Fresh‑clone → `make env && pip install -e . && python -m projectname.cli score ...` **succeeds**.
* Outputs (metrics/CSV) are **identical** or within a **tiny tolerance** on the same hardware/runtime.
* **Seeds** set consistently; **deterministic flags** used where available (Torch).
* **Data is fixed**: versioned snapshot (in repo or via LFS) **or** cached API responses with **checksums**.

### Lockfiles & manifests

* Keep **requirements.txt pinned** (exact versions) and also store a **freeze**: `pip freeze > requirements-lock.txt`.
* Record a **manifest** per run: Git commit, dirty flag, Python & lib versions, seed values, config, data file hashes, CLI args. Save as `reports/manifest.json`.

### Deterministic training & scoring

* Torch: `torch.backends.cudnn.deterministic=True`, `benchmark=False`, `torch.use_deterministic_algorithms(True)` (when possible).
* DataLoader: seeded generator + `worker_init_fn` to seed workers.
* Sort/group data deterministically (`sort_values(["ticker","date"])`).

### Common pitfalls

* Hidden internet calls (APIs) during scoring.
* Unpinned dependencies.
* Missing seeds in numpy, random, torch.
* Dirty working tree (uncommitted changes).

---

## In‑class lab (35 min)

> Run each block as its **own cell** in Colab. Replace `REPO_NAME` if needed. Partners **pair‑audit** and open issues for anything not reproducible.

### 0) Mount & prepare dirs

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME = "unified-stocks-teamX"  # <- change
BASE_DIR  = "/content/drive/MyDrive/dspt25"
REPO_DIR  = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, sys, platform, subprocess, json, hashlib, time
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
for p in ["reports","models","scripts","docs","docs/figs","data/processed","data/raw",".github/workflows"]:
    pathlib.Path(p).mkdir(parents=True, exist_ok=True)
print("Working dir:", os.getcwd(), "| Python:", sys.version.split()[0], "| OS:", platform.system())
```

### 1) Add a **Repro Audit Checklist** to the repo

Create `docs/repro_audit_checklist.md`:

```markdown
# Reproducibility Audit Checklist (Session 24)

## Environment
- [ ] Repo fresh-clones and installs: `make env && pip install -e .`
- [ ] `requirements.txt` is pinned (== versions); `requirements-lock.txt` exists
- [ ] Python version and OS noted in README

## Data
- [ ] `data/processed/*` has checksums recorded
- [ ] No hidden network calls during scoring (`offline`/cached)
- [ ] Train/val split code is deterministic and documented

## Seeds & determinism
- [ ] Seeds set for `random`, `numpy`, `torch`; DataLoader worker init seeded
- [ ] Torch deterministic flags set; no non-deterministic ops used
- [ ] Sorting by `["ticker","date"]` before windowing

## CLI & artifacts
- [ ] `python -m projectname.cli split-info` works
- [ ] `python -m projectname.cli score --model lin_lags` writes CSV
- [ ] Two back-to-back runs produce identical metrics (within tolerance)
- [ ] A `reports/manifest.json` records commit hash, env, checksums, CLI args

## CI (fast)
- [ ] A small CI job runs `python -m projectname.cli score --model lin_lags`
- [ ] CI artifacts uploaded or printed

## Known deviations (explain/ticket links)
- [ ] ...
```

### 2) Add a **simple data checksum** helper (one cell to create the file)

```python
from pathlib import Path
import hashlib, json, glob

def file_sha256(path: str|Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1<<20), b""):
            h.update(chunk)
    return h.hexdigest()

checks = {}
for pat in ["data/processed/*.parquet","data/raw/*.parquet","data/*.db"]:
    for p in glob.glob(pat):
        checks[p] = {"sha256": file_sha256(p), "bytes": Path(p).stat().st_size}

Path("reports/data_checksums.json").write_text(json.dumps(checks, indent=2))
print("Wrote reports/data_checksums.json with", len(checks), "entries")
```

### 3) **Automated reproducibility audit** script

Create `scripts/repro_audit.py`:

```python
#!/usr/bin/env python
from __future__ import annotations
import os, sys, json, time, platform, subprocess, hashlib, glob
from pathlib import Path

TOL = 1e-12  # numerical tolerance for equality of metrics

def sha256_text(s: str) -> str:
    import hashlib
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def run(cmd: list[str]) -> tuple[int, str, str]:
    p = subprocess.run(cmd, capture_output=True, text=True)
    return p.returncode, p.stdout.strip(), p.stderr.strip()

def git_info() -> dict:
    def _get(args):
        rc,out,err = run(["git"]+args)
        return out if rc==0 else f"NA({err})"
    return {
        "commit": _get(["rev-parse","HEAD"]),
        "dirty": _get(["status","--porcelain"]) != "",
        "branch": _get(["rev-parse","--abbrev-ref","HEAD"]),
        "remote": _get(["remote","-v"]),
    }

def pip_freeze() -> str:
    rc,out,err = run([sys.executable,"-m","pip","freeze","--exclude-editable"])
    return out

def read_csv_first_row(path: Path) -> dict:
    import pandas as pd
    df = pd.read_csv(path)
    if len(df)==0:
        return {}
    return df.iloc[:1].to_dict(orient="records")[0]

def main():
    start = time.time()
    manifest = {
        "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
        "python": sys.version.split()[0],
        "platform": platform.platform(),
        "git": git_info(),
    }

    # Add freeze & hash
    freeze = pip_freeze()
    Path("reports").mkdir(exist_ok=True)
    Path("requirements-lock.txt").write_text(freeze)
    manifest["pip_freeze_sha256"] = sha256_text(freeze)

    # Data checksums (optional precomputed)
    checks_path = Path("reports/data_checksums.json")
    if checks_path.exists():
        manifest["data_checksums"] = json.loads(checks_path.read_text())
    else:
        manifest["data_checksums"] = {}

    # 1) Split info should work
    rc,out,err = run([sys.executable,"-m","projectname.cli","split-info"])
    manifest["split_info_ok"] = (rc==0)
    if rc!=0:
        print("split-info failed:", err, file=sys.stderr)

    # 2) Score twice (lin_lags) and compare metrics
    out1 = Path("reports/cli_linlags_run1.csv"); out2 = Path("reports/cli_linlags_run2.csv")
    cmd = [sys.executable,"-m","projectname.cli","score","--model","lin_lags","--out",str(out1)]
    rc1,_,err1 = run(cmd)
    rc2,_,err2 = run([*cmd[:-1], str(out2)])
    manifest["score_run1_ok"] = (rc1==0); manifest["score_run2_ok"] = (rc2==0)
    if rc1!=0 or rc2!=0:
        print("score failed:", err1 or err2, file=sys.stderr)

    # Compare metrics (read printed CSVs)
    import pandas as pd
    ok_equal = True
    try:
        d1 = pd.read_csv(out1)
        d2 = pd.read_csv(out2)
        # Aggregate micro MAE across both runs
        import numpy as np
        mae1 = float(np.mean(np.abs(d1["y_true"].values - d1["yhat"].values)))
        mae2 = float(np.mean(np.abs(d2["y_true"].values - d2["yhat"].values)))
        manifest["metrics"] = {"run1_mae": mae1, "run2_mae": mae2, "abs_diff": abs(mae1-mae2)}
        ok_equal = abs(mae1 - mae2) <= TOL
    except Exception as e:
        ok_equal = False
        manifest["metrics_error"] = repr(e)

    manifest["ok_equal_within_tol"] = ok_equal
    manifest["elapsed_sec"] = round(time.time()-start, 2)

    mpath = Path("reports/manifest.json")
    mpath.write_text(json.dumps(manifest, indent=2))
    print("Wrote", mpath)

    # Non-zero exit on failure
    if not (manifest["split_info_ok"] and manifest["score_run1_ok"] and manifest["score_run2_ok"] and ok_equal):
        print("Repro audit FAILED (see reports/manifest.json).", file=sys.stderr)
        sys.exit(2)
    else:
        print("Repro audit PASSED.")

if __name__ == "__main__":
    main()
```

Make it executable and run:

```bash
%%bash
chmod +x scripts/repro_audit.py
python scripts/repro_audit.py || true
```

You should see `reports/manifest.json`, `requirements-lock.txt`, and two `reports/cli_linlags_run*.csv`. If the audit fails, **open a GitHub issue** with the error and your manifest attached.

### 4) Add Makefile + CI snippets (fast)

Append to `Makefile`:

```make
.PHONY: lock repro-audit
lock: ## Write requirements-lock.txt from pip freeze
\tpip freeze --exclude-editable > requirements-lock.txt

repro-audit: ## Run fast reproducibility audit (lin_lags only)
\tpython scripts/repro_audit.py
```

Add a tiny CI job (append to `.github/workflows/ci.yml`):

```yaml
- name: Repro audit (fast)
  run: |
    pip install -e ".[dev]"
    python scripts/repro_audit.py
```

---

## (Optional) FastAPI “hello score” (10 min in class; copy‑paste runnable)

### A. Minimal service

Create `serve/app.py`:

```python
# serve/app.py
from __future__ import annotations
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel
from pathlib import Path
import pandas as pd
from projectname.config import load_config
from projectname.utils.splits import make_rolling_origin_splits
from projectname.utils.metrics import mae, smape
from projectname.models.baselines import predict_naive, predict_lin_lags

app = FastAPI(title="Unified Stocks Scoring (Educational)")

class ScoreRequest(BaseModel):
    model: str = "lin_lags"          # "naive" | "lin_lags"
    split: int = 1                   # use rolling-origin split id
    start: str | None = None         # optional override
    end: str | None = None

def _load_df(cfg):
    df = pd.read_parquet(cfg.data.features_file)
    df["date"] = pd.to_datetime(df["date"])
    df["ticker"] = df["ticker"].astype("category")
    return df.sort_values(["ticker","date"]).reset_index(drop=True)

@app.get("/health")
def health():
    import subprocess
    def _g(args): 
        try: return subprocess.check_output(["git"]+args, text=True).strip()
        except Exception: return "NA"
    return {"status":"ok","commit":_g(["rev-parse","HEAD"])}

@app.post("/score")
def score(req: ScoreRequest):
    cfg = load_config("config/config.yaml")
    df = _load_df(cfg)
    feats = [c for c in cfg.features_use if c in df.columns]

    if req.start and req.end:
        vstart, vend = pd.to_datetime(req.start), pd.to_datetime(req.end)
        dates = sorted(df["date"].unique())
        # train = everything before vstart respecting train_min (no embargo here for simplicity)
        idx = next((i for i,d in enumerate(dates) if d>=vstart), None)
        if idx is None or idx < cfg.eval.train_min:
            raise HTTPException(400, detail="Not enough history before start.")
        tstart, tend = dates[0], dates[idx-1]
    else:
        splits = make_rolling_origin_splits(df["date"], cfg.eval.train_min, cfg.eval.val_size, cfg.eval.val_size, cfg.eval.embargo)
        try: tstart, tend, vstart, vend = splits[req.split-1]
        except IndexError: raise HTTPException(400, detail="Split not available.")

    train_df = df[(df["date"]>=tstart)&(df["date"]<=tend)].copy()
    val_df   = df[(df["date"]>=vstart)&(df["date"]<=vend)].copy()
    if len(val_df)==0: raise HTTPException(400, detail="Empty validation slice")

    if req.model == "naive":
        preds = predict_naive(val_df)
    elif req.model == "lin_lags":
        if not feats: raise HTTPException(400, detail="No feature columns available.")
        preds = predict_lin_lags(train_df, val_df, feats)
    else:
        raise HTTPException(400, detail="Model not supported in demo service")

    y, yhat = preds["y_true"], preds["yhat"]
    return {"model": req.model,
            "split": req.split,
            "start": str(vstart.date()), "end": str(vend.date()),
            "n": int(len(preds)),
            "mae": mae(y,yhat), "smape": smape(y,yhat)}
```

Install & run locally:

```bash
%%bash
pip install fastapi "uvicorn[standard]"
python -m uvicorn serve.app:app --reload --port 8000
```

Test (from another shell):

```bash
# health
curl -s localhost:8000/health | jq
# score
curl -s -X POST localhost:8000/score -H "Content-Type: application/json" \
     -d '{"model":"lin_lags","split":1}' | jq
```

> **Note:** In Colab you can run uvicorn, but exposing the port publicly requires a tunnel; treat this as **local optional**.

---

## Wrap‑up (10 min)

* Reproducibility is a **discipline**, not an afterthought: lock the env, freeze data, write manifests, and keep the audit **fast**.
* CI should run the **fast path** (lin\_lags) to guard your repo.
* Optional FastAPI shows how to wrap your scoring pipeline for interactive demos.

---

## Homework (due before Session 25)

**Goal:** Produce a **release candidate** with a reproducibility statement, tag `v1.0-rc`, and resolve/triage audit issues.

### A. Update README (repro statement)

Add a “Reproducibility” section to `README.md`:

```markdown
## Reproducibility

- Fresh clone:
```

make env
pip install -e ".\[dev]"
python -m projectname.cli split-info
python -m projectname.cli score --model lin\_lags --out reports/cli\_linlags.csv

```
- Audit:
```

python scripts/repro\_audit.py

```
- We commit `requirements.txt` (pinned) and `requirements-lock.txt` (pip freeze hash: see `reports/manifest.json`).
- Data checksums in `reports/data_checksums.json`.
```

### B. Generate a simple changelog automatically

Create `scripts/make_changelog.py`:

```python
#!/usr/bin/env python
import subprocess, datetime
from pathlib import Path

def run(args): 
    return subprocess.check_output(args, text=True).strip()

def main():
    since = "v0.0.0"
    # If previous tag exists, use it
    try:
        last = run(["git","describe","--tags","--abbrev=0"])
        since = last
    except Exception:
        pass
    log = run(["git","log",f"{since}..HEAD","--pretty=format:- %h %s (%an)"])
    today = datetime.date.today().isoformat()
    header = f"## {today} v1.0-rc\n\n"
    body = header + (log if log else "- Initial release candidate\n")
    p = Path("CHANGELOG.md")
    if p.exists():
        old = p.read_text()
        p.write_text(body + "\n" + old)
    else:
        p.write_text("# Changelog\n\n" + body)
    print("Updated CHANGELOG.md")

if __name__ == "__main__":
    main()
```

Run:

```bash
%%bash
chmod +x scripts/make_changelog.py
python scripts/make_changelog.py
git add CHANGELOG.md
git commit -m "chore: update changelog for v1.0-rc"
```

### C. Create & push the tag

```bash
%%bash
git tag -a v1.0-rc -m "Release candidate for reproducible pipeline"
git push origin v1.0-rc
```

### D. Close or triage audit issues

* For each checklist failure, open or update a GitHub issue with:

  * Error snippet, your `reports/manifest.json`, steps to reproduce.
  * Label: `repro`, `rc`.
  * Either **closed** with a fix (commit hash referenced) or **triaged** with scope and owner.

### E. (Optional) Add a CI artifact step to upload the manifest

Append to `.github/workflows/ci.yml`:

```yaml
- name: Upload manifest
  uses: actions/upload-artifact@v4
  with:
    name: repro-manifest
    path: reports/manifest.json
```

---

## Instructor checklist (before class)

* Dry‑run `scripts/repro_audit.py` on a fresh runtime; verify it passes.
* Ensure `projectname.cli` works without internet (no hidden API calls).
* Be ready to demo FastAPI locally if time permits.

## Grading (pass/revise)

* `scripts/repro_audit.py` runs and writes `reports/manifest.json`.
* `requirements-lock.txt` present and hash recorded in manifest.
* At least one **issue** opened (if a failure) or a note “All checks passed” with manifest attached.
* `CHANGELOG.md` updated by script; tag `v1.0-rc` exists and is pushed.
* README has a clear **repro statement**.

---

### Notes and gotchas

* **Tolerance** for metric equality is set very tight (`1e-12`). If your environment shows tiny FP drift, relax to `1e-9`.
* If you rely on **GPU** kernels that are non‑deterministic, keep the audit on **CPU** or replace those ops.
* If any **models/…pt** checkpoints are referenced, keep them out of the audit path (we only audit **scoring with lin\_lags** here to keep CI fast).

This session gets you to a **release‑candidate** state: a repo that **fresh‑clones and scores deterministically**, with a **manifest**, a **changelog**, and an optional **service wrapper**, ready for Session 25’s communication & poster work.
