---
title: "Session 17 — Feature Timing, Biases & Leakage"
---
Below is a complete lecture package for **Session 17 — Feature Timing, Biases & Leakage** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll **freeze a static ticker universe** (avoid survivorship bias), **formalize label definitions** (t+1 and multi‑step), and add a **leakage test suite** that fails if any feature at time *t* uses information from *t+1* or later.

> **Educational use only — not trading advice.**
> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) with `data/processed/returns.parquet` and `data/processed/features_v1.parquet` from Sessions 9–10. Cells include safe fallbacks when files are missing.

---

## Session 17 — Feature Timing, Biases & Leakage (75 min)

### Learning goals

By the end of class, students can:

1. Explain and **avoid look‑ahead** and **survivorship** biases.
2. Freeze and use a **static ticker universe** chosen from the **train window** (not the whole history).
3. Define labels correctly (e.g., **t+1** and **t+5**) and verify them with tests.
4. Add **leakage tests** that recompute trusted features and fail on any future‑peek.

---

## Agenda (75 min)

* **(10 min)** Slides: what leakage looks like; examples; how it sneaks in
* **(10 min)** Slides: survivorship bias (today’s constituents ≠ past reality); freezing a universe
* **(10 min)** Slides: label definitions (t+1, multi‑step) and alignment rules
* **(35 min)** **In‑class lab**:

  1. Freeze a static universe from the first split’s train window
  2. Add leakage tests that recompute known‑good features
  3. Add multi‑step labels (e.g., t+5) with tests
* **(10 min)** Wrap‑up & homework brief

---

## Slides / talking points (drop into your deck)

### What is data leakage?

* **Look‑ahead leakage:** using any info from *t+1* or later to compute features at *t* or to scale/normalize train and validation together.
* **Common culprits:** `shift(-1)` in features, global scaling fit on full data, forward‑fill across split boundaries, using today’s close to predict today’s close.

### Survivorship bias

* Using **today’s index membership** to pick tickers for the past ⇒ drops delisted/removed names ⇒ **optimistically biased** results.
* **Cure:** freeze a **static universe** from the **training window** (e.g., all tickers with ≥ 252 observations by the end of the first train window). Save it and **filter by it** for all future experiments.

### Label definitions (be explicit)

* **t+1 log return**: `r_1d = log_return.shift(-1)` per ticker (your Session‑9 label).
* **t+5 log return** (multi‑step): `r_5d = log_return.shift(-1) + … + log_return.shift(-5)` per ticker.
* Rules: labels come from **future**; features come from **≤ t**. Splits with **embargo** reduce adjacency leakage.

---

## In‑class lab (35 min, Colab‑friendly)

> Run each block as its own cell. Update `REPO_NAME` as needed.

### 0) Setup & load data (with fallbacks)

```python
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_NAME  = "unified-stocks-teamX"   # <- change if needed
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, numpy as np, pandas as pd
from pathlib import Path
pathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)
os.chdir(REPO_DIR)
for p in ["data/raw","data/processed","data/static","reports","scripts","tests"]:
    Path(p).mkdir(parents=True, exist_ok=True)
print("Working dir:", os.getcwd())

# Load returns or synthesize a small fallback
rpath = Path("data/processed/returns.parquet")
if rpath.exists():
    returns = pd.read_parquet(rpath)
else:
    rng = np.random.default_rng(0)
    dates = pd.bdate_range("2022-01-03", periods=360)
    rows=[]
    for t in ["AAPL","MSFT","GOOGL","AMZN","NVDA","TSLA","META","NFLX"]:
        eps = rng.normal(0,0.012,size=len(dates)).astype("float32")
        adj = 100*np.exp(np.cumsum(eps))
        df = pd.DataFrame({
            "date": dates, "ticker": t,
            "adj_close": adj.astype("float32"),
            "log_return": np.r_[np.nan, np.diff(np.log(adj))].astype("float32")
        })
        df["r_1d"] = df["log_return"].shift(-1)
        df["weekday"] = df["date"].dt.weekday.astype("int8")
        df["month"]   = df["date"].dt.month.astype("int8")
        rows.append(df)
    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)
    returns["ticker"] = returns["ticker"].astype("category")
    returns.to_parquet(rpath, index=False)

# Load features_v1 or construct minimal lags for tests
fpath = Path("data/processed/features_v1.parquet")
if fpath.exists():
    feats = pd.read_parquet(fpath).sort_values(["ticker","date"]).reset_index(drop=True)
else:
    feats = returns.sort_values(["ticker","date"]).copy()
    for k in [1,2,3]:
        feats[f"lag{k}"] = feats.groupby("ticker")["log_return"].shift(k)
    feats["roll_mean_20"] = feats.groupby("ticker")["log_return"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)
    feats["roll_std_20"]  = feats.groupby("ticker")["log_return"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)
    feats["zscore_20"]    = (feats["log_return"] - feats["roll_mean_20"]) / (feats["roll_std_20"] + 1e-8)
    feats = feats.dropna().reset_index(drop=True)

# Harmonize types
returns["date"] = pd.to_datetime(returns["date"])
feats["date"]   = pd.to_datetime(feats["date"])
returns["ticker"] = returns["ticker"].astype("category")
feats["ticker"]   = feats["ticker"].astype("category")
returns = returns.sort_values(["ticker","date"]).reset_index(drop=True)
feats   = feats.sort_values(["ticker","date"]).reset_index(drop=True)
returns.head(3), feats.head(3)
```

### 1) Freeze a **static universe** from the **first split’s train window**

```python
import numpy as np, pandas as pd

def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    i = train_min - 1; splits=[]
    while True:
        if i >= len(u): break
        a,b = u[0], u[i]
        vs = i + embargo + 1
        ve = vs + val_size - 1
        if ve >= len(u): break
        splits.append((a,b,u[vs],u[ve]))
        i += step
    return splits

splits = make_rolling_origin_splits(returns["date"], train_min=252, val_size=63, step=63, embargo=5)
assert len(splits) >= 1, "Not enough history for a first split."
a,b,c,d = splits[0]
print("First train window:", a.date(), "→", b.date())

# Eligible = tickers with at least train_min rows by train_end (b)
train_slice = returns[(returns["date"]>=a) & (returns["date"]<=b)]
counts = train_slice.groupby("ticker").size()
eligible = counts[counts >= 252].index.sort_values()
universe = pd.DataFrame({"ticker": eligible})
univ_name = f"data/static/universe_{b.date()}.csv"
universe.to_csv(univ_name, index=False)
print("Saved static universe:", univ_name, "| tickers:", len(universe))
universe.head()
```

> From now on, **filter** your data to `universe` before modeling/evaluation.

### 2) Apply the static universe to your features

```python
feats_static = feats[feats["ticker"].isin(set(universe["ticker"]))].copy()
feats_static.to_parquet("data/processed/features_v1_static.parquet", compression="zstd", index=False)
print("Wrote data/processed/features_v1_static.parquet", feats_static.shape)
```

### 3) Add **leakage tests** that recompute trusted features & compare

Create a high‑value test file that **fails** if any feature depends on future rows.

```python
# tests/test_leakage_features.py
from __future__ import annotations
import numpy as np, pandas as pd
import pytest

SAFE_ROLL = 20

@pytest.fixture(scope="session")
def df():
    import pandas as pd
    import pathlib
    p = pathlib.Path("data/processed/features_v1_static.parquet")
    if not p.exists():
        p = pathlib.Path("data/processed/features_v1.parquet")
    df = pd.read_parquet(p).sort_values(["ticker","date"]).reset_index(drop=True)
    df["date"] = pd.to_datetime(df["date"])
    return df

def test_label_definition_r1d(df):
    for tkr, g in df.groupby("ticker"):
        assert g["r_1d"].iloc[:-1].equals(g["log_return"].iloc[1:]), f"r_1d mismatch for {tkr}"

def _recompute_safe(g: pd.DataFrame) -> pd.DataFrame:
    # Recompute causal features using only <= t information
    out = pd.DataFrame(index=g.index)
    s = g["log_return"]
    out["lag1"] = s.shift(1)
    out["lag2"] = s.shift(2)
    out["lag3"] = s.shift(3)
    rm = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).mean()
    rs = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).std()
    out["roll_mean_20"] = rm
    out["roll_std_20"]  = rs
    out["zscore_20"]    = (s - rm) / (rs + 1e-8)
    # EWM & expanding if present
    out["exp_mean"] = s.expanding(min_periods=SAFE_ROLL).mean()
    out["exp_std"]  = s.expanding(min_periods=SAFE_ROLL).std()
    out["ewm_mean_20"] = s.ewm(span=20, adjust=False).mean()
    out["ewm_std_20"]  = s.ewm(span=20, adjust=False).std()
    # RSI(14) if adj_close present
    if "adj_close" in g:
        delta = g["adj_close"].diff()
        up = delta.clip(lower=0).ewm(alpha=1/14, adjust=False).mean()
        dn = (-delta.clip(upper=0)).ewm(alpha=1/14, adjust=False).mean()
        rs = up / (dn + 1e-12)
        out["rsi_14"] = 100 - (100/(1+rs))
    return out

@pytest.mark.parametrize("col", ["lag1","lag2","lag3","roll_mean_20","roll_std_20","zscore_20","exp_mean","exp_std","ewm_mean_20","ewm_std_20","rsi_14"])
def test_features_match_causal_recompute(df, col):
    if col not in df.columns:
        pytest.skip(f"{col} not present")
    # Compare per ticker to avoid cross-group alignment issues
    for tkr, g in df.groupby("ticker", sort=False):
        ref = _recompute_safe(g)
        if col not in ref.columns: 
            continue
        a = g[col].to_numpy()
        b = ref[col].to_numpy()
        # Allow NaNs at the start; compare where both finite
        mask = np.isfinite(a) & np.isfinite(b)
        if mask.sum() == 0: 
            continue
        diff = np.nanmax(np.abs(a[mask] - b[mask]))
        assert float(diff) <= 1e-6, f"{col} deviates from causal recompute for {tkr}: max |Δ|={diff}"

def test_no_feature_equals_target(df):
    y = df["r_1d"].to_numpy()
    for col in df.select_dtypes(include=["float32","float64"]).columns:
        if col in {"r_1d","log_return"}: 
            continue
        x = df[col].to_numpy()
        # Proportion of exact equality (within tiny tol) should not be high
        eq = np.isfinite(x) & np.isfinite(y) & (np.abs(x - y) < 1e-12)
        assert eq.mean() < 0.8, f"Suspicious: feature {col} equals target too often"
```

Run tests now:

```python
!pytest -q tests/test_leakage_features.py
```

> If a test fails, **fix the pipeline**, don’t weaken the test.

### 4) Add **multi‑step labels** (e.g., t+5) and tests

```python
# scripts/make_multistep_labels.py
from __future__ import annotations
import pandas as pd, numpy as np
from pathlib import Path

def make_multistep(in_parquet="data/processed/returns.parquet", horizons=(5,)):
    df = pd.read_parquet(in_parquet).sort_values(["ticker","date"]).reset_index(drop=True)
    for H in horizons:
        # r_Hd = sum of next H log returns: shift(-1) ... shift(-H)
        s = df.groupby("ticker")["log_return"]
        acc = None
        for h in range(1, H+1):
            sh = s.shift(-h)
            acc = sh if acc is None else (acc + sh)
        df[f"r_{H}d"] = acc
    out = df
    Path("data/processed").mkdir(parents=True, exist_ok=True)
    out.to_parquet("data/processed/returns_multistep.parquet", compression="zstd", index=False)
    print("Wrote data/processed/returns_multistep.parquet", out.shape)

if __name__ == "__main__":
    make_multistep()
```

Run it:

```python
!python scripts/make_multistep_labels.py
```

Add a test for label correctness:

```python
# tests/test_labels_multistep.py
import pandas as pd, numpy as np

def test_r5d_definition():
    df = pd.read_parquet("data/processed/returns_multistep.parquet").sort_values(["ticker","date"])
    if "r_5d" not in df.columns:
        return
    for tkr, g in df.groupby("ticker"):
        lr = g["log_return"]
        r5 = sum(lr.shift(-h) for h in range(1,6))
        diff = (g["r_5d"] - r5).abs().max()
        assert float(diff) < 1e-10, f"r_5d misdefined for {tkr} (max |Δ|={diff})"
```

Run:

```python
!pytest -q tests/test_labels_multistep.py
```

---

## Wrap‑up (10 min)

* **Static universe** removes **survivorship bias**: pick tickers with adequate history **by train end** and **stick to them**.
* Label definitions must be **explicit and tested** (t+1, t+5).
* Leakage tests **recompute causal features** and compare—if you accidentally used `shift(-1)` or cross‑split fills, tests fail.

---

## Homework (due before Session 18)

**Goal:** Document your evaluation protocol and ship a concise “leakage & bias” memo, plus a one‑command audit.

### Part A — Generate a **protocol memo** (`reports/eval_protocol.md`)

```python
# scripts/write_eval_protocol.py
from __future__ import annotations
import pandas as pd, numpy as np
from pathlib import Path
from datetime import date

def make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):
    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))
    i = train_min - 1; out=[]
    while True:
        if i >= len(u): break
        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1
        if ve >= len(u): break
        out.append((a,b,u[vs],u[ve])); i += step
    return out

def main():
    ret = pd.read_parquet("data/processed/returns.parquet").sort_values(["ticker","date"])
    splits = make_rolling_origin_splits(ret["date"])
    a,b,c,d = splits[0]
    # Universe info
    univ_files = sorted(Path("data/static").glob("universe_*.csv"))
    univ = univ_files[-1] if univ_files else None
    univ_count = pd.read_csv(univ).shape[0] if univ else ret["ticker"].nunique()
    md = []
    md += ["# Evaluation Protocol (Leakage‑Aware)", ""]
    md += ["**Date:** " + date.today().isoformat(), ""]
    md += ["## Splits", f"- Train window (split 1): **{a.date()} → {b.date()}**",
           f"- Embargo: **5** business days", f"- Validation window: **{c.date()} → {d.date()}**",
           f"- Step between origins: **63** business days", ""]
    md += ["## Static Universe", f"- Universe file: **{univ.name if univ else '(none)'}**",
           f"- Count: **{univ_count}** tickers", 
           "- Selection rule: tickers with ≥252 obs by first train end; fixed for all splits.", ""]
    md += ["## Labels", "- `r_1d` = next‑day log return `log_return.shift(-1)` per ticker.",
           "- `r_5d` (if used) = sum of `log_return.shift(-1..-5)`.", ""]
    md += ["## Leakage Controls",
           "- Features computed from ≤ t only (rolling/ewm/expanding without negative shifts).",
           "- No forward‑fill across split boundaries; embargo = 5 days.",
           "- Scalers/normalizers fit on TRAIN only.",
           "- Tests: `tests/test_leakage_features.py`, `tests/test_labels_multistep.py`.", ""]
    md += ["## Caveats",
           "- Educational dataset; not investment advice.",
           "- Survivorship minimized via static universe; still subject to data vendor quirks.", ""]
    Path("reports").mkdir(parents=True, exist_ok=True)
    Path("reports/eval_protocol.md").write_text("\n".join(md))
    print("Wrote reports/eval_protocol.md")

if __name__ == "__main__":
    main()
```

Run:

```python
!python scripts/write_eval_protocol.py
```

### Part B — One‑command **leakage audit** target

Append to your `Makefile`:

```make
.PHONY: leakage-audit
leakage-audit: ## Run leakage & label tests; write eval protocol
\tpytest -q tests/test_leakage_features.py tests/test_labels_multistep.py
\tpython scripts/write_eval_protocol.py
```

Then run:

```bash
make leakage-audit
```

### Part C — Short memo (1–2 pages max)

* Open `reports/eval_protocol.md` and add **two paragraphs** in your own words:

  1. Why these splits and embargo are credible for your task.
  2. Where leakage could still hide (e.g., future macro revisions, implicit target leakage), and how you’d detect it.

> Submit the updated `reports/eval_protocol.md` and a screenshot of `make leakage-audit` passing.

### Part D — (Optional) Quarto inclusion

Add this to your Quarto report:

````markdown
## Evaluation Protocol (Leakage‑Aware)

```{python}
from pathlib import Path
print(Path("reports/eval_protocol.md").read_text())
````

```

---

## Instructor checklist (before class)
- Ensure `returns.parquet` and `features_v1.parquet` exist or fallback works.  
- Intentionally create a leaked feature (e.g., `lag1 = log_return.shift(-1)`) on your copy to show tests **failing**, then fix.  
- Decide an anchor date policy for universe freeze; today’s lab uses **first split’s train end**.

## Emphasize while teaching
- **Define labels first**, then prove features are **causal (≤ t)**.  
- Freezing the **universe** is small effort with big impact on credibility.  
- Tests are your **guardrails**—if they go red, **don’t** relax them; fix the pipeline.

## Grading (pass/revise)
- `data/static/universe_YYYY-MM-DD.csv` created; `features_v1_static.parquet` filtered by it.  
- Leakage tests present and **green** on the clean pipeline; **red** if you inject a future‑peek.  
- `reports/eval_protocol.md` exists and includes student commentary.  
- `make leakage-audit` runs without errors.

You now have a **credibility layer** on top of your data pipeline—ready to analyze regimes and calibration next (Session 18).
```
