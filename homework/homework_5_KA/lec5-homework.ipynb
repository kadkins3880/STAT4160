{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Colab cell\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"I4_j62ifBNHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758328073740,"user_tz":300,"elapsed":3367,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"0ad57a92-5d01-4a34-8b04-7e1f66df6264"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"KQB-iMN3BDQ4","executionInfo":{"status":"ok","timestamp":1758328073745,"user_tz":300,"elapsed":7,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"outputs":[],"source":["# Adjust these two for YOUR repo\n","REPO_OWNER = \"ywanglab\"\n","REPO_NAME  = \"STAT4160\"   # e.g., unified-stocks-team1\n","\n","BASE_DIR   = \"/content/drive/MyDrive/dspt25\"\n","CLONE_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n","REPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n","\n","import os, pathlib\n","pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)\n"]},{"cell_type":"code","source":["\n","import os, subprocess, shutil, pathlib\n","\n","if not pathlib.Path(CLONE_DIR).exists():\n","    !git clone {REPO_URL} {CLONE_DIR}\n","else:\n","    # If the folder exists, just ensure it's a git repo and pull latest\n","    os.chdir(CLONE_DIR)\n","    # !git status\n","    # !git pull --rebase # !git pull --ff-only\n","os.chdir(CLONE_DIR)\n","print(\"Working dir:\", os.getcwd())"],"metadata":{"id":"n6jMrUCACz0a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758328073766,"user_tz":300,"elapsed":16,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"28e08960-8a8f-4e12-b665-8dc6d7488914"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Working dir: /content/drive/MyDrive/dspt25/STAT4160\n"]}]},{"cell_type":"markdown","source":["%%bash: Jupyter notebook magic command. use bash (Bourne Again Shell) command\n","```\n","set -euo pipefail\n","```\n","This is a common shell “strict mode” setting:\n","\n","-`e`: Exit immediately if any command returns a nonzero (error) status.\n","\n","-`u`: Treat undefined variables as errors (instead of silently treating them as empty strings).\n","\n","-`o` pipefail: If a pipeline has multiple commands (like cmd1 | cmd2), the whole pipeline fails if any command fails, not just the last one.\n","\n","Together: helps catch errors early and avoids running later commands with broken assumptions."],"metadata":{"id":"rD2OHNHlZmie"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","pwd\n","ls -la"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7UIDoWRZcR3","executionInfo":{"status":"ok","timestamp":1758328073800,"user_tz":300,"elapsed":32,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"69ff7cdf-60c0-43e2-991e-6a785347332c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dspt25/STAT4160\n","total 670\n","-rw------- 1 root root      0 Sep 20 00:22 1\n","-rw------- 1 root root      6 Sep 17 13:37 a.txt\n","drwx------ 2 root root   4096 Sep 20 00:25 data\n","drwx------ 2 root root   4096 Sep 20 00:24 docs\n","drwx------ 2 root root   4096 Sep 20 00:22 docs1\n","-rw------- 1 root root      0 Sep 17 13:37 docs.txt\n","drwx------ 2 root root   4096 Sep 20 00:24 .git\n","-rw------- 1 root root    205 Sep 20 00:22 .gitattributes\n","drwx------ 2 root root   4096 Sep 20 00:22 .github\n","-rw------- 1 root root    452 Sep 20 00:22 .gitignore\n","-rw------- 1 root root     47 Sep 17 13:29 hello.sh\n","drwx------ 2 root root   4096 Sep 20 00:24 homework\n","-rw------- 1 root root 604095 Sep 20 00:22 index.pdf\n","-rw------- 1 root root    188 Sep  7 18:39 index.qmd\n","-rw------- 1 root root   2684 Sep 20 00:24 Makefile\n","-rw------- 1 root root     23 Sep 20 00:22 myfirstfrommac.txt\n","-rw------- 1 root root     22 Sep 20 00:22 myfirstlocalfile.txt\n","drwx------ 2 root root   4096 Sep 20 00:24 notebooks\n","drwx------ 2 root root   4096 Sep  7 18:39 .quarto\n","drwx------ 2 root root   4096 Sep 20 00:22 quarto-demo\n","-rw------- 1 root root    843 Sep 20 00:24 _quarto.yml\n","-rw------- 1 root root     65 Sep 20 00:22 README.md\n","-rw------- 1 root root    493 Sep 20 00:22 references.bib\n","drwx------ 2 root root   4096 Sep 20 00:25 reports\n","-rw------- 1 root root  13025 Sep 20 00:22 requirements-lock.txt\n","-rw------- 1 root root    138 Sep 20 00:22 requirements.txt\n","drwx------ 2 root root   4096 Sep 20 00:24 scripts\n","drwx------ 2 root root   4096 Sep 20 00:24 sql\n","drwx------ 2 root root   4096 Sep 20 00:24 src\n","-rw------- 1 root root     58 Sep 15 13:53 stocks.csv\n","-rw------- 1 root root    141 Sep 20 00:22 test.md\n","-rw------- 1 root root    111 Sep 20 00:22 tickers_25.csv\n","drwx------ 2 root root   4096 Sep 20 00:22 tools\n"]}]},{"cell_type":"markdown","source":["```\n","rng.integers(low, high=None, size=None, dtype=int, endpoint=False)\n","Series.diff(periods=1)\n","DataFrame.diff(periods=1, axis=0)\n","```"],"metadata":{"id":"wb3w_LTce0F_"}},{"cell_type":"code","source":["# Generates data/raw/prices.csv with columns: ticker,date,adj_close,volume,log_return\n","import pandas as pd, numpy as np, os\n","from pathlib import Path\n","\n","Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n","tickers = pd.read_csv(\"tickers_25.csv\")[\"ticker\"].tolist() if os.path.exists(\"tickers_25.csv\") else [\n","    \"AAPL\",\"MSFT\",\"AMZN\",\"GOOGL\",\"META\",\"NVDA\",\"TSLA\",\"JPM\",\"JNJ\",\"V\",\n","    \"PG\",\"HD\",\"BAC\",\"XOM\",\"CVX\",\"PFE\",\"KO\",\"DIS\",\"NFLX\",\"INTC\",\"CSCO\",\"ORCL\",\"T\",\"VZ\",\"WMT\"\n","]\n","dates = pd.bdate_range(\"2020-01-01\", periods=180)  # ~ 9 months\n","rng = np.random.default_rng(7)\n","\n","frames=[]\n","for t in tickers:\n","    r = rng.normal(0, 0.01, len(dates))  #rng.normal(mean, std, size)\n","    price = 100*np.exp(np.cumsum(r))  # cumsum: simulate random walk; exp(): multiplicative growth (geometric Brownian motion style)\n","    vol = rng.integers(1e5, 5e6, len(dates))\n","    df = pd.DataFrame({\"ticker\": t, \"date\": dates, \"adj_close\": price, \"volume\": vol})\n","    df[\"log_return\"] = np.log(df[\"adj_close\"]).diff().fillna(0) #log return. diff: log(P_t-P_{t-1}). fillna(0): fill the value for day 1 (no prev) with 0\n","    frames.append(df)\n","\n","out = pd.concat(frames, ignore_index=True)\n","out.to_csv(\"data/raw/prices.csv\", index=False)\n","out.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"jruJH_zYb27v","executionInfo":{"status":"ok","timestamp":1758328073912,"user_tz":300,"elapsed":108,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"0fd88cba-a945-4d89-99be-13de2f682b08"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  ticker       date   adj_close   volume  log_return\n","0   AAPL 2020-01-01  100.001230  4457901    0.000000\n","1   AAPL 2020-01-02  100.300426  2664190    0.002987\n","2   AAPL 2020-01-03  100.025841  4100245   -0.002741\n","3   AAPL 2020-01-06   99.138974  4586613   -0.008906\n","4   AAPL 2020-01-07   98.689241  1556062   -0.004547"],"text/html":["\n","  <div id=\"df-08737537-8554-4b84-afc5-311a05c262d2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ticker</th>\n","      <th>date</th>\n","      <th>adj_close</th>\n","      <th>volume</th>\n","      <th>log_return</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>AAPL</td>\n","      <td>2020-01-01</td>\n","      <td>100.001230</td>\n","      <td>4457901</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>AAPL</td>\n","      <td>2020-01-02</td>\n","      <td>100.300426</td>\n","      <td>2664190</td>\n","      <td>0.002987</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AAPL</td>\n","      <td>2020-01-03</td>\n","      <td>100.025841</td>\n","      <td>4100245</td>\n","      <td>-0.002741</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AAPL</td>\n","      <td>2020-01-06</td>\n","      <td>99.138974</td>\n","      <td>4586613</td>\n","      <td>-0.008906</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>AAPL</td>\n","      <td>2020-01-07</td>\n","      <td>98.689241</td>\n","      <td>1556062</td>\n","      <td>-0.004547</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08737537-8554-4b84-afc5-311a05c262d2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-08737537-8554-4b84-afc5-311a05c262d2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-08737537-8554-4b84-afc5-311a05c262d2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-1ba1dfb8-703b-4790-bff0-6c711688620c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ba1dfb8-703b-4790-bff0-6c711688620c')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-1ba1dfb8-703b-4790-bff0-6c711688620c button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"out","summary":"{\n  \"name\": \"out\",\n  \"rows\": 4500,\n  \"fields\": [\n    {\n      \"column\": \"ticker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"JNJ\",\n          \"KO\",\n          \"AAPL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-01-01 00:00:00\",\n        \"max\": \"2020-09-08 00:00:00\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"2020-01-28 00:00:00\",\n          \"2020-02-28 00:00:00\",\n          \"2020-08-03 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"adj_close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.945406052872386,\n        \"min\": 70.67409687995661,\n        \"max\": 127.06926224634927,\n        \"num_unique_values\": 4500,\n        \"samples\": [\n          97.1574226891718,\n          93.88475935496746,\n          90.53376703133165\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1401054,\n        \"min\": 102908,\n        \"max\": 4998411,\n        \"num_unique_values\": 4497,\n        \"samples\": [\n          3839586,\n          3967197,\n          2117129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"log_return\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009832119588720484,\n        \"min\": -0.036610818102067455,\n        \"max\": 0.04061606310651911,\n        \"num_unique_values\": 4476,\n        \"samples\": [\n          -0.013844677963457563,\n          0.0028203016036165707,\n          0.00496722431822505\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["1. `wc` Short for word count.\n","By default it prints number of lines, words, and bytes in a file.\n","Common options:\n","\n","`wc -l file` → line count only.\n","\n","`wc -w file` → word count only.\n","\n","`wc -c file` → byte count.\n","\n","2. `tee`\n","Think of a plumbing T-joint: it splits a stream into two.\n","\n","`tee file.txt` → writes the output to a file and to the terminal at the same time.\n","\n","3. `tail -n +2`\n","Normally `tail -n 10 file` shows the last 10 lines of a file.\n","But if you use +N, it starts printing from line N to the end.\n","\n","4. `cut -d, -f1 data/raw/prices.csv`\n","\n","`cut` splits text into fields.\n","\n","`-d,` says delimiter is a comma.\n","\n","`-f1` picks field 1 → the ticker column.\n","\n","Output = first column of the CSV (including header)."],"metadata":{"id":"Vbp8dFDFhczx"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","# How many lines? (including header)\n","wc -l data/raw/prices.csv | tee reports/prices_wc.txt\n","\n","# First 5 lines, save to a sample\n","head -n 5 data/raw/prices.csv | tee data/raw/prices_sample.csv\n","\n","set +o pipefail  # uncomment to fix the SIGPIPE error\n","# Show ticker column only (field 1), excluding header\n","cut -d, -f1 data/raw/prices.csv | tail -n +2 | head -n 10  #|| true  # swallow SIGPIPE explicitly\n","set -o pipefail  #uncomment to change back"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5zyCacufeIY","executionInfo":{"status":"ok","timestamp":1758328073964,"user_tz":300,"elapsed":46,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"d430a5a5-3df6-4c10-e759-e98b642cd916"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["4501 data/raw/prices.csv\n","ticker,date,adj_close,volume,log_return\n","AAPL,2020-01-01,100.00123016092391,4457901,0.0\n","AAPL,2020-01-02,100.30042606816971,2664190,0.00298745537508438\n","AAPL,2020-01-03,100.02584117375997,4100245,-0.0027413785536225532\n","AAPL,2020-01-06,99.13897423972712,4586613,-0.008905918387572598\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n","AAPL\n"]}]},{"cell_type":"markdown","source":["## `grep` = search lines matching a regex and print them.\n","`grep` stands for `Global Regular Expression Print`.\n","* **Plain `grep`** = Basic Regular Expressions (BRE).\n","\n","  * In BRE, some operators like `+` and `|` are treated as *literal characters* unless you escape them.\n","  * Example: `grep 'a\\+'` matches “aaa…” in BRE.\n","\n","* **`grep -E`** = Extended Regular Expressions (ERE).\n","\n","  * Here, operators like `+` (one or more) and `|` (alternation) work *without* a backslash.\n","  * Example: `grep -E 'cat|dog' file.txt` matches lines with either “cat” or “dog”.\n","  * Example: `grep -E 'a+'` matches one or more “a”.\n","\n","* `grep` (BRE) is stricter, you must escape.\n","* `grep -E` (ERE) is friendlier for regex like `|` and `+`.\n","\n","\n","3. `sort -t, -k3,3nr`\n","\n","   * Sort by 3rd column (abs log return), numeric, reverse (largest first).\n","\n","4. `head -n 5`\n","\n","   * Take top 5 rows.\n","\n","Effect: the 5 biggest up or down days for NVDA, sorted by absolute return.\n","\n","---\n","\n","`sort` → group identical tickers together.\n","\n","`uniq -c` → collapse duplicates and count how many there were."],"metadata":{"id":"soZ9NtJNopLH"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","set +o pipefail\n","# All rows for NVDA OR MSFT (extended regex with alternation)\n","grep -E '^(NVDA|MSFT),' data/raw/prices.csv | head -n 3  # -E: extended regex\n","\n","# Rows where ticker starts with a vowel (A, E, I, O, U)\n","grep -E '^(A|E|I|O|U)[A-Z]*,' data/raw/prices.csv | head -n 3  #[A-Z]*: 0+ uppercase letters\n","\n","# Count rows per ticker quickly (just for demo)\n","cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq -c | head"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ofCQeiGxmPQj","executionInfo":{"status":"ok","timestamp":1758328074038,"user_tz":300,"elapsed":66,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"f692ba91-8ded-4a4e-8a5c-44132c8b0246"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["MSFT,2020-01-01,99.86247367796362,593024,0.0\n","MSFT,2020-01-02,99.8551257815958,2835784,-7.358286291303529e-05\n","MSFT,2020-01-03,98.54112146451733,1525535,-0.013246455506441102\n","AAPL,2020-01-01,100.00123016092391,4457901,0.0\n","AAPL,2020-01-02,100.30042606816971,2664190,0.00298745537508438\n","AAPL,2020-01-03,100.02584117375997,4100245,-0.0027413785536225532\n","    180 AAPL\n","    180 AMZN\n","    180 BAC\n","    180 CSCO\n","    180 CVX\n","    180 DIS\n","    180 GOOGL\n","    180 HD\n","    180 INTC\n","    180 JNJ\n"]}]},{"cell_type":"markdown","source":["```\n","mkdir -p path/to/dir\n","```\n","* “parents”: create any parent directories as needed.\n","\n","* No error if existing: if the directory already exists, it just does nothing (and exits successfully)."],"metadata":{"id":"QuQEC7mBdlEC"}},{"cell_type":"markdown","source":["## 1. **Replace ISO date dashes with slashes**\n","`sed`: Steam EDitor. Unix command-line tool for sarching,filtering,and transforming text streams.\n","```bash\n","sed -i '1!s/\\([0-9]\\{4\\}\\)-\\([0-9]\\{2\\}\\)-\\([0-9]\\{2\\}\\)/\\1\\/\\2\\/\\3/g' data/interim/prices_copy.csv\n","```\n","\n","* **`sed -i`** → edit the file *in place*.\n","* **`1!`** → apply the command to all lines *except line 1* (skip the header).\n","* **`s/.../.../g`** → substitution: find a pattern, replace it globally in each line.\n","* Regex:\n","\n","  * `\\([0-9]\\{4\\}\\)` = capture 4 digits (year).\n","  * `-` = literal dash.\n","  * `\\([0-9]\\{2\\}\\)` = capture 2 digits (month).\n","  * Another dash.\n","  * `\\([0-9]\\{2\\}\\)` = capture 2 digits (day).\n","* Replacement: `\\1/\\2/\\3`\n","\n","  * Use capture groups 1, 2, 3 (year, month, day) separated by `/`.\n","\n","Effect:\n","`2020-01-02` → `2020/01/02`\n","(but leaves header untouched).\n","\n","---\n","* `sed` uses **Basic Regular Expressions (BRE)** by default.\n","* In BRE:\n","\n","  * `(...)` (grouping) must be written as `\\(...\\)` (escaped).\n","  * `{n}` (quantifier) must be written as `\\{n\\}` (escaped).\n","* So `\\([0-9]\\{4\\}\\)` means “capture 4 digits.”\n","\n","If you used `sed -E` (Extended RE mode), you could drop most backslashes:\n","\n","```bash\n","sed -E '1!s/([0-9]{4})-([0-9]{2})-([0-9]{2})/\\1\\/\\2\\/\\3/g' file.csv\n","```\n","* A **group** (aka “capture group”) in regex is a part of the pattern wrapped in parentheses.\n","* It “remembers” what matched inside it.\n","* Later, you can **reuse** it in the replacement string as `\\1`, `\\2`, `\\3`, etc.\n","\n","## 2. **Normalize ticker to lowercase (first column)**\n","\n","1. **Keep the header**\n","\n","   ```bash\n","   head -n 1 data/interim/prices_copy.csv > data/interim/prices_lower.csv\n","   ```\n","\n","   * Take just the header line and save it as the new file.\n","\n","2. **Process the body (data rows)**\n","\n","   ```bash\n","   tail -n +2 data/interim/prices_copy.csv \\\n","   | awk -F, 'BEGIN{OFS=\",\"}{ $1=tolower($1); print }' >> data/interim/prices_lower.csv\n","   ```\n","\n","   * `tail -n +2` → skip header, output from 2nd line onward.\n","   * `awk -F,` → split fields on commas.\n","   * `BEGIN{OFS=\",\"}` → set output field separator as comma (preserve CSV format).\n","   * `{ $1=tolower($1); print }` → convert the first field (ticker) to lowercase, then print the whole row.\n","   * `>>` appends to the new file.\n","\n","* Header stays the same.\n","* All tickers (first column) become lowercase (`AAPL` → `aapl`).\n","\n","---\n"],"metadata":{"id":"Pplr8u86f25m"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","# Make a copy so we don't touch the raw file\n","cp data/raw/prices.csv data/interim/ || mkdir -p data/interim && cp data/raw/prices.csv data/interim\n","cp data/interim/prices.csv data/interim/prices_copy.csv\n","\n","# Replace ISO date dashes with slashes (2020-01-02 -> 2020/01/02) in-place\n","sed -i '1!s/\\([0-9]\\{4\\}\\)-\\([0-9]\\{2\\}\\)-\\([0-9]\\{2\\}\\)/\\1\\/\\2\\/\\3/g' data/interim/prices_copy.csv\n","\n","# Normalize ticker to lowercase (first column) using sed's capture groups and tolower via awk (hybrid example)\n","head -n 1 data/interim/prices_copy.csv > data/interim/prices_lower.csv\n","tail -n +2 data/interim/prices_copy.csv | awk -F, 'BEGIN{OFS=\",\"}{ $1=tolower($1); print }' >> data/interim/prices_lower.csv\n","\n","head -n 3 data/interim/prices_lower.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oC8lu3sxc12w","executionInfo":{"status":"ok","timestamp":1758328074186,"user_tz":300,"elapsed":150,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"bbdc2b3b-9e8a-40c2-b99c-80811ca13d56"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["ticker,date,adj_close,volume,log_return\n","aapl,2020/01/01,100.00123016092391,4457901,0.0\n","aapl,2020/01/02,100.30042606816971,2664190,0.00298745537508438\n"]},{"output_type":"stream","name":"stderr","text":["cp: cannot create regular file 'data/interim/': Not a directory\n"]}]},{"cell_type":"markdown","source":["* **awk** is named after its original authors:\n","\n","  * **A**lfred V. Aho\n","  * **P**eter J. **W**einberger\n","  * **B**rian W. **K**ernighan\n","\n","So “awk” is their initials.\n","It’s both a **programming language** (for text/data processing) and a **command-line tool**.\n","\n","---\n","\n","## 2. Syntax in `awk`\n","\n","The general structure is:\n","\n","```bash\n","awk 'pattern { action }' file\n","```\n","\n","* **`pattern`** → condition to match (like a filter).\n","* **`{ action }`** → what to do when the pattern matches.\n","* Both are optional:\n","\n","  * If you omit `pattern`, action applies to every line.\n","  * If you omit `{ action }`, default action is `print $0` (print the whole line).\n","\n","### Special symbols:\n","\n","* **`$1, $2, ...`** → fields (columns) in the current line, split by `-F` delimiter.\n","* **`$0`** → the whole line.\n","* **`NR`** → current line number.\n","* **`{ ... }`** → block of code (can have multiple statements separated by `;`).\n","* **`END { ... }`** → run once, after processing all lines.\n","\n","---\n","\n","## 3. What is `-k2,2nr` in `sort`\n","general form:\n","```\n","sort -kM,N\n","```\n","\n","`M` = starting field number.\n","\n","`N` = ending field number.\n","\n","If you give both, the sort key is everything from field `M` through field `N`.\n","\n","If you only give `-kM`, then the key runs from field `M` all the way to the end of the line.\n","\n","Fields are determined by the delimiter (`-t`).\n","* `-t,` → use **comma** as field separator.\n","* `-k2,2` → sort by **column 2 only**.\n","* `n` → numeric sort (treat “10” as 10, not as string).\n","* `r` → reverse order (largest first).\n","\n","So `-k2,2nr` = “sort by **2nd column**, numerically, descending”.\n","\n","---\n","\n","* By default, `head` show  **10 lines**.\n","* You can override with `-n N`.\n","\n","  * Example: `head -n 5 file.txt` shows the first 5 lines.\n","\n","### (a) Compute mean log\\_return per ticker\n","“Group by ticker, compute average log return, rank them.”\n","```bash\n","awk -F, 'NR>1 { sum[$1]+=$5; n[$1]++ }\n","         END { OFS=\",\"; print \"ticker\",\"mean_log_return\";\n","               for (t in sum) print t, sum[t]/n[t] }' data/raw/prices.csv \\\n","| sort -t, -k2,2nr | head\n","```\n","* `-F,` → field separator is a comma (CSV).\n","* `NR>1` → skip header line (NR = record number).\n","* `{ sum[$1]+=$5; n[$1]++ }` →\n","\n","  * `$1` = ticker (first column).\n","  * `$5` = log\\_return (fifth column).\n","  * For each ticker, accumulate `sum[ticker]` and count `n[ticker]`.\n","* `END { ... }` → after reading all rows:\n","\n","  * `OFS=\",\"` → output field separator = comma.\n","  * Print a header row.\n","  * For each ticker `t`, print `t, average = sum[t]/n[t]`.\n","* Pipe to `sort -t, -k2,2nr`:\n","\n","  * `-t,` → use comma as delimiter.\n","  * `-k2,2nr` → sort by column 2 (mean) numerically, descending.\n","* `head` → show top few.\n","\n"," Effect: top tickers ranked by average log return.\n","\n","---\n","\n","### (b) Top 5 dates with highest absolute log\\_return for NVDA\n","“Filter NVDA, compute absolute returns, find the 5 most extreme days.”\n","```bash\n","awk -F, 'NR>1 && $1==\"NVDA\" { print $2, $5 }' data/raw/prices.csv \\\n","| awk '{ if ($2<0) s=-$2; else s=$2; print $1\",\"$2\",\"s }' \\\n","| sort -t, -k3,3nr | head -n 5\n","```\n","1. `awk -F, 'NR>1 && $1==\"NVDA\" { print $2, $5 }'`\n","\n","   * Skip header.\n","   * Only rows where ticker (`$1`) is NVDA.\n","   * Print date (`$2`) and log\\_return (`$5`).\n","\n","2. Pipe into another `awk`:\n","\n","   ```awk\n","   { if ($2<0) s=-$2; else s=$2; print $1\",\"$2\",\"s }\n","   ```\n","\n","   * Compute absolute value of log\\_return (`s`).\n","   * Print: `date,log_return,abs_log_return`.\n"],"metadata":{"id":"TVlq6UA3tgxI"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","# Compute mean log_return per ticker (skip header). -F, sets comma as field separator.\n","awk -F, 'NR>1 { sum[$1]+=$5; n[$1]++ } END { OFS=\",\"; print \"ticker\",\"mean_log_return\"; for (t in sum) print t, sum[t]/n[t] }' data/raw/prices.csv \\\n","| sort -t, -k2,2nr | head\n","\n","# Top 5 dates with highest absolute log_return for NVDA\n","awk -F, 'NR>1 && $1==\"NVDA\" { print $2, $5 }' data/raw/prices.csv \\\n","| awk '{ if ($2<0) s=-$2; else s=$2; print $1\",\"$2\",\"s }' \\\n","| sort -t, -k3,3nr | head -n 5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FfjUUfsQpjrk","executionInfo":{"status":"ok","timestamp":1758328074237,"user_tz":300,"elapsed":34,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"3386398b-8e18-4bd5-9713-d8161e067555"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["WMT,2.24177e-05\n","PFE,0.00129911\n","T,0.000994674\n","ORCL,0.00073139\n","V,0.000592352\n","GOOGL,0.000489848\n","NFLX,0.000474115\n","JPM,0.000342749\n","AMZN,0.000134249\n","ticker,mean_log_return\n","2020-03-11,0.028288986337162925,0.028288986337162925\n","2020-06-08,-0.027802932151547388,0.0278029\n","2020-08-20,0.02705506369509436,0.02705506369509436\n","2020-07-23,0.026552935706134484,0.026552935706134484\n","2020-06-09,0.026508047390848333,0.026508047390848333\n"]}]},{"cell_type":"markdown","source":["* `comm file1 file2` → compares **two sorted files line by line**.\n","* It outputs **three columns**:\n","\n","  1. Lines only in `file1`.\n","  2. Lines only in `file2`.\n","  3. Lines in both.\n","\n","You can suppress columns with options:\n","\n","* `-1` → suppress col 1 (only in file1).\n","* `-2` → suppress col 2 (only in file2).\n","* `-3` → suppress col 3 (common lines).\n","\n","* `comm -23 file1 file2`\n","\n","  * Suppress col 2 and 3 → show **only lines unique to file1**.\n","* `comm -13 file1 file2`\n","\n","  * Suppress col 1 and 3 → show **only lines unique to file2**.\n","\n","```bash\n","sed 's/^/  /'\n","```\n","\n","* `s/^/  /` = substitute start of line (`^`) with two spaces.\n","* Effect: indent each output line by 2 spaces → makes results more readable.\n","\n","**Important:** `comm` requires both input files to be **sorted**. If they aren’t, results will be wrong."],"metadata":{"id":"dfhqerOnoKtp"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","# Unique tickers actually present in the file\n","cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq > data/interim/tickers_in_data.txt\n","\n","# Compare to our canonical list from tickers_25.csv\n","cut -d, -f1 tickers_25.csv | tail -n +2 | sort > data/interim/tickers_25.txt\n","\n","echo \"Only in data:\"; comm -23 data/interim/tickers_in_data.txt data/interim/tickers_25.txt | sed 's/^/  /'\n","echo \"Only in canonical:\"; comm -13 data/interim/tickers_in_data.txt data/interim/tickers_25.txt | sed 's/^/  /'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"km5DsyubnOkH","executionInfo":{"status":"ok","timestamp":1758328074314,"user_tz":300,"elapsed":82,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"14da8396-3bff-4166-aced-0d4a14c9b196"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Only in data:\n","Only in canonical:\n"]}]},{"cell_type":"markdown","source":["```bash\n","find data -type f -name \"*.csv\" -printf \"%p,%s bytes\\n\" | sort | head\n","```\n","\n","* `find data -type f -name \"*.csv\"` → find all regular files ending with `.csv` under `data/`.\n","* `-printf \"%p,%s bytes\\n\"` → custom output:\n","\n","  * `%p` = path of the file.\n","  * `%s` = file size in bytes.\n","* `| sort` → sort results alphabetically (by path).\n","* `| head` → show only the first 10 results.\n","\n","Effect: a neat file listing with paths and sizes.\n","Note: `-printf` is GNU (GNU's not unix) `find` only; on macOS/Colab (BSD: Berkeley Software Distribution), you’d use `-exec stat` instead.\n","\n","---\n","\n","```bash\n","find data -type f -name \"*.csv\" -print0 | xargs -0 -I{} sh -c 'echo -n \"{},\"; wc -l < \"{}\"'\n","```\n","\n","* `find ... -print0` → prints file paths separated by **nulls** (`\\0`), safe for weird filenames (with spaces, quotes, etc.).\n","* `xargs -0` → reads null-separated paths. `xargs` builds and executes command lines from stdin.\n","* `-I{}` → replace `{}` with each filename in the command.\n","* `sh -c 'echo -n \"{},\"; wc -l < \"{}\"'` →\n","\n","  * `echo -n \"{},\"` → print filename followed by a comma (no newline).\n","  * `-n`: no new line (in the same line).\n","  * `wc -l < \"{}\"` → count lines in the file, print number.\n","  * -c: scripts\n","\n","Effect: outputs `filename.csv,<linecount>` for each CSV.\n","\n","```bash\n","find data -type f -name \"*.csv\" -size +1000k -print0 | xargs -0 -I{} gzip -kf \"{}\"\n","```\n","\n","* `-size +1000k` → match files larger than **1000 kilobytes ≈ 1 MB**.\n","* `-print0 | xargs -0` → null-safe passing to `gzip`.\n","* `gzip -kf \"{}\"` →\n","\n","  * `-k` = keep original file (don’t delete after compressing).\n","  * `-f` = force overwrite if `.gz` already exists.\n","\n","Effect: creates `filename.csv.gz` alongside the original CSV for any file bigger than \\~1MB.\n","\n","---\n","\n","* **GNU find** → standard on Linux (e.g., Ubuntu, Debian, Colab).\n","* **BSD find** → standard on BSD-based systems like macOS and FreeBSD.\n","\n","On macOS, you use the `stat` command inside `find`:\n","\n","```bash\n","find data -type f -name \"*.csv\" -exec stat -f \"%N,%z bytes\" {} \\;\n","```\n","\n","* `-exec ... {} \\;` → run `stat` on each found file.\n","* `-f \"%N,%z bytes\"` → BSD `stat` format string:\n","\n","  * `%N` = filename\n","  * `%z` = file size in bytes\n","\n","Example output:\n","\n","```\n","data/raw/prices.csv,123456 bytes\n","```\n","On Linux → you typically get GNU utilities (e.g. GNU find with -printf).\n","\n","On macOS → you get BSD utilities (e.g. BSD find without -printf).\n","\n","Perfect — let’s go step by step.\n","\n","---\n","\n","### Syntax of `find`\n","\n","General form:\n","\n","```bash\n","find [path...] [options] [tests] [actions]\n","```\n","\n","* **`[path...]`** → where to start searching (default is `.`).\n","* **`[options]`** → control things like depth or following symlinks.\n","* **`[tests]`** → conditions that must be true for a file to match (e.g. name, type, size).\n","* **`[actions]`** → what to do with each match (e.g. print, exec, delete).\n","\n","---\n","\n","### Common **tests**\n","\n","* `-name \"*.csv\"` → filename matches pattern.\n","* `-type f` → regular file.\n","* `-type d` → directory.\n","* `-size +1000k` → larger than 1000 KB.\n","* `-mtime -7` → modified within last 7 days.\n","\n","---\n","\n","### Common **actions**\n","\n","* `-print` → print the file path (default if no action given).\n","* `-printf` (GNU only) → custom output format.\n","* `-exec command {} \\;` → run a command on each file (`{}` replaced with filename).\n","* `-delete` → remove matching files.\n","\n","---\n","\n","### Examples\n","\n","```bash\n","# 1. Find all CSVs under data/\n","find data -type f -name \"*.csv\"\n","\n","# 2. Find large files (>1MB)\n","find . -type f -size +1000k\n","\n","# 3. Find and delete .tmp files\n","find . -type f -name \"*.tmp\" -delete\n","\n","# 4. Find files and run wc -l on each\n","find . -type f -name \"*.csv\" -exec wc -l {} \\;\n","```\n","---\n","\n","## Difference between `' '` (single quotes) and `\" \"` (double quotes)\n","\n","* **Single quotes `'...'`**\n","\n","  * Take everything **literally**.\n","  * No variable expansion, no backslash escapes (except `'\\''` trick).\n","  * Example:\n","\n","    ```bash\n","    name=world\n","    echo 'Hello $name'   # → Hello $name\n","    ```\n","\n","* **Double quotes `\"...\"`**\n","\n","  * Allow **expansion** of variables, command substitution, and some escapes.\n","  * Example:\n","\n","    ```bash\n","    name=world\n","    echo \"Hello $name\"   # → Hello world\n","    ```\n","\n","\n","\n"],"metadata":{"id":"gCNTrBbHpXm1"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","# Show all CSVs under data/, printing sizes\n","find data -type f -name \"*.csv\" -printf \"%p,%s bytes\\n\" | sort | head\n","\n","# Count lines in each CSV (null-safe for weird filenames)\n","find data -type f -name \"*.csv\" -print0 | xargs -0 -I{} sh -c 'echo -n \"{},\"; wc -l < \"{}\"'\n","\n","# Gzip-compress any CSV larger than ~1MB (demo threshold: 1e6 bytes)\n","find data -type f -name \"*.csv\" -size +1000k -print0 | xargs -0 -I{} gzip -kf \"{}\"  # -k keeps original"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xeadxc3NoyeC","executionInfo":{"status":"ok","timestamp":1758328074336,"user_tz":300,"elapsed":28,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"2546b167-f988-48a7-ab68-7a1beef3e286"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["data/interim/prices_copy.csv,282169 bytes\n","data/interim/prices.csv,282169 bytes\n","data/interim/prices_lower.csv,282169 bytes\n","data/raw/prices.csv,282169 bytes\n","data/raw/prices_sample.csv,280 bytes\n","data/raw/prices.csv,4501\n","data/raw/prices_sample.csv,5\n","data/interim/prices.csv,4501\n","data/interim/prices_copy.csv,4501\n","data/interim/prices_lower.csv,4501\n"]}]},{"cell_type":"markdown","source":["### Creating the script with a here-doc\n","\n","```bash\n","cat > scripts/qa_csv.sh << 'EOF'\n","...\n","EOF\n","chmod +x scripts/qa_csv.sh\n","```\n","\n","* Writes everything between `<< 'EOF'` and `EOF` to `scripts/qa_csv.sh`.\n","* **Quotes around `EOF`** are important: they **prevent variable/command expansion** while writing, so `$1`, `$FILE`, etc. are preserved literally.\n","* Here document (<<): a way to feed a block of text to a command’s standard input.\n","\n","* cat > file.sh → runs cat and redirects its stdout into file.sh. Normally, cat just copies its stdin to stdout.\n","\n","* << 'EOF' ... EOF → the lines in between are treated as stdin for cat.\n","\n","So the effect: everything between EOF markers is written to file.sh.\n","\n","---\n","\n","```bash\n","#!/usr/bin/env bash\n","# Simple CSV health check\n","# Usage: scripts/qa_csv.sh path/to/file.csv required_columns_csv\n","set -euo pipefail\n","IFS=$'\\n\\t'\n","```\n","\n","* Shebang(sh+bang: bang: `!`) picks bash from PATH.\n","* `IFS=$'\\n\\t'` set IFS (Internal Field Separator) to newline + tab. makes word-splitting safer (don’t split on spaces by default).\n","* $'...' is a special ANSI C quoting form in bash.\n","Inside it, escape sequences like `\\n` (newline), `\\t` (tab), `\\x41` (hex A) are interpreted.\n","Without the `$`, '...' is just literal characters.\n","\n","\n","\n","### Inputs & defaults\n","\n","```bash\n","FILE=\"${1:-}\"\n","REQUIRED=\"${2:-ticker,date,adj_close,volume,log_return}\"\n","```\n","\n","* `$1` first positional paramter passed to the script. It is the CSV path; if missing, empty string.\n","* `$2` is a comma-separated list of required columns; default shown.\n","* `${var:-default}`:\n","Use `$var` if set and non-empty; otherwise use default. `:-` requies to use `{ }`.\n","  *`${var-default}` → use default if unset (empty counts as set)\n","\n","  *`${var:-default}` → use default if unset or empty\n","\n","  * `${var:?msg}` → error (with msg) if unset/empty\n","\n","  * `${var:+alt}` → use alt if var is set (else empty)\n","\n","### Tiny error helper\n","\n","```bash\n","err() { echo \"ERROR: $*\" >&2; exit 1; }\n","[[ -z \"$FILE\" ]] && err \"No CSV file provided.\"\n","[[ ! -f \"$FILE\" ]] && err \"File not found: $FILE\"\n","```\n","\n","* `err` prints to stderr (2) and exits. stdout: 1. `>&2`: send it to stderr.\n","* Basic presence & path checks.\n","* `$*` = all positional parameters (`$1` `$2` `$3` ...).\n","* In \"${*}\", they’re joined into one string.\n","\n","* `$*` → expands to all args as one string (joined by IFS).\n","\n","* `$@` → expands to each arg separately (safe for iteration).\n","* `[[ ... ]]` is bash’s test command (safer than [ ... ]): Supports regex matching.\n","\n","* `-z` = “string length is zero.” So `[[ -z \"$FILE\" ]]` means “is `$FILE` empty?”\n","\n","* Double quotes around `$FILE`:\n","Prevent errors if `$FILE` is unset or contains spaces.\n","Without quotes, `[[ -z $FILE ]]` could break if $FILE is empty or has spaces.\n","\n","* `-f` = “is this a regular file?”\n","`[[ -f \"$FILE\" ]]` → true if $FILE exists and is a file.\n","\n","### 1) Non-empty file with header\n","\n","```bash\n","LINES=$(wc -l < \"$FILE\" || true)\n","[[ \"${LINES:-0}\" -lt 2 ]] && err \"File has <2 lines (missing data?): $FILE\"\n","HEADER=$(head -n 1 \"$FILE\")\n","```\n","\n","* `wc -l < \"$FILE\"` counts lines; `|| true` prevents strict-mode exit if `wc` failed (defensive).\n","* `<2` lines → likely missing data row(s).\n","* `HEADER` grabs the first line to inspect column names.\n","\n","### 2) Check required columns exist (exact token match)\n","\n","```bash\n","IFS=',' read -r -a req <<< \"$REQUIRED\"\n","for col in \"${req[@]}\"; do\n","  echo \"$HEADER\" | grep -q -E \"(^|,)${col}(,|$)\" || err \"Missing required column: $col\"\n","done\n","```\n","* Temporary IFS: `IFS=',' cmd ...` sets IFS for that command only (and its children).\n","In bash, a VAR=value cmd prefix is a per-command environment assignment; it does not permanently change the parent shell’s IFS.\n","\n","`read`: reads one line from stdin and splits it into fields using IFS.\n","\n","`-r`: don’t treat backslashes as escapes (reads text literally).\n","\n","`-a req`: put the split fields into array `req` (`req[0], req[1]`, …).\n","\n","* `<<<` Feeds the string `REQUIRED` into the previous command; Splits `REQUIRED` into array `req`.\n","* The regex `(^|,),col,(|$)` ensures the **whole header token** equals `col` (not a substring).\n","* `\"${req[@]}\"`  req is an array.\n","`${req[@]}` expands to each element of the array separately.\n","\n","* Contrast: `${req[*]}` expands to all elements as one string.\n","\n","`grep -q -E \"(^|,)${col}(,|$)\"`\n","\n","`-q` = quiet (suppress output, just return success/failure).\n","\n","`-E` = extended regex (so | works without backslashes).\n","\n","Regex breakdown:\n","\n","  * `(^|,)` → start of line OR a comma before the column name.\n","\n","  * `${col}` → variable with column name.\n","\n","  * `(,|$)` → comma after the name OR end of line.\n","\n","\n","### 3) Basic NA/blank checks for numeric columns\n","\n","```bash\n","NUMERIC=\"adj_close,volume,log_return\"\n","IFS=',' read -r -a nums <<< \"$NUMERIC\"\n","for col in \"${nums[@]}\"; do\n","  # find column index (1-based)\n","  idx=$(awk -F, -v COL=\"$col\" 'NR==1{for(i=1;i<=NF;i++) if($i==COL) print i}' \"$FILE\")\n","  [[ -z \"${idx:-}\" ]] && err \"Column not found: $col\"\n","\n","  # count blanks/\"NA\" (rows 2+) in that column\n","  bad=$(awk -F, -v I=\"$idx\" 'NR>1 && ($I==\"\" || $I==\"NA\") {c++} END{print c+0}' \"$FILE\")\n","  [[ \"$bad\" -gt 0 ]] && err \"Found $bad blank/NA in column: $col\"\n","done\n","```\n","\n","* First `awk` scans the **header row** to find the index of `col`.\n","* Second `awk` scans data rows (`NR>1`) and increments for empty strings \"\" or literal `NA`.\n","* Fails the script if any blanks/`NA` found.\n","* `-v I=\"$idx\"` `-v`: sets an `awk` variable before the program starts. passes the column index into awk as variable I.\n","\n","* `NR>1` → skip header.\n","\n","* `($I==\"\" || $I==\"NA\")` → check if that column is blank or NA.\n","\n","\n","* `END{print c+0}` → after finishing file, print the counter (defaults to 0).\n","\n","* If `bad > 0`, error out.\n","\n","---\n","\n"],"metadata":{"id":"D5frGeaTUo1Q"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","mkdir -p scripts\n","\n","cat > scripts/qa_csv.sh << 'EOF'\n","#!/usr/bin/env bash\n","# Simple CSV health check\n","# Usage: scripts/qa_csv.sh path/to/file.csv required_columns_csv\n","set -euo pipefail\n","IFS=$'\\n\\t'\n","\n","FILE=\"${1:-}\"\n","REQUIRED=\"${2:-ticker,date,adj_close,volume,log_return}\"\n","\n","err() { echo \"ERROR: $*\" >&2; exit 1; }\n","[[ -z \"$FILE\" ]] && err \"No CSV file provided.\"\n","[[ ! -f \"$FILE\" ]] && err \"File not found: $FILE\"\n","\n","# 1) Non-empty and header present\n","LINES=$(wc -l < \"$FILE\" || true)\n","[[ \"${LINES:-0}\" -lt 2 ]] && err \"File has <2 lines (missing data?): $FILE\"\n","\n","HEADER=$(head -n 1 \"$FILE\")\n","# 2) All required columns present\n","IFS=',' read -r -a req <<< \"$REQUIRED\"\n","for col in \"${req[@]}\"; do\n","  echo \"$HEADER\" | grep -q -E \"(^|,)${col}(,|$)\" || err \"Missing required column: $col\"\n","done\n","\n","# 3) No obvious NA/blank values in required numeric cols (basic check)\n","NUMERIC=\"adj_close,volume,log_return\"\n","IFS=',' read -r -a nums <<< \"$NUMERIC\"\n","for col in \"${nums[@]}\"; do\n","  # find column index\n","  idx=$(awk -F, -v COL=\"$col\" 'NR==1{for(i=1;i<=NF;i++) if($i==COL) print i}' \"$FILE\")\n","  [[ -z \"${idx:-}\" ]] && err \"Column not found: $col\"\n","  # check any blank values from row 2 onward\n","  bad=$(awk -F, -v I=\"$idx\" 'NR>1 && ($I==\"\" || $I==\"NA\") {c++} END{print c+0}' \"$FILE\")\n","  [[ \"$bad\" -gt 0 ]] && err \"Found $bad blank/NA in column: $col\"\n","done\n","\n","echo \"OK: $FILE passed basic CSV QA ($LINES lines).\"\n","EOF\n","\n","chmod +x scripts/qa_csv.sh"],"metadata":{"id":"K7EX58rJ0-Hg","executionInfo":{"status":"ok","timestamp":1758328074367,"user_tz":300,"elapsed":29,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","scripts/qa_csv.sh data/raw/prices.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8WVUFqBTrK3","executionInfo":{"status":"ok","timestamp":1758328074539,"user_tz":300,"elapsed":169,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"0722b972-db6b-4b14-92ee-fff1122eeebc"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["OK: data/raw/prices.csv passed basic CSV QA (4501 lines).\n"]}]},{"cell_type":"code","source":["!scripts/qa_csv.sh data/interim/prices_lower.csv ticker,date,adj_close"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqoqBEzBTt4p","executionInfo":{"status":"ok","timestamp":1758328074592,"user_tz":300,"elapsed":60,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"c09a3b32-1c73-4a58-c2c3-25706e65f9e7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["OK: data/interim/prices_lower.csv passed basic CSV QA (4501 lines).\n"]}]},{"cell_type":"markdown","source":["# Homework"],"metadata":{"id":"NOSX_mHkqWTc"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","mkdir -p reports data/interim\n","\n","# 1) Count lines and unique tickers\n","{\n","  echo \"Lines (incl header): $(wc -l < data/raw/prices.csv)\";\n","  echo \"Unique tickers: $(cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | wc -l)\";\n","} | tee reports/data_counts.txt\n","\n","# 2) Top-10 days by absolute log_return across all tickers\n","tail -n +2 data/raw/prices.csv \\\n","| awk -F, '{a=$5; if(a<0) a=-a; print $1\",\"$2\",\"$5\",\"a}' \\\n","| sort -t, -k4,4nr | head -n 10 \\\n","| tee reports/top10_abs_moves.csv || true\n","\n","# 3) Mean log_return per ticker (CSV)\n","awk -F, 'NR>1 { s[$1]+=$5; n[$1]++ } END { OFS=\",\"; print \"ticker,mean_log_return\"; for(t in s) print t, s[t]/n[t] }' \\\n","  data/raw/prices.csv | sort -t, -k2,2nr | tee reports/mean_return_by_ticker.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JfV6RhtNqYqU","executionInfo":{"status":"ok","timestamp":1758328074655,"user_tz":300,"elapsed":65,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"cf0d1283-693e-40cc-8864-cb10f8bada20"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Lines (incl header): 4501\n","Unique tickers: 25\n","XOM,2020-05-08,-9.545439969294023e-05,9.54544e-05\n","NFLX,2020-02-26,9.243919974366577e-05,9.243919974366577e-05\n","HD,2020-07-10,9.230948293748042e-05,9.230948293748042e-05\n","PFE,2020-06-23,8.716362939864553e-05,8.716362939864553e-05\n","BAC,2020-06-04,-8.67879702681762e-05,8.6788e-05\n","CSCO,2020-06-18,8.461770772161259e-05,8.461770772161259e-05\n","VZ,2020-01-21,-8.349958001385716e-05,8.34996e-05\n","AAPL,2020-08-03,7.959099184873253e-05,7.959099184873253e-05\n","MSFT,2020-01-02,-7.358286291303529e-05,7.35829e-05\n","JNJ,2020-02-19,6.050414894520628e-05,6.050414894520628e-05\n","WMT,2.24177e-05\n","PFE,0.00129911\n","T,0.000994674\n","ORCL,0.00073139\n","V,0.000592352\n","GOOGL,0.000489848\n","NFLX,0.000474115\n","JPM,0.000342749\n","AMZN,0.000134249\n","ticker,mean_log_return\n","NVDA,-0.000120328\n","CVX,-0.000129288\n","HD,-0.00015012\n","BAC,-0.000158227\n","MSFT,-0.000274487\n","CSCO,-0.000283718\n","META,-0.0004515\n","KO,-0.000466965\n","JNJ,-0.000634333\n","XOM,-0.0010357\n","TSLA,-0.00109702\n","PG,-0.00115799\n","DIS,-0.00119287\n","VZ,-0.00161965\n","AAPL,-0.00176934\n","INTC,-9.90175e-05\n"]}]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","mkdir -p data/interim\n","\n","# Extract header once\n","HEADER=$(head -n 1 data/raw/prices.csv)\n","\n","# Create per-ticker files with header + rows (null-safe not necessary here)\n","cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | while read -r T; do\n","  mkdir -p \"data/interim/ticker=${T}\"\n","  {\n","    echo \"$HEADER\"\n","    awk -F, -v TK=\"$T\" 'NR==1 || $1==TK' data/raw/prices.csv # change NR==1 to NR>1 to avoid writing the header twice\n","  } > \"data/interim/ticker=${T}/prices_${T}.csv\"\n","done\n","\n","# Verify one example\n","ls -la data/interim/ticker=AAPL | head"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-WK_wncuqGg","executionInfo":{"status":"ok","timestamp":1758328075150,"user_tz":300,"elapsed":493,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"6d57296c-50e7-40d1-9e31-9c6f7dc59f01"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["total 12\n","-rw------- 1 root root 11487 Sep 20 00:27 prices_AAPL.csv\n"]}]},{"cell_type":"markdown","source":["The total 12 is the sum of disk blocks used by files in that directory (12 × 1 KiB blocks ≈ 11,487 bytes, which matches the size shown)."],"metadata":{"id":"kbf68MP1wwC6"}},{"cell_type":"code","source":["# To see that many per-ticker folders/files were created, list the parent:\n","!ls -1 data/interim | head          # shows directories like ticker=AAPL, ticker=MSFT, ...\n","!find data/interim -maxdepth 2 -name 'prices_*.csv' | head\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qe6nO-sMxptx","executionInfo":{"status":"ok","timestamp":1758328075373,"user_tz":300,"elapsed":221,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"c27aef53-14a7-4a7d-840a-2529f5a4883b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["prices_copy.csv\n","prices.csv\n","prices_lower.csv\n","ticker=AAPL\n","ticker=AMZN\n","ticker=BAC\n","ticker=CSCO\n","ticker=CVX\n","ticker=DIS\n","ticker=GOOGL\n","data/interim/prices_copy.csv\n","data/interim/prices_lower.csv\n","data/interim/ticker=AAPL/prices_AAPL.csv\n","data/interim/ticker=AMZN/prices_AMZN.csv\n","data/interim/ticker=BAC/prices_BAC.csv\n","data/interim/ticker=CSCO/prices_CSCO.csv\n","data/interim/ticker=CVX/prices_CVX.csv\n","data/interim/ticker=DIS/prices_DIS.csv\n","data/interim/ticker=GOOGL/prices_GOOGL.csv\n","data/interim/ticker=HD/prices_HD.csv\n"]}]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","# Append or create a Makefile\n","{\n","  echo \"\"\n","  echo \"qa:\"\n","  echo \"\\tscripts/qa_csv.sh data/raw/prices.csv\"\n","  echo \"\"\n","  echo \"split-by-ticker:\"\n","  echo \"\\tbash -c 'HEADER=\\$(head -n 1 data/raw/prices.csv); cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | while read -r T; do mkdir -p data/interim/ticker=\\$\\$T; { echo \\\"\\$\\$HEADER\\\"; awk -F, -v TK=\\\"\\$\\$T\\\" '\\\"'NR==1 || \\$1==TK'\\\"' data/raw/prices.csv; } > data/interim/ticker=\\$\\$T/prices_\\$\\$T.csv; done'\"\n","} >> Makefile\n","\n","cat Makefile"],"metadata":{"id":"acg24az1wOo3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758328075438,"user_tz":300,"elapsed":61,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"c3f5a268-7e55-447b-fcee-baad05b55b3b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["# Makefile — unified-stocks\n","SHELL := /bin/bash\n",".SHELLFLAGS := -eu -o pipefail -c\n",".ONESHELL:\n","\n","\n","PY := python\n","QUARTO := quarto\n","\n","START ?= 2020-01-01\n","END   ?= 2025-08-01\n","ROLL  ?= 30\n","\n","DATA_RAW := data/raw/prices.csv\n","FEATS    := data/processed/features.parquet\n","REPORT   := docs/reports/eda.html\n","\n","# Default target\n",".DEFAULT_GOAL := help\n","\n",".PHONY: help all clean clobber qa report backup\n","\n","help: ## Show help for each target\n","\t@awk 'BEGIN {FS = \":.*##\"; printf \"Available targets:\\n\"} /^[a-zA-Z0-9_\\-]+:.*##/ {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' $(MAKEFILE_LIST)\n","\n","# all: $(DATA_RAW) $(FEATS) report backup ## Run the full pipeline and back up artifacts\n","all: $(DATA_RAW) $(FEATS) report train backup\n","\n","$(DATA_RAW): scripts/get_prices.py tickers_25.csv\n","\t$(PY) scripts/get_prices.py --tickers tickers_25.csv --start $(START) --end $(END) --out $(DATA_RAW)\n","\n","$(FEATS): scripts/build_features.py $(DATA_RAW) scripts/qa_csv.sh\n","\t# Basic QA first\n","\tscripts/qa_csv.sh $(DATA_RAW)\n","\t$(PY) scripts/build_features.py --input $(DATA_RAW) --out $(FEATS) --roll $(ROLL)\n","\n","# --- add after FEATS definition, near other targets ---\n","\n","TRAIN_METRICS := reports/baseline_metrics.json\n","\n",".PHONY: train\n","train: $(TRAIN_METRICS) ## Train toy baseline and write metrics\n","\n","$(TRAIN_METRICS): scripts/train_baseline.py $(FEATS)\n","\t$(PY) scripts/train_baseline.py --features $(FEATS) --out-metrics $(TRAIN_METRICS)\n","\n","report: $(REPORT) ## Render Quarto EDA to docs1/\n","$(REPORT): reports/eda.qmd _quarto.yml docs1/style.css\n","\t$(QUARTO) render reports/eda.qmd -P symbol:AAPL -P start_date=$(START) -P end_date=$(END) -P rolling=$(ROLL) --output-dir docs1/\n","\t@test -f $(REPORT) || (echo \"Report not generated.\" && exit 1)\n","\n","backup: ## Rsync selected artifacts to backups/<timestamp>/\n","\t./scripts/backup.sh\n","\n","clean: ## Remove intermediate artifacts (safe)\n","\trm -rf data/interim\n","\trm -rf data/processed/*.parquet || true\n","\n","clobber: clean ## Remove generated reports and backups (dangerous)\n","\trm -rf docs/reports || true\n","\trm -rf backups || true\n","\n","DB := data/prices.db\n","\n",".PHONY: db sql-report\n","db: ## Build/refresh SQLite database from CSVs\n","\tpython scripts/build_db.py --db $(DB) --tickers tickers_25.csv --prices data/raw/prices.csv\n","\n","sql-report: db ## Generate a simple SQL-driven CSV summary\n","\t$(PY) - <<-'PY'\n","\timport pandas as pd, sqlite3, os\n","\tcon = sqlite3.connect(\"data/prices.db\")\n","\tdf = pd.read_sql_query(\"\"\"\n","\tSELECT m.sector,\n","\t       COUNT(*) AS n_obs,\n","\t       AVG(ABS(p.log_return)) AS mean_abs_return\n","\tFROM prices p\n","\tJOIN meta m ON p.ticker = m.ticker\n","\tGROUP BY m.sector\n","\tORDER BY n_obs DESC;\n","\t\"\"\", con)\n","\tos.makedirs(\"reports\", exist_ok=True)\n","\tdf.to_csv(\"reports/sql_sector_summary.csv\", index=False)\n","\tprint(df.head())\n","\tcon.close()\n","\tPY\n","\n","\n","qa:\n","\\tscripts/qa_csv.sh data/raw/prices.csv\n","\n","split-by-ticker:\n","\\tbash -c 'HEADER=$(head -n 1 data/raw/prices.csv); cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | while read -r T; do mkdir -p data/interim/ticker=$$T; { echo \"$$HEADER\"; awk -F, -v TK=\"$$T\" '\"'NR==1 || $1==TK'\"' data/raw/prices.csv; } > data/interim/ticker=$$T/prices_$$T.csv; done'\n"]}]},{"cell_type":"markdown","source":["The previous `Makefile` will lead to errors as the tab `\\t` is not expanded, which is required.\n","\n","Let's first delete the lines starting from `qa:`, then use a here-doc to fix it."],"metadata":{"id":"roie4uU6OVse"}},{"cell_type":"markdown","source":["* `^qa:` = regex that matches any line **starting with `qa:`** (`^` anchors to line start).\n","* `,` = address range operator (“from … to …”).\n","* `$` = the **last line** of the file.\n","* So `/^qa:/,$` = “all lines from the first line that begins with `qa:` **through to the end of the file**.”\n","* `d` = **delete** those lines (in sed, delete means “skip printing them”).\n","* So everything from `qa:` through end is removed.\n","\n","General command\n","\n","```bash\n","sed [options] 'address command' file\n","```\n","\n","or with multiple commands:\n","\n","```bash\n","sed [options] 'address1 command1; address2 command2' file\n","```\n","\n","* **`[options]`** → e.g. `-n` (suppress automatic printing), `-i` (edit in place).\n","* **`address`** → which lines the command applies to.\n","\n","  * Single line number: `3` → apply to line 3.\n","  * Range: `5,10` → lines 5 through 10.\n","  * Regex: `/pattern/` → any line matching `pattern`.\n","  * Range with regex: `/start/,/end/`.\n","\n","  * `d` → delete the line(s).\n","  * `p` → print the line(s).\n","  * `s/old/new/` → substitute.\n","  * `q` → quit.\n","\n","Example:\n","\n","```bash\n","sed '2,4d' file.txt   # delete lines 2 through 4\n","sed '/foo/s/bar/baz/' file.txt   # substitute \"bar\" with \"baz\" only on lines matching \"foo\"\n","```"],"metadata":{"id":"YO_icnfSRSly"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","sed -i '/^qa:/,$d' Makefile    # remove the erroneous lines. singel quote to prevent the shell to expand `$d$`\n","cat Makefile\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8UAeH0hREzj","executionInfo":{"status":"ok","timestamp":1758328075458,"user_tz":300,"elapsed":16,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"7f652376-df53-4833-d9fb-aece478c2633"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["# Makefile — unified-stocks\n","SHELL := /bin/bash\n",".SHELLFLAGS := -eu -o pipefail -c\n",".ONESHELL:\n","\n","\n","PY := python\n","QUARTO := quarto\n","\n","START ?= 2020-01-01\n","END   ?= 2025-08-01\n","ROLL  ?= 30\n","\n","DATA_RAW := data/raw/prices.csv\n","FEATS    := data/processed/features.parquet\n","REPORT   := docs/reports/eda.html\n","\n","# Default target\n",".DEFAULT_GOAL := help\n","\n",".PHONY: help all clean clobber qa report backup\n","\n","help: ## Show help for each target\n","\t@awk 'BEGIN {FS = \":.*##\"; printf \"Available targets:\\n\"} /^[a-zA-Z0-9_\\-]+:.*##/ {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' $(MAKEFILE_LIST)\n","\n","# all: $(DATA_RAW) $(FEATS) report backup ## Run the full pipeline and back up artifacts\n","all: $(DATA_RAW) $(FEATS) report train backup\n","\n","$(DATA_RAW): scripts/get_prices.py tickers_25.csv\n","\t$(PY) scripts/get_prices.py --tickers tickers_25.csv --start $(START) --end $(END) --out $(DATA_RAW)\n","\n","$(FEATS): scripts/build_features.py $(DATA_RAW) scripts/qa_csv.sh\n","\t# Basic QA first\n","\tscripts/qa_csv.sh $(DATA_RAW)\n","\t$(PY) scripts/build_features.py --input $(DATA_RAW) --out $(FEATS) --roll $(ROLL)\n","\n","# --- add after FEATS definition, near other targets ---\n","\n","TRAIN_METRICS := reports/baseline_metrics.json\n","\n",".PHONY: train\n","train: $(TRAIN_METRICS) ## Train toy baseline and write metrics\n","\n","$(TRAIN_METRICS): scripts/train_baseline.py $(FEATS)\n","\t$(PY) scripts/train_baseline.py --features $(FEATS) --out-metrics $(TRAIN_METRICS)\n","\n","report: $(REPORT) ## Render Quarto EDA to docs1/\n","$(REPORT): reports/eda.qmd _quarto.yml docs1/style.css\n","\t$(QUARTO) render reports/eda.qmd -P symbol:AAPL -P start_date=$(START) -P end_date=$(END) -P rolling=$(ROLL) --output-dir docs1/\n","\t@test -f $(REPORT) || (echo \"Report not generated.\" && exit 1)\n","\n","backup: ## Rsync selected artifacts to backups/<timestamp>/\n","\t./scripts/backup.sh\n","\n","clean: ## Remove intermediate artifacts (safe)\n","\trm -rf data/interim\n","\trm -rf data/processed/*.parquet || true\n","\n","clobber: clean ## Remove generated reports and backups (dangerous)\n","\trm -rf docs/reports || true\n","\trm -rf backups || true\n","\n","DB := data/prices.db\n","\n",".PHONY: db sql-report\n","db: ## Build/refresh SQLite database from CSVs\n","\tpython scripts/build_db.py --db $(DB) --tickers tickers_25.csv --prices data/raw/prices.csv\n","\n","sql-report: db ## Generate a simple SQL-driven CSV summary\n","\t$(PY) - <<-'PY'\n","\timport pandas as pd, sqlite3, os\n","\tcon = sqlite3.connect(\"data/prices.db\")\n","\tdf = pd.read_sql_query(\"\"\"\n","\tSELECT m.sector,\n","\t       COUNT(*) AS n_obs,\n","\t       AVG(ABS(p.log_return)) AS mean_abs_return\n","\tFROM prices p\n","\tJOIN meta m ON p.ticker = m.ticker\n","\tGROUP BY m.sector\n","\tORDER BY n_obs DESC;\n","\t\"\"\", con)\n","\tos.makedirs(\"reports\", exist_ok=True)\n","\tdf.to_csv(\"reports/sql_sector_summary.csv\", index=False)\n","\tprint(df.head())\n","\tcon.close()\n","\tPY\n","\n","\n"]}]},{"cell_type":"markdown","source":["* `-c`: read commands from a string.\n","* `@bash`: make will not echo the command itself\n","* Every $ that should reach the shell must be $$ in a Makefile."],"metadata":{"id":"p9rzPg5tOZUB"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","cat >> Makefile <<'MAKE'   # note need to attach >>\n","qa:\n","\t# TAB above!\n","\tscripts/qa_csv.sh data/raw/prices.csv\n","\n","split-by-ticker:\n","\t@bash -c 'HEADER=$$(head -n 1 data/raw/prices.csv); \\\n","\t  cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort -u | \\\n","\t  while read -r T; do \\\n","\t    mkdir -p data/interim/ticker=$$T; \\\n","\t    { echo \"$$HEADER\"; \\\n","\t      awk -F, -v TK=\"$$T\" '\"'\"'NR>1 && $$1==TK'\"'\"' data/raw/prices.csv; \\\n","\t    } > data/interim/ticker=$$T/prices_$$T.csv; \\\n","\t  done'\n","\n","MAKE\n"],"metadata":{"id":"xFZLZS7IOiy_","executionInfo":{"status":"ok","timestamp":1758328075498,"user_tz":300,"elapsed":37,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# check if Makefile is successfully modified.\n","!cat Makefile"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5Mw3vnBVYvX","executionInfo":{"status":"ok","timestamp":1758328075559,"user_tz":300,"elapsed":56,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"5d98464c-6f5e-4902-e57f-0e439d89b058"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["# Makefile — unified-stocks\n","SHELL := /bin/bash\n",".SHELLFLAGS := -eu -o pipefail -c\n",".ONESHELL:\n","\n","\n","PY := python\n","QUARTO := quarto\n","\n","START ?= 2020-01-01\n","END   ?= 2025-08-01\n","ROLL  ?= 30\n","\n","DATA_RAW := data/raw/prices.csv\n","FEATS    := data/processed/features.parquet\n","REPORT   := docs/reports/eda.html\n","\n","# Default target\n",".DEFAULT_GOAL := help\n","\n",".PHONY: help all clean clobber qa report backup\n","\n","help: ## Show help for each target\n","\t@awk 'BEGIN {FS = \":.*##\"; printf \"Available targets:\\n\"} /^[a-zA-Z0-9_\\-]+:.*##/ {printf \"  \\033[36m%-18s\\033[0m %s\\n\", $$1, $$2}' $(MAKEFILE_LIST)\n","\n","# all: $(DATA_RAW) $(FEATS) report backup ## Run the full pipeline and back up artifacts\n","all: $(DATA_RAW) $(FEATS) report train backup\n","\n","$(DATA_RAW): scripts/get_prices.py tickers_25.csv\n","\t$(PY) scripts/get_prices.py --tickers tickers_25.csv --start $(START) --end $(END) --out $(DATA_RAW)\n","\n","$(FEATS): scripts/build_features.py $(DATA_RAW) scripts/qa_csv.sh\n","\t# Basic QA first\n","\tscripts/qa_csv.sh $(DATA_RAW)\n","\t$(PY) scripts/build_features.py --input $(DATA_RAW) --out $(FEATS) --roll $(ROLL)\n","\n","# --- add after FEATS definition, near other targets ---\n","\n","TRAIN_METRICS := reports/baseline_metrics.json\n","\n",".PHONY: train\n","train: $(TRAIN_METRICS) ## Train toy baseline and write metrics\n","\n","$(TRAIN_METRICS): scripts/train_baseline.py $(FEATS)\n","\t$(PY) scripts/train_baseline.py --features $(FEATS) --out-metrics $(TRAIN_METRICS)\n","\n","report: $(REPORT) ## Render Quarto EDA to docs1/\n","$(REPORT): reports/eda.qmd _quarto.yml docs1/style.css\n","\t$(QUARTO) render reports/eda.qmd -P symbol:AAPL -P start_date=$(START) -P end_date=$(END) -P rolling=$(ROLL) --output-dir docs1/\n","\t@test -f $(REPORT) || (echo \"Report not generated.\" && exit 1)\n","\n","backup: ## Rsync selected artifacts to backups/<timestamp>/\n","\t./scripts/backup.sh\n","\n","clean: ## Remove intermediate artifacts (safe)\n","\trm -rf data/interim\n","\trm -rf data/processed/*.parquet || true\n","\n","clobber: clean ## Remove generated reports and backups (dangerous)\n","\trm -rf docs/reports || true\n","\trm -rf backups || true\n","\n","DB := data/prices.db\n","\n",".PHONY: db sql-report\n","db: ## Build/refresh SQLite database from CSVs\n","\tpython scripts/build_db.py --db $(DB) --tickers tickers_25.csv --prices data/raw/prices.csv\n","\n","sql-report: db ## Generate a simple SQL-driven CSV summary\n","\t$(PY) - <<-'PY'\n","\timport pandas as pd, sqlite3, os\n","\tcon = sqlite3.connect(\"data/prices.db\")\n","\tdf = pd.read_sql_query(\"\"\"\n","\tSELECT m.sector,\n","\t       COUNT(*) AS n_obs,\n","\t       AVG(ABS(p.log_return)) AS mean_abs_return\n","\tFROM prices p\n","\tJOIN meta m ON p.ticker = m.ticker\n","\tGROUP BY m.sector\n","\tORDER BY n_obs DESC;\n","\t\"\"\", con)\n","\tos.makedirs(\"reports\", exist_ok=True)\n","\tdf.to_csv(\"reports/sql_sector_summary.csv\", index=False)\n","\tprint(df.head())\n","\tcon.close()\n","\tPY\n","\n","\n","qa:\n","\t# TAB above!\n","\tscripts/qa_csv.sh data/raw/prices.csv\n","\n","split-by-ticker:\n","\t@bash -c 'HEADER=$$(head -n 1 data/raw/prices.csv); \\\n","\t  cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort -u | \\\n","\t  while read -r T; do \\\n","\t    mkdir -p data/interim/ticker=$$T; \\\n","\t    { echo \"$$HEADER\"; \\\n","\t      awk -F, -v TK=\"$$T\" '\"'\"'NR>1 && $$1==TK'\"'\"' data/raw/prices.csv; \\\n","\t    } > data/interim/ticker=$$T/prices_$$T.csv; \\\n","\t  done'\n","\n"]}]},{"cell_type":"code","source":["!chmod +x scripts/qa_csv.sh"],"metadata":{"id":"ZT1J6rjzEbPx","executionInfo":{"status":"ok","timestamp":1758328075637,"user_tz":300,"elapsed":76,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","# chmod +x scripts/qa_csv.sh\n","make qa\n","make split-by-ticker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHxFAGb2ze97","executionInfo":{"status":"ok","timestamp":1758328076267,"user_tz":300,"elapsed":609,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"a377961b-a185-4bfc-c3ce-76155eac060f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["# TAB above!\n","scripts/qa_csv.sh data/raw/prices.csv\n","OK: data/raw/prices.csv passed basic CSV QA (4501 lines).\n"]}]},{"cell_type":"markdown","source":["## Approach B (much simpler): make the shell for the target be bash and avoid `bash -c`\n","\n","```make\n","SHELL := bash\n",".ONESHELL:\n","\n","split-by-ticker:\n","\tset -euo pipefail\n","\tHEADER=$$(head -n 1 data/raw/prices.csv)\n","\tcut -d, -f1 data/raw/prices.csv | tail -n +2 | sort -u | while read -r T; do\n","\t  mkdir -p \"data/interim/ticker=$$T\"\n","\t  {\n","\t    echo \"$$HEADER\"\n","\t    awk -F, -v TK=\"$$T\" 'NR>1 && $$1==TK' data/raw/prices.csv\n","\t  } > \"data/interim/ticker=$$T/prices_$$T.csv\"\n","\tdone\n","```\n","\n","Notes:\n","\n","* `SHELL := bash` makes recipes run in bash (not `/bin/sh`).\n","* `.ONESHELL:` makes the **whole recipe run in one shell**, so variables like `HEADER` persist across lines, and you don’t need `bash -c` or the quote gymnastics.\n","* You still need **`$$`** for every `$` intended for the shell/awk.\n","\n","\n","## Approach C:  move the loop into a script:\n","\n","```bash\n","# scripts/split_by_ticker.sh\n","#!/usr/bin/env bash\n","set -euo pipefail\n","HEADER=$(head -n 1 data/raw/prices.csv)\n","cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort -u | while read -r T; do\n","  mkdir -p \"data/interim/ticker=$T\"\n","  { echo \"$HEADER\"\n","    awk -F, -v TK=\"$T\" 'NR>1 && $1==TK' data/raw/prices.csv\n","  } > \"data/interim/ticker=$T/prices_$T.csv\"\n","done\n","```\n","\n","Then in Makefile:\n","\n","```make\n","split-by-ticker:\n","\t./scripts/split_by_ticker.sh\n","```\n","\n","This avoids all Makefile quoting rules and is easiest to maintain.\n"],"metadata":{"id":"sxWQqj6-QMJD"}},{"cell_type":"code","source":["%%bash\n","set -euo pipefail\n","cd \"/content/drive/MyDrive/dspt25/STAT4160\"\n","\n","{\n","  echo \"# Mini EDA (shell-only)\"\n","  echo \"Generated: $(date)\"\n","  echo\n","  echo \"## Counts\"\n","  echo \"Lines (incl header): $(wc -l < data/raw/prices.csv)\"\n","  echo \"Unique tickers: $(cut -d, -f1 data/raw/prices.csv | tail -n +2 | sort | uniq | wc -l)\"\n","  echo\n","  echo \"## Top 5 absolute daily moves\"\n","  tail -n +2 data/raw/prices.csv \\\n","  | awk -F, '{a=$5; if(a<0) a=-a; print $1\",\"$2\",\"$5\",\"a}' \\\n","  | sort -t, -k4,4nr | head -n 5\n","  echo\n","  echo \"## Mean log_return by ticker (top 10)\"\n","  awk -F, 'NR>1 { s[$1]+=$5; n[$1]++ } END { for(t in s) printf \"%s,%.6f\\n\", t, s[t]/n[t] }' \\\n","    data/raw/prices.csv | sort -t, -k2,2nr | head -n 10\n","} | tee reports/mini_eda.txt || True"],"metadata":{"id":"HB2_udPFPIex","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758328076293,"user_tz":300,"elapsed":24,"user":{"displayName":"Kincayde Adkins","userId":"06640678183636116981"}},"outputId":"49aff0ed-6699-4a00-cc8a-cb0ecceb7d7b"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["# Mini EDA (shell-only)\n","Generated: Sat Sep 20 12:27:55 AM UTC 2025\n","\n","## Counts\n","Lines (incl header): 4501\n","Unique tickers: 25\n","\n","## Top 5 absolute daily moves\n","XOM,2020-05-08,-9.545439969294023e-05,9.54544e-05\n","NFLX,2020-02-26,9.243919974366577e-05,9.243919974366577e-05\n","HD,2020-07-10,9.230948293748042e-05,9.230948293748042e-05\n","PFE,2020-06-23,8.716362939864553e-05,8.716362939864553e-05\n","BAC,2020-06-04,-8.67879702681762e-05,8.6788e-05\n","\n","## Mean log_return by ticker (top 10)\n","PFE,0.001299\n","T,0.000995\n","ORCL,0.000731\n","V,0.000592\n","GOOGL,0.000490\n","NFLX,0.000474\n","JPM,0.000343\n","AMZN,0.000134\n","WMT,0.000022\n","INTC,-0.000099\n"]}]}]}