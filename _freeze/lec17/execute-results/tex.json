{
  "hash": "b309a8325de28c273e6e3cfcaeeb44ec",
  "result": {
    "markdown": "---\ntitle: \"Session 17 — Feature Timing, Biases & Leakage\"\n---\n\nBelow is a complete lecture package for **Session 17 — Feature Timing, Biases & Leakage** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll **freeze a static ticker universe** (avoid survivorship bias), **formalize label definitions** (t+1 and multi‑step), and add a **leakage test suite** that fails if any feature at time *t* uses information from *t+1* or later.\n\n> **Educational use only — not trading advice.**\n> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) with `data/processed/returns.parquet` and `data/processed/features_v1.parquet` from Sessions 9–10. Cells include safe fallbacks when files are missing.\n\n---\n\n## Session 17 — Feature Timing, Biases & Leakage (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Explain and **avoid look‑ahead** and **survivorship** biases.\n2. Freeze and use a **static ticker universe** chosen from the **train window** (not the whole history).\n3. Define labels correctly (e.g., **t+1** and **t+5**) and verify them with tests.\n4. Add **leakage tests** that recompute trusted features and fail on any future‑peek.\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: what leakage looks like; examples; how it sneaks in\n* **(10 min)** Slides: survivorship bias (today’s constituents ≠ past reality); freezing a universe\n* **(10 min)** Slides: label definitions (t+1, multi‑step) and alignment rules\n* **(35 min)** **In‑class lab**:\n\n  1. Freeze a static universe from the first split’s train window\n  2. Add leakage tests that recompute known‑good features\n  3. Add multi‑step labels (e.g., t+5) with tests\n* **(10 min)** Wrap‑up & homework brief\n\n---\n\n## Slides / talking points (drop into your deck)\n\n### What is data leakage?\n\n* **Look‑ahead leakage:** using any info from *t+1* or later to compute features at *t* or to scale/normalize train and validation together.\n* **Common culprits:** `shift(-1)` in features, global scaling fit on full data, forward‑fill across split boundaries, using today’s close to predict today’s close.\n\n### Survivorship bias\n\n* Using **today’s index membership** to pick tickers for the past ⇒ drops delisted/removed names ⇒ **optimistically biased** results.\n* **Cure:** freeze a **static universe** from the **training window** (e.g., all tickers with ≥ 252 observations by the end of the first train window). Save it and **filter by it** for all future experiments.\n\n### Label definitions (be explicit)\n\n* **t+1 log return**: `r_1d = log_return.shift(-1)` per ticker (your Session‑9 label).\n* **t+5 log return** (multi‑step): `r_5d = log_return.shift(-1) + … + log_return.shift(-5)` per ticker.\n* Rules: labels come from **future**; features come from **≤ t**. Splits with **embargo** reduce adjacency leakage.\n\n---\n\n## In‑class lab (35 min, Colab‑friendly)\n\n> Run each block as its own cell. Update `REPO_NAME` as needed.\n\n### 0) Setup & load data (with fallbacks)\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # <- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"data/static\",\"reports\",\"scripts\",\"tests\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns or synthesize a small fallback\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n    rows=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"TSLA\",\"META\",\"NFLX\"]:\n        eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates, \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        rows.append(df)\n    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Load features_v1 or construct minimal lags for tests\nfpath = Path(\"data/processed/features_v1.parquet\")\nif fpath.exists():\n    feats = pd.read_parquet(fpath).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nelse:\n    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    feats[\"roll_mean_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)\n    feats[\"roll_std_20\"]  = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n    feats[\"zscore_20\"]    = (feats[\"log_return\"] - feats[\"roll_mean_20\"]) / (feats[\"roll_std_20\"] + 1e-8)\n    feats = feats.dropna().reset_index(drop=True)\n\n# Harmonize types\nreturns[\"date\"] = pd.to_datetime(returns[\"date\"])\nfeats[\"date\"]   = pd.to_datetime(feats[\"date\"])\nreturns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\nfeats[\"ticker\"]   = feats[\"ticker\"].astype(\"category\")\nreturns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfeats   = feats.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nreturns.head(3), feats.head(3)\n```\n\n### 1) Freeze a **static universe** from the **first split’s train window**\n\n```python\nimport numpy as np, pandas as pd\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1; splits=[]\n    while True:\n        if i >= len(u): break\n        a,b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve >= len(u): break\n        splits.append((a,b,u[vs],u[ve]))\n        i += step\n    return splits\n\nsplits = make_rolling_origin_splits(returns[\"date\"], train_min=252, val_size=63, step=63, embargo=5)\nassert len(splits) >= 1, \"Not enough history for a first split.\"\na,b,c,d = splits[0]\nprint(\"First train window:\", a.date(), \"→\", b.date())\n\n# Eligible = tickers with at least train_min rows by train_end (b)\ntrain_slice = returns[(returns[\"date\"]>=a) & (returns[\"date\"]<=b)]\ncounts = train_slice.groupby(\"ticker\").size()\neligible = counts[counts >= 252].index.sort_values()\nuniverse = pd.DataFrame({\"ticker\": eligible})\nuniv_name = f\"data/static/universe_{b.date()}.csv\"\nuniverse.to_csv(univ_name, index=False)\nprint(\"Saved static universe:\", univ_name, \"| tickers:\", len(universe))\nuniverse.head()\n```\n\n> From now on, **filter** your data to `universe` before modeling/evaluation.\n\n### 2) Apply the static universe to your features\n\n```python\nfeats_static = feats[feats[\"ticker\"].isin(set(universe[\"ticker\"]))].copy()\nfeats_static.to_parquet(\"data/processed/features_v1_static.parquet\", compression=\"zstd\", index=False)\nprint(\"Wrote data/processed/features_v1_static.parquet\", feats_static.shape)\n```\n\n### 3) Add **leakage tests** that recompute trusted features & compare\n\nCreate a high‑value test file that **fails** if any feature depends on future rows.\n\n```python\n# tests/test_leakage_features.py\nfrom __future__ import annotations\nimport numpy as np, pandas as pd\nimport pytest\n\nSAFE_ROLL = 20\n\n@pytest.fixture(scope=\"session\")\ndef df():\n    import pandas as pd\n    import pathlib\n    p = pathlib.Path(\"data/processed/features_v1_static.parquet\")\n    if not p.exists():\n        p = pathlib.Path(\"data/processed/features_v1.parquet\")\n    df = pd.read_parquet(p).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    return df\n\ndef test_label_definition_r1d(df):\n    for tkr, g in df.groupby(\"ticker\"):\n        assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"r_1d mismatch for {tkr}\"\n\ndef _recompute_safe(g: pd.DataFrame) -> pd.DataFrame:\n    # Recompute causal features using only <= t information\n    out = pd.DataFrame(index=g.index)\n    s = g[\"log_return\"]\n    out[\"lag1\"] = s.shift(1)\n    out[\"lag2\"] = s.shift(2)\n    out[\"lag3\"] = s.shift(3)\n    rm = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).mean()\n    rs = s.rolling(SAFE_ROLL, min_periods=SAFE_ROLL).std()\n    out[\"roll_mean_20\"] = rm\n    out[\"roll_std_20\"]  = rs\n    out[\"zscore_20\"]    = (s - rm) / (rs + 1e-8)\n    # EWM & expanding if present\n    out[\"exp_mean\"] = s.expanding(min_periods=SAFE_ROLL).mean()\n    out[\"exp_std\"]  = s.expanding(min_periods=SAFE_ROLL).std()\n    out[\"ewm_mean_20\"] = s.ewm(span=20, adjust=False).mean()\n    out[\"ewm_std_20\"]  = s.ewm(span=20, adjust=False).std()\n    # RSI(14) if adj_close present\n    if \"adj_close\" in g:\n        delta = g[\"adj_close\"].diff()\n        up = delta.clip(lower=0).ewm(alpha=1/14, adjust=False).mean()\n        dn = (-delta.clip(upper=0)).ewm(alpha=1/14, adjust=False).mean()\n        rs = up / (dn + 1e-12)\n        out[\"rsi_14\"] = 100 - (100/(1+rs))\n    return out\n\n@pytest.mark.parametrize(\"col\", [\"lag1\",\"lag2\",\"lag3\",\"roll_mean_20\",\"roll_std_20\",\"zscore_20\",\"exp_mean\",\"exp_std\",\"ewm_mean_20\",\"ewm_std_20\",\"rsi_14\"])\ndef test_features_match_causal_recompute(df, col):\n    if col not in df.columns:\n        pytest.skip(f\"{col} not present\")\n    # Compare per ticker to avoid cross-group alignment issues\n    for tkr, g in df.groupby(\"ticker\", sort=False):\n        ref = _recompute_safe(g)\n        if col not in ref.columns: \n            continue\n        a = g[col].to_numpy()\n        b = ref[col].to_numpy()\n        # Allow NaNs at the start; compare where both finite\n        mask = np.isfinite(a) & np.isfinite(b)\n        if mask.sum() == 0: \n            continue\n        diff = np.nanmax(np.abs(a[mask] - b[mask]))\n        assert float(diff) <= 1e-6, f\"{col} deviates from causal recompute for {tkr}: max |Δ|={diff}\"\n\ndef test_no_feature_equals_target(df):\n    y = df[\"r_1d\"].to_numpy()\n    for col in df.select_dtypes(include=[\"float32\",\"float64\"]).columns:\n        if col in {\"r_1d\",\"log_return\"}: \n            continue\n        x = df[col].to_numpy()\n        # Proportion of exact equality (within tiny tol) should not be high\n        eq = np.isfinite(x) & np.isfinite(y) & (np.abs(x - y) < 1e-12)\n        assert eq.mean() < 0.8, f\"Suspicious: feature {col} equals target too often\"\n```\n\nRun tests now:\n\n```python\n!pytest -q tests/test_leakage_features.py\n```\n\n> If a test fails, **fix the pipeline**, don’t weaken the test.\n\n### 4) Add **multi‑step labels** (e.g., t+5) and tests\n\n```python\n# scripts/make_multistep_labels.py\nfrom __future__ import annotations\nimport pandas as pd, numpy as np\nfrom pathlib import Path\n\ndef make_multistep(in_parquet=\"data/processed/returns.parquet\", horizons=(5,)):\n    df = pd.read_parquet(in_parquet).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    for H in horizons:\n        # r_Hd = sum of next H log returns: shift(-1) ... shift(-H)\n        s = df.groupby(\"ticker\")[\"log_return\"]\n        acc = None\n        for h in range(1, H+1):\n            sh = s.shift(-h)\n            acc = sh if acc is None else (acc + sh)\n        df[f\"r_{H}d\"] = acc\n    out = df\n    Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n    out.to_parquet(\"data/processed/returns_multistep.parquet\", compression=\"zstd\", index=False)\n    print(\"Wrote data/processed/returns_multistep.parquet\", out.shape)\n\nif __name__ == \"__main__\":\n    make_multistep()\n```\n\nRun it:\n\n```python\n!python scripts/make_multistep_labels.py\n```\n\nAdd a test for label correctness:\n\n```python\n# tests/test_labels_multistep.py\nimport pandas as pd, numpy as np\n\ndef test_r5d_definition():\n    df = pd.read_parquet(\"data/processed/returns_multistep.parquet\").sort_values([\"ticker\",\"date\"])\n    if \"r_5d\" not in df.columns:\n        return\n    for tkr, g in df.groupby(\"ticker\"):\n        lr = g[\"log_return\"]\n        r5 = sum(lr.shift(-h) for h in range(1,6))\n        diff = (g[\"r_5d\"] - r5).abs().max()\n        assert float(diff) < 1e-10, f\"r_5d misdefined for {tkr} (max |Δ|={diff})\"\n```\n\nRun:\n\n```python\n!pytest -q tests/test_labels_multistep.py\n```\n\n---\n\n## Wrap‑up (10 min)\n\n* **Static universe** removes **survivorship bias**: pick tickers with adequate history **by train end** and **stick to them**.\n* Label definitions must be **explicit and tested** (t+1, t+5).\n* Leakage tests **recompute causal features** and compare—if you accidentally used `shift(-1)` or cross‑split fills, tests fail.\n\n---\n\n## Homework (due before Session 18)\n\n**Goal:** Document your evaluation protocol and ship a concise “leakage & bias” memo, plus a one‑command audit.\n\n### Part A — Generate a **protocol memo** (`reports/eval_protocol.md`)\n\n```python\n# scripts/write_eval_protocol.py\nfrom __future__ import annotations\nimport pandas as pd, numpy as np\nfrom pathlib import Path\nfrom datetime import date\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1; out=[]\n    while True:\n        if i >= len(u): break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve >= len(u): break\n        out.append((a,b,u[vs],u[ve])); i += step\n    return out\n\ndef main():\n    ret = pd.read_parquet(\"data/processed/returns.parquet\").sort_values([\"ticker\",\"date\"])\n    splits = make_rolling_origin_splits(ret[\"date\"])\n    a,b,c,d = splits[0]\n    # Universe info\n    univ_files = sorted(Path(\"data/static\").glob(\"universe_*.csv\"))\n    univ = univ_files[-1] if univ_files else None\n    univ_count = pd.read_csv(univ).shape[0] if univ else ret[\"ticker\"].nunique()\n    md = []\n    md += [\"# Evaluation Protocol (Leakage‑Aware)\", \"\"]\n    md += [\"**Date:** \" + date.today().isoformat(), \"\"]\n    md += [\"## Splits\", f\"- Train window (split 1): **{a.date()} → {b.date()}**\",\n           f\"- Embargo: **5** business days\", f\"- Validation window: **{c.date()} → {d.date()}**\",\n           f\"- Step between origins: **63** business days\", \"\"]\n    md += [\"## Static Universe\", f\"- Universe file: **{univ.name if univ else '(none)'}**\",\n           f\"- Count: **{univ_count}** tickers\", \n           \"- Selection rule: tickers with ≥252 obs by first train end; fixed for all splits.\", \"\"]\n    md += [\"## Labels\", \"- `r_1d` = next‑day log return `log_return.shift(-1)` per ticker.\",\n           \"- `r_5d` (if used) = sum of `log_return.shift(-1..-5)`.\", \"\"]\n    md += [\"## Leakage Controls\",\n           \"- Features computed from ≤ t only (rolling/ewm/expanding without negative shifts).\",\n           \"- No forward‑fill across split boundaries; embargo = 5 days.\",\n           \"- Scalers/normalizers fit on TRAIN only.\",\n           \"- Tests: `tests/test_leakage_features.py`, `tests/test_labels_multistep.py`.\", \"\"]\n    md += [\"## Caveats\",\n           \"- Educational dataset; not investment advice.\",\n           \"- Survivorship minimized via static universe; still subject to data vendor quirks.\", \"\"]\n    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n    Path(\"reports/eval_protocol.md\").write_text(\"\\n\".join(md))\n    print(\"Wrote reports/eval_protocol.md\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun:\n\n```python\n!python scripts/write_eval_protocol.py\n```\n\n### Part B — One‑command **leakage audit** target\n\nAppend to your `Makefile`:\n\n```make\n.PHONY: leakage-audit\nleakage-audit: ## Run leakage & label tests; write eval protocol\n\\tpytest -q tests/test_leakage_features.py tests/test_labels_multistep.py\n\\tpython scripts/write_eval_protocol.py\n```\n\nThen run:\n\n```bash\nmake leakage-audit\n```\n\n### Part C — Short memo (1–2 pages max)\n\n* Open `reports/eval_protocol.md` and add **two paragraphs** in your own words:\n\n  1. Why these splits and embargo are credible for your task.\n  2. Where leakage could still hide (e.g., future macro revisions, implicit target leakage), and how you’d detect it.\n\n> Submit the updated `reports/eval_protocol.md` and a screenshot of `make leakage-audit` passing.\n\n### Part D — (Optional) Quarto inclusion\n\nAdd this to your Quarto report:\n\n````markdown\n## Evaluation Protocol (Leakage‑Aware)\n\n::: {.cell execution_count=1}\n````` {.python .cell-code}\nfrom pathlib import Path\nprint(Path(\"reports/eval_protocol.md\").read_text())\n````\n`````\n:::\n\n\n---\n\n## Instructor checklist (before class)\n- Ensure `returns.parquet` and `features_v1.parquet` exist or fallback works.  \n- Intentionally create a leaked feature (e.g., `lag1 = log_return.shift(-1)`) on your copy to show tests **failing**, then fix.  \n- Decide an anchor date policy for universe freeze; today’s lab uses **first split’s train end**.\n\n## Emphasize while teaching\n- **Define labels first**, then prove features are **causal (≤ t)**.  \n- Freezing the **universe** is small effort with big impact on credibility.  \n- Tests are your **guardrails**—if they go red, **don’t** relax them; fix the pipeline.\n\n## Grading (pass/revise)\n- `data/static/universe_YYYY-MM-DD.csv` created; `features_v1_static.parquet` filtered by it.  \n- Leakage tests present and **green** on the clean pipeline; **red** if you inject a future‑peek.  \n- `reports/eval_protocol.md` exists and includes student commentary.  \n- `make leakage-audit` runs without errors.\n\nYou now have a **credibility layer** on top of your data pipeline—ready to analyze regimes and calibration next (Session 18).\n```\n\n",
    "supporting": [
      "lec17_files\\figure-pdf"
    ],
    "filters": []
  }
}