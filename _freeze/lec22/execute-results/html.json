{
  "hash": "0bf6b7e9fa0515738519b34c7527de79",
  "result": {
    "markdown": "---\ntitle: \"Session 22 — Adapting GPT to Time Series\"\n---\n\nBelow is a complete lecture package for **Session 22 — Adapting GPT to Time Series** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll **replace token embeddings with a linear projection of real‑valued features**, keep **positional embeddings** and a **causal mask**, and train a **tiny Transformer (GPT‑style)** to predict **next‑day return**.\n\n> **Educational use only — not trading advice.**\n> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) and availability of `data/processed/features_v1.parquet` (or `features_v1_static.parquet`) with columns like `ticker`, `date`, `log_return`, `r_1d`, plus features (`lag1..lag3`, `roll_std_20`, `zscore_20`, …). Cells include **safe fallbacks** so the class can run even if files are missing.\n\n---\n\n## Session 22 — Adapting GPT to Time Series (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Convert a **window of real‑valued features** into a sequence input for a GPT‑style model.\n2. Use **positional embeddings** and **causal self‑attention** for **sequence‑to‑one regression** (predict `r_1d[t]` from `x[≤t]`).\n3. Train with **MAE/Huber** loss, **early stopping**, and **AMP** (optional on CUDA).\n4. Save reproducible artifacts (checkpoint + metrics CSV) and produce a quick loss plot.\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: from tokens to real‑valued features; causal masking & alignment; loss choices\n* **(10 min)** Slides: tiny TS‑GPT architecture (feature projector → pos emb → blocks → head)\n* **(35 min)** **In‑class lab**: dataset → TS‑GPT → train/validate → save metrics & checkpoint → quick plots\n* **(10 min)** Wrap‑up + homework brief\n* **(10 min)** Buffer / Q\\&A\n\n---\n\n## Slide talking points (drop into your deck)\n\n### 1) From characters to features\n\n* Char‑GPT maps **discrete tokens → embeddings**.\n* **TS‑GPT** maps **real‑valued features `x_t∈ℝ^F` → `ℝ^d_model`** via a **linear projection** at each time step:\n\n  $$\n  h_t^{(0)} = W_\\text{proj} x_t + b + \\text{pos\\_emb}(t).\n  $$\n\n### 2) Labels & alignment (no leakage)\n\n* **Input window:** $X_{t-T+1:t}$ (length $T$).\n* **Target:** $y_t = r\\_1d[t]$ (next‑day log return), computed **outside** the model and **shifted by −1**.\n* **Causal mask** ensures the block at step $t$ cannot see $>t$.\n\n### 3) Loss & scaling\n\n* **Loss:** MAE or **Huber** (robust to outliers in returns); report **MAE** and **sMAPE**.\n* **Scaling:** Fit a **feature scaler on TRAIN only**, reuse for VAL/TEST.\n\n### 4) Tiny TS‑GPT config (Colab‑friendly)\n\n* `d_model=64`, `n_head=2`, `n_layer=2`, `d_ff=128`, `ctx=64`, dropout 0.0–0.1, batch 128–256.\n* Save: `models/tsgpt_split1.pt` and `reports/tsgpt_split1_metrics.csv`.\n\n---\n\n## In‑class lab (35 min, Colab‑friendly)\n\n> Run each block as its **own cell**. Update `REPO_NAME` if needed.\n\n### 0) Setup & device\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME = \"unified-stocks-teamX\"  # <- change if needed\nBASE_DIR  = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR  = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, random, math, time\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\",\"docs/figs\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd(), \"| Python\", sys.version.split()[0], \"| OS\", platform.system())\n\nimport numpy as np, pandas as pd, torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Device:\", device)\n\ndef seed_everything(seed=2222):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\nseed_everything()\n```\n\n### 1) Load features; build split; TRAIN‑fit scaler (with fallbacks)\n\n```python\nfrom pathlib import Path\n\n# Prefer static universe from Session 17\nf_static = Path(\"data/processed/features_v1_static.parquet\")\nf_base   = Path(\"data/processed/features_v1.parquet\")\n\nif f_static.exists():\n    df = pd.read_parquet(f_static)\nelif f_base.exists():\n    df = pd.read_parquet(f_base)\nelse:\n    # Fallback from returns; add minimal features\n    rpath = Path(\"data/processed/returns.parquet\")\n    if rpath.exists():\n        df = pd.read_parquet(rpath).sort_values([\"ticker\",\"date\"])\n    else:\n        # synthesize small dataset\n        rng = np.random.default_rng(0)\n        dates = pd.bdate_range(\"2022-01-03\", periods=320)\n        frames=[]\n        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\"]:\n            eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n            adj = 100*np.exp(np.cumsum(eps))\n            g = pd.DataFrame({\n                \"date\": dates, \"ticker\": t,\n                \"adj_close\": adj.astype(\"float32\"),\n                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n            })\n            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n            frames.append(g)\n        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    # add basic features\n    df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    for k in [1,2,3]:\n        df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n    df[\"roll_std_20\"]  = df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n    df[\"zscore_20\"]    = (df[\"log_return\"] - df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)) / (df[\"roll_std_20\"] + 1e-8)\n    df = df.dropna().reset_index(drop=True)\n\n# Harmonize and subset for speed\ndf[\"date\"] = pd.to_datetime(df[\"date\"]); df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nkeep = df[\"ticker\"].cat.categories.tolist()[:10]  # up to 10 tickers for class\ndf = df[df[\"ticker\"].isin(keep)].copy()\n\n# Choose features (keep small & causal)\nCAND = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\",\"weekday\",\"month\"]\nFEATS = [c for c in CAND if c in df.columns]\nassert \"r_1d\" in df.columns and len(FEATS)>0, \"Missing label or features.\"\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i=train_min-1; out=[]\n    while True:\n        if i>=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\nsplits = make_rolling_origin_splits(df[\"date\"], 252, 63, 63, 5)\nassert splits, \"Not enough history.\"\na,b,c,d = splits[0]\ntrain_df = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)].copy()\nval_df   = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)].copy()\nprint(\"Split1: train\", a.date(), \"→\", b.date(), \"| val\", c.date(), \"→\", d.date(),\n      \"| train rows:\", len(train_df), \"| val rows:\", len(val_df))\n```\n\n### 2) Dataset with **feature projection** targetting `r_1d`\n\n```python\nfrom torch.utils.data import Dataset, DataLoader\nimport json\n\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self\n    def transform(self, X): return (X - self.mean_) / self.std_\n    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\n\nclass WindowedTS(Dataset):\n    \"\"\"\n    Sliding windows per ticker. Each item:\n      X_scaled: (T, F), y: scalar r_1d at window end, ticker_id: long\n    \"\"\"\n    def __init__(self, frame: pd.DataFrame, feats, T=64, scaler: FeatureScaler|None=None,\n                 ticker2id: dict|None=None):\n        self.T = int(T); self.feats=list(feats); self.idx=[]; self.groups={}\n        # Build ticker ids (frozen on TRAIN)\n        if ticker2id is None:\n            cats = frame[\"ticker\"].astype(\"category\").cat.categories.tolist()\n            self.ticker2id = {t:i for i,t in enumerate(cats)}\n        else:\n            self.ticker2id = dict(ticker2id)\n        # Per‑ticker arrays and index of valid windows\n        for tkr, g in frame.groupby(\"ticker\"):\n            g = g.sort_values(\"date\").reset_index(drop=True)\n            X = g[self.feats].to_numpy(\"float32\")\n            y = g[\"r_1d\"].to_numpy(\"float32\")\n            tid = self.ticker2id[str(tkr)]\n            for end in range(self.T-1, len(g)):\n                if np.isfinite(y[end]): self.idx.append((str(tkr), end, tid))\n            self.groups[str(tkr)] = {\"X\": X, \"y\": y}\n        # Fit scaler on TRAIN if not provided\n        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.groups[t][\"X\"] for t in self.groups], 0))\n\n    def __len__(self): return len(self.idx)\n    def __getitem__(self, i):\n        tkr, end, tid = self.idx[i]\n        g = self.groups[tkr]\n        X = g[\"X\"][end-self.T+1:end+1]\n        X = self.scaler.transform(X)\n        y = g[\"y\"][end]\n        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), torch.tensor(tid, dtype=torch.long)\n\n# Build TRAIN/VAL datasets & loaders\nCTX = 64; BATCH=256; WORKERS=2; PIN=torch.cuda.is_available()\n\ntrain_ds = WindowedTS(train_df, FEATS, T=CTX)\nval_ds   = WindowedTS(val_df,   FEATS, T=CTX, scaler=train_ds.scaler, ticker2id=train_ds.ticker2id)\n\n# Persist scaler (reproducibility)\nPath(\"models\").mkdir(exist_ok=True)\nPath(\"models/tsgpt_scaler_split1.json\").write_text(json.dumps(train_ds.scaler.state_dict()))\n\ndef _seed_worker(_):\n    ws = torch.initial_seed() % (2**32)\n    np.random.seed(ws); random.seed(ws)\n\ng = torch.Generator(); g.manual_seed(42)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),\n                          worker_init_fn=_seed_worker, generator=g)\nval_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),\n                          worker_init_fn=_seed_worker)\n\nlen(train_ds), len(val_ds), next(iter(train_loader))[0].shape\n```\n\n### 3) **Time‑Series GPT** (feature projector + pos emb + Transformer blocks + regression head)\n\n```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nBLOCK_SIZE = CTX  # maximum context supported\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):\n        super().__init__()\n        assert d_model % n_head == 0\n        self.n_head = n_head; self.d_head = d_model // n_head\n        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n        self.proj = nn.Linear(d_model, d_model, bias=False)\n        self.attn_drop = nn.Dropout(dropout); self.resid_drop = nn.Dropout(dropout)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)).unsqueeze(0).unsqueeze(0))\n    def forward(self, x):\n        B,T,C = x.size()\n        qkv = self.qkv(x); q,k,v = qkv.split(C, dim=2)\n        q = q.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        k = k.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        v = v.view(B,T,self.n_head,self.d_head).transpose(1,2)\n        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d_head)\n        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1); att = self.attn_drop(att)\n        y = att @ v\n        y = y.transpose(1,2).contiguous().view(B,T,C)\n        return self.resid_drop(self.proj(y))\n\nclass Block(nn.Module):\n    def __init__(self, d_model: int, n_head: int, d_ff: int, dropout: float = 0.0):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model); self.attn = CausalSelfAttention(d_model, n_head, dropout)\n        self.ln2 = nn.LayerNorm(d_model); self.mlp  = nn.Sequential(\n            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model), nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass TimeSeriesGPT(nn.Module):\n    \"\"\"\n    Real-valued sequence → regression (predict r_1d at last step).\n    \"\"\"\n    def __init__(self, in_features: int, d_model=64, n_head=2, n_layer=2, d_ff=128, ctx=64, dropout=0.0,\n                 n_tickers: int|None=None, d_ticker: int=0):\n        super().__init__()\n        self.ctx = ctx\n        self.proj = nn.Linear(in_features, d_model)        # FEATURE PROJECTION\n        self.pos  = nn.Embedding(ctx, d_model)             # POSITIONAL EMBEDDINGS\n        self.id_emb = nn.Embedding(n_tickers, d_ticker) if (n_tickers and d_ticker>0) else None\n        augmented = d_model + (d_ticker if self.id_emb else 0)\n        self.blocks = nn.ModuleList([Block(augmented, n_head, d_ff, dropout) for _ in range(n_layer)])\n        self.ln = nn.LayerNorm(augmented)\n        self.head = nn.Linear(augmented, 1)                # REGRESSION HEAD\n\n        self.apply(self._init)\n    def _init(self, m):\n        if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight); \n        if isinstance(m, nn.Embedding): nn.init.normal_(m.weight, 0.0, 0.02)\n    def forward(self, x, ticker_ids=None):\n        # x: (B,T,F)\n        B,T,F = x.size()\n        pos = torch.arange(T, device=x.device)\n        h = self.proj(x) + self.pos(pos)[None,:,:]         # (B,T,d_model)\n        if self.id_emb is not None and ticker_ids is not None:\n            e = self.id_emb(ticker_ids).unsqueeze(1).expand(-1, T, -1)   # (B,T,d_ticker)\n            h = torch.cat([h, e], dim=-1)                                 # (B,T,d_model+d_ticker)\n        for blk in self.blocks: h = blk(h)\n        h = self.ln(h)\n        yhat = self.head(h[:,-1,:]).squeeze(-1)            # use last time step’s hidden state\n        return yhat\n\n# Instantiate tiny model (no ticker embedding for baseline)\nD_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT = 64, 2, 2, 128, 0.0\nmodel = TimeSeriesGPT(in_features=len(FEATS), d_model=D_MODEL, n_head=N_HEAD,\n                      n_layer=N_LAYER, d_ff=D_FF, ctx=CTX, dropout=DROPOUT,\n                      n_tickers=None, d_ticker=0).to(device)\nsum(p.numel() for p in model.parameters())/1e6, device\n```\n\n### 4) Train (MAE or Huber), early stop, checkpoint, and metrics\n\n```python\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef mae_t(y, yhat): return torch.mean(torch.abs(y - yhat))\ndef smape_t(y, yhat, eps=1e-8): return torch.mean(2.0*torch.abs(y - yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\nUSE_HUBER = True\nhuber_delta = 0.01\nhuber = torch.nn.HuberLoss(delta=huber_delta, reduction=\"mean\")\n\ndef train_one_epoch(model, loader, opt, scaler, device, use_amp=True):\n    model.train(); tot=0.0; n=0\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        opt.zero_grad(set_to_none=True)\n        if use_amp and device.type==\"cuda\":\n            with autocast(dtype=torch.float16):\n                yhat = model(xb)\n                loss = huber(yhat, yb) if USE_HUBER else mae_t(yb, yhat)\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n        else:\n            yhat = model(xb); loss = huber(yhat, yb) if USE_HUBER else mae_t(yb, yhat)\n            loss.backward(); opt.step()\n        bs = xb.size(0); tot += loss.item()*bs; n += bs\n    return tot/max(n,1)\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval(); m_mae=m_smape=0.0; n=0\n    for xb, yb, _ in loader:\n        xb=xb.to(device, non_blocking=True).float()\n        yb=yb.to(device, non_blocking=True).float()\n        yhat = model(xb)\n        bs=xb.size(0)\n        m_mae += mae_t(yb, yhat).item()*bs\n        m_smape += smape_t(yb, yhat).item()*bs\n        n+=bs\n    return {\"mae\": m_mae/max(n,1), \"smape\": m_smape/max(n,1)}\n\ndef fit(model, train_loader, val_loader, epochs=12, lr=2e-3, wd=1e-5, patience=3, use_amp=True):\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(0.9,0.99))\n    scaler = GradScaler(enabled=(use_amp and device.type==\"cuda\"))\n    best=float(\"inf\"); best_ep=-1; ckpt=Path(\"models/tsgpt_split1.pt\")\n    history=[]\n    for ep in range(1, epochs+1):\n        tr = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n        val = evaluate(model, val_loader, device)\n        history.append({\"epoch\":ep,\"train_mae\":tr,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"]})\n        print(f\"Epoch {ep:02d}  train_mae={tr:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}\")\n        if val[\"mae\"] < best - 1e-6:\n            best = val[\"mae\"]; best_ep=ep\n            torch.save({\"model\": model.state_dict(),\n                        \"epoch\": ep,\n                        \"config\": {\"ctx\":CTX,\"d_model\":D_MODEL,\"n_head\":N_HEAD,\"n_layer\":N_LAYER,\n                                   \"d_ff\":D_FF,\"dropout\":DROPOUT,\"feats\":FEATS,\"huber\":USE_HUBER}}, ckpt)\n        elif ep - best_ep >= patience:\n            print(f\"Early stopping at epoch {ep} (best {best:.5f} @ {best_ep})\")\n            break\n    return history, best, best_ep, ckpt\n\nhistory, best, best_ep, ckpt = fit(model, train_loader, val_loader, epochs=10, lr=2e-3, wd=1e-5, patience=3, use_amp=True)\nprint(\"Best val_mae:\", best, \"| epoch:\", best_ep, \"| saved:\", ckpt.exists())\n\n# Save metrics CSV & quick plot data\npd.DataFrame(history).to_csv(\"reports/tsgpt_train_curve.csv\", index=False)\nfinal = evaluate(model, val_loader, device)\npd.DataFrame([{\"split\":1,\"model\":\"tsgpt\",\"ctx\":CTX,\"val_mae\":final[\"mae\"],\"val_smape\":final[\"smape\"],\n               \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3),\"best_epoch\":best_ep}]).to_csv(\n    \"reports/tsgpt_split1_metrics.csv\", index=False)\nfinal\n```\n\n### 5) Quick loss plot (optional in‑class visualization)\n\n```python\nimport matplotlib.pyplot as plt, pandas as pd\ncur = pd.read_csv(\"reports/tsgpt_train_curve.csv\")\nplt.figure(figsize=(6,3.5))\nplt.plot(cur[\"epoch\"], cur[\"train_mae\"], marker=\"o\", label=\"train MAE\")\nplt.plot(cur[\"epoch\"], cur[\"val_mae\"], marker=\"s\", label=\"val MAE\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"MAE\"); plt.title(\"TS-GPT training\"); plt.legend(); plt.tight_layout()\nplt.savefig(\"docs/figs/tsgpt_train_curve.png\", dpi=160)\n\"Saved docs/figs/tsgpt_train_curve.png\"\n```\n\n---\n\n## Wrap‑up (10 min) — emphasize these points\n\n* **Feature projection ≈ token embeddings** for real‑valued inputs; **positional embeddings + causal mask** stay the same.\n* Use **TRAIN‑fit scaler** to avoid leakage; keep the **rolling‑origin split** and **embargo** from previous sessions.\n* For noisy returns, **Huber/MAE** often stabilize training better than MSE.\n* Save checkpoints & metrics; we’ll ablate config choices (context/head/dropout) in the homework.\n\n---\n\n## Homework (due before Session 23)\n\n**Goal:** Run a small ablation on **context length (32 vs 64)**, **dropout (0.0 vs 0.1)**, and **heads (2 vs 4)**, then summarize results in one table and add a short discussion to your Quarto report.\n\n### A. Training script: `scripts/tsgpt_train.py` (configurable TS‑GPT)\n\n```python\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, json, math, random\nfrom pathlib import Path\nimport numpy as np, pandas as pd, torch, torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# --------- Dataset & scaler (compact) ----------\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self\n    def transform(self, X): return (X - self.mean_) / self.std_\n\nclass WindowedTS(Dataset):\n    def __init__(self, df, feats, T=64, scaler=None, ticker2id=None):\n        self.T=T; self.feats=list(feats); self.idx=[]; self.g={}\n        if ticker2id is None:\n            cats=df[\"ticker\"].astype(\"category\").cat.categories.tolist()\n            self.ticker2id={t:i for i,t in enumerate(cats)}\n        else: self.ticker2id=dict(ticker2id)\n        for tkr,g in df.groupby(\"ticker\"):\n            g=g.sort_values(\"date\").reset_index(drop=True)\n            X=g[self.feats].to_numpy(\"float32\"); y=g[\"r_1d\"].to_numpy(\"float32\")\n            for end in range(T-1,len(g)):\n                if np.isfinite(y[end]): self.idx.append((str(tkr), end, self.ticker2id[str(tkr)]))\n            self.g[str(tkr)]={\"X\":X,\"y\":y}\n        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.g[t][\"X\"] for t in self.g],0))\n    def __len__(self): return len(self.idx)\n    def __getitem__(self,i):\n        tkr,end,tid=self.idx[i]; g=self.g[tkr]\n        X=self.scaler.transform(g[\"X\"][end-self.T+1:end+1]); y=g[\"y\"][end]\n        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), torch.tensor(tid, dtype=torch.long)\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i=train_min-1; out=[]\n    while True:\n        if i>=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\n# --------- Model ----------\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, d_model, n_head, ctx, p=0.0):\n        super().__init__(); assert d_model % n_head == 0\n        self.nh=n_head; dh=d_model//n_head; self.dh=dh; self.ctx=ctx\n        self.qkv=nn.Linear(d_model,3*d_model,bias=False); self.proj=nn.Linear(d_model,d_model,bias=False)\n        self.ad=nn.Dropout(p); self.rd=nn.Dropout(p)\n        self.register_buffer(\"mask\", torch.tril(torch.ones(ctx,ctx)).unsqueeze(0).unsqueeze(0))\n    def forward(self,x):\n        B,T,C=x.shape; qkv=self.qkv(x); q,k,v=qkv.split(C,dim=2)\n        q=q.view(B,T,self.nh,self.dh).transpose(1,2); k=k.view(B,T,self.nh,self.dh).transpose(1,2); v=v.view(B,T,self.nh,self.dh).transpose(1,2)\n        att=(q @ k.transpose(-2,-1))/math.sqrt(self.dh); att=att.masked_fill(self.mask[:,:,:T,:T]==0, float(\"-inf\"))\n        att=att.softmax(dim=-1); att=self.ad(att); y=att @ v; y=y.transpose(1,2).contiguous().view(B,T,C)\n        return self.rd(self.proj(y))\n\nclass Block(nn.Module):\n    def __init__(self, d_model, n_head, ctx, d_ff, p=0.0):\n        super().__init__()\n        self.ln1=nn.LayerNorm(d_model); self.att=CausalSelfAttention(d_model,n_head,ctx,p)\n        self.ln2=nn.LayerNorm(d_model); self.mlp=nn.Sequential(nn.Linear(d_model,d_ff), nn.GELU(), nn.Dropout(p), nn.Linear(d_ff,d_model), nn.Dropout(p))\n    def forward(self,x): x=x+self.att(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x\n\nclass TimeSeriesGPT(nn.Module):\n    def __init__(self, in_f, ctx=64, d_model=64, n_head=2, n_layer=2, d_ff=128, p=0.0, n_tickers=None, d_ticker=0):\n        super().__init__(); self.ctx=ctx\n        self.proj=nn.Linear(in_f,d_model); self.pos=nn.Embedding(ctx,d_model)\n        self.id_emb=nn.Embedding(n_tickers,d_ticker) if (n_tickers and d_ticker>0) else None\n        aug=d_model+(d_ticker if self.id_emb else 0)\n        self.blocks=nn.ModuleList([Block(aug,n_head,ctx,d_ff,p) for _ in range(n_layer)])\n        self.ln=nn.LayerNorm(aug); self.head=nn.Linear(aug,1)\n        self.apply(self._init)\n    def _init(self,m):\n        if isinstance(m,nn.Linear): nn.init.xavier_uniform_(m.weight)\n        if isinstance(m,nn.Embedding): nn.init.normal_(m.weight,0.0,0.02)\n    def forward(self,x,tid=None):\n        B,T,F=x.shape; pos=torch.arange(T, device=x.device)\n        h=self.proj(x)+self.pos(pos)[None,:,:]\n        if self.id_emb is not None and tid is not None:\n            e=self.id_emb(tid).unsqueeze(1).expand(-1,T,-1); h=torch.cat([h,e],dim=-1)\n        for blk in self.blocks: h=blk(h)\n        h=self.ln(h); return self.head(h[:,-1,:]).squeeze(-1)\n\ndef main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--context\", type=int, default=64)\n    ap.add_argument(\"--d_model\", type=int, default=64)\n    ap.add_argument(\"--n_head\", type=int, default=2)\n    ap.add_argument(\"--n_layer\", type=int, default=2)\n    ap.add_argument(\"--d_ff\", type=int, default=128)\n    ap.add_argument(\"--dropout\", type=float, default=0.0)\n    ap.add_argument(\"--epochs\", type=int, default=10)\n    ap.add_argument(\"--batch\", type=int, default=256)\n    ap.add_argument(\"--lr\", type=float, default=2e-3)\n    ap.add_argument(\"--patience\", type=int, default=3)\n    ap.add_argument(\"--tickers\", type=int, default=10)\n    ap.add_argument(\"--out\", default=\"reports/tsgpt_metrics.csv\")\n    args=ap.parse_args()\n\n    # Load data\n    f_static = Path(\"data/processed/features_v1_static.parquet\")\n    df = pd.read_parquet(f_static) if f_static.exists() else pd.read_parquet(args.features)\n    df=df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True); df[\"ticker\"]=df[\"ticker\"].astype(\"category\")\n    cand=[\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\",\"weekday\",\"month\"]\n    feats=[c for c in cand if c in df.columns]; assert \"r_1d\" in df.columns and feats\n    keep=df[\"ticker\"].cat.categories.tolist()[:args.tickers]; df=df[df[\"ticker\"].isin(keep)].copy()\n\n    # Split\n    def splits(dates, train_min=252, val=63, step=63, embargo=5):\n        u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n        while True:\n            if i>=len(u): break\n            a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val-1\n            if ve>=len(u): break\n            out.append((a,b,u[vs],u[ve])); i+=step\n        return out\n    a,b,c,d = splits(df[\"date\"])[0]\n    tr=df[(df[\"date\"]>=a)&(df[\"date\"]<=b)]; va=df[(df[\"date\"]>=c)&(df[\"date\"]<=d)]\n\n    # Datasets\n    tr_ds=WindowedTS(tr, feats, T=args.context); va_ds=WindowedTS(va, feats, T=args.context, scaler=tr_ds.scaler, ticker2id=tr_ds.ticker2id)\n    pin=torch.cuda.is_available()\n    def _seed(_): ws=torch.initial_seed()%2**32; np.random.seed(ws); random.seed(ws)\n    g=torch.Generator(); g.manual_seed(42)\n    tr_ld=DataLoader(tr_ds, batch_size=args.batch, shuffle=True, drop_last=True, num_workers=2, pin_memory=pin, worker_init_fn=_seed, generator=g)\n    va_ld=DataLoader(va_ds, batch_size=args.batch, shuffle=False, drop_last=False, num_workers=2, pin_memory=pin, worker_init_fn=_seed)\n\n    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    net=TimeSeriesGPT(len(feats), ctx=args.context, d_model=args.d_model, n_head=args.n_head, n_layer=args.n_layer, d_ff=args.d_ff, p=args.dropout).to(device)\n    opt=torch.optim.AdamW(net.parameters(), lr=args.lr, weight_decay=1e-5, betas=(0.9,0.99))\n    huber=torch.nn.HuberLoss(delta=0.01)\n\n    def mae_t(y,yhat): return torch.mean(torch.abs(y - yhat))\n    def smape_t(y,yhat,eps=1e-8): return torch.mean(2*torch.abs(y-yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\n    best=1e9; best_ep=0\n    for ep in range(1, args.epochs+1):\n        net.train()\n        for xb,yb,_ in tr_ld:\n            xb=xb.to(device).float(); yb=yb.to(device).float()\n            opt.zero_grad(set_to_none=True)\n            yhat=net(xb); loss=huber(yhat,yb); loss.backward()\n            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n            opt.step()\n        # val\n        net.eval(); m_mae=m_smape=0.0; n=0\n        with torch.no_grad():\n            for xb,yb,_ in va_ld:\n                xb=xb.to(device).float(); yb=yb.to(device).float()\n                yhat=net(xb); bs=xb.size(0)\n                m_mae += mae_t(yb,yhat).item()*bs; m_smape += smape_t(yb,yhat).item()*bs; n+=bs\n        val_mae=m_mae/n; val_smape=m_smape/n\n        print(f\"Epoch {ep:02d}  val_mae={val_mae:.5f}  val_sMAPE={val_smape:.5f}\")\n        if val_mae < best-1e-6: best=val_mae; best_ep=ep\n        elif ep-best_ep >= args.patience: break\n\n    Path(\"reports\").mkdir(exist_ok=True)\n    pd.DataFrame([{\"model\":\"tsgpt\",\"ctx\":args.context,\"d_model\":args.d_model,\"n_head\":args.n_head,\n                   \"n_layer\":args.n_layer,\"dropout\":args.dropout,\"val_mae\":best,\"best_epoch\":best_ep}]).to_csv(args.out, index=False)\n    print(\"Wrote\", args.out)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun examples:\n\n```bash\n%%bash\nchmod +x scripts/tsgpt_train.py\npython scripts/tsgpt_train.py --context 32 --dropout 0.0 --n_head 2 --n_layer 2 --out reports/tsgpt_run_a.csv\npython scripts/tsgpt_train.py --context 64 --dropout 0.1 --n_head 4 --n_layer 2 --out reports/tsgpt_run_b.csv\n```\n\n### B. Summarize ablations into one table\n\n```python\nimport pandas as pd, glob\npaths = glob.glob(\"reports/tsgpt_run_*.csv\")\nrows = []\nfor p in paths:\n    try:\n        r = pd.read_csv(p).iloc[0].to_dict(); r[\"run\"] = p; rows.append(r)\n    except Exception as e:\n        print(\"Skip\", p, e)\nab = pd.DataFrame(rows).sort_values(\"val_mae\")\nab.to_csv(\"reports/tsgpt_ablation_summary.csv\", index=False)\nab.head(10)\n```\n\n### C. Add to Quarto report (one section)\n\nIn `reports/eda.qmd` (or `reports/ts_gpt.qmd`), add:\n\n````markdown\n## Tiny Transformer for Time Series\n\n**Split 1** results for TS‑GPT (sequence‑to‑one regression).\n\n::: {#80a7e242 .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\nprint(pd.read_csv(\"reports/tsgpt_split1_metrics.csv\"))\nprint(pd.read_csv(\"reports/tsgpt_ablation_summary.csv\").sort_values(\"val_mae\").head(8))\n````\n\n![TS‑GPT training curve](../docs/figs/tsgpt_train_curve.png)\n\n````\n\n### D. Minimal tests (protect causal mask & window shapes)\n```python\n# tests/test_tsgpt_mask_and_shapes.py\nimport torch, pandas as pd\nfrom pathlib import Path\n\ndef test_mask_is_causal():\n    from scripts.tsgpt_train import TimeSeriesGPT\n    m = TimeSeriesGPT(in_f=4, ctx=16, d_model=32, n_head=2, n_layer=1, d_ff=64, p=0.0)\n    att = [b.att for b in m.blocks][0]\n    M = att.mask[0,0]\n    assert torch.all(M.triu(diagonal=1)==0)\n\ndef test_window_shape():\n    df = pd.read_parquet(\"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"])\n    feats = [c for c in [\"log_return\",\"lag1\",\"lag2\",\"lag3\"] if c in df.columns]\n    assert feats\n    from scripts.tsgpt_train import WindowedTS, make_splits\n    a,b,c,d = make_splits(df[\"date\"])[0]\n    ds = WindowedTS(df[(df[\"date\"]>=a)&(df[\"date\"]<=b)], feats, T=32)\n    X, y, tid = ds[0]\n    assert X.shape == (32, len(feats))\n    assert torch.isfinite(torch.tensor(y)).item() == 1\n````\n\nRun:\n\n```bash\n%%bash\npytest -q -k tsgpt_mask_and_shapes\n`````\n:::\n\n\n### E. (Optional) Makefile targets\n\nAppend to `Makefile`:\n\n```make\n.PHONY: train-tsgpt ablate-tsgpt\ntrain-tsgpt: ## Train TS-GPT on split 1 (tiny config)\n\\tpython scripts/tsgpt_train.py --context 64 --n_head 2 --n_layer 2 --dropout 0.0 --out reports/tsgpt_run_base.csv\n\nablate-tsgpt: ## Run two small ablations\n\\tpython scripts/tsgpt_train.py --context 32 --n_head 2 --dropout 0.0 --out reports/tsgpt_run_a.csv\n\\tpython scripts/tsgpt_train.py --context 64 --n_head 4 --dropout 0.1 --out reports/tsgpt_run_b.csv\n```\n\n---\n\n## Instructor checklist (before class)\n\n* Ensure at least `log_return`, `r_1d`, and a few lags are present, or rely on fallback generation.\n* Dry‑run the tiny config on CPU or GPU; confirm val MAE decreases in <5 minutes.\n* Prepare one slide mapping “char embedding” → “feature projection”.\n\n## Emphasize while teaching\n\n* **Causality and alignment** are unchanged from char‑GPT; only the **input embedding** changes.\n* **Scaling on TRAIN only**; never refit on VAL/TEST.\n* Report at least **MAE and sMAPE**; add calibration by regime later if desired.\n\n## Grading (pass/revise)\n\n* `models/tsgpt_split1.pt` and `reports/tsgpt_split1_metrics.csv` exist.\n* Students ran at least **two ablations** and wrote `reports/tsgpt_ablation_summary.csv`.\n* Tests pass (`test_tsgpt_mask_and_shapes`).\n* Quarto report updated with numbers and the training curve.\n\nYou now have a **tiny GPT adapted to time series**, ready to be compared to your GRU/LSTM baselines and incorporated into the unified multi‑asset pipeline.\n\n",
    "supporting": [
      "lec22_files"
    ],
    "filters": [],
    "includes": {}
  }
}