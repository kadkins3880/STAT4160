{
  "hash": "deec0ebf11816aa2f9d8a912a489626f",
  "result": {
    "markdown": "---\ntitle: \"Session 13\"\n---\n\nBelow is a complete lecture package for **Session 13 — pytest + Data Validation** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. You’ll add **high‑value tests** around your features and a **Pandera** (optional) schema, practice **logging**, and wire everything so tests run fast and deterministically.\n\n> **Assumptions:** You completed Session 9–12 and have `data/processed/features_v1.parquet` (or `features_v1_ext.parquet`). If a file is missing, the lab provides a small synthetic fallback so tests still run.\n> **Goal today:** Make it **hard to ship bad data** by adding precise, fast tests.\n\n---\n\n## Session 13 — pytest + Data Validation (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Write **fast, high‑signal tests** for data pipelines (shapes, dtypes, nulls, **no look‑ahead**).\n2. Validate a DataFrame with **Pandera** (schema + value checks) or **custom checks** only.\n3. Use **logging** effectively and capture logs in tests.\n4. Run tests in Colab / locally and prepare for CI in Session 14.\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: What to test (and not), “data tests” vs unit tests, speed budget\n* **(10 min)** Slides: Pandera schemas & custom checks; tolerance and stability\n* **(10 min)** Slides: Logging basics (`logging`, levels, handlers); testing logs with `caplog`\n* **(35 min)** **In‑class lab**: add `tests/test_features.py` (+ optional Pandera test), fixtures, config; run & fix\n* **(10 min)** Wrap‑up + homework briefing\n\n---\n\n## Slides / talking points (drop into your deck)\n\n### What to test (fast, crisp)\n\n* **Contract tests** for data:\n\n  * **Schema**: required columns exist; dtypes sane (`ticker` categorical, calendar ints).\n  * **Nulls**: no NAs in training‑critical cols.\n  * **Semantics**: `r_1d` is **lead** of `log_return`; rolling features computed from **past only**.\n  * **Keys**: no duplicate `(ticker, date)`; dates strictly increasing within ticker.\n* Keep tests **under \\~5s total** (CI budget). Avoid long recomputations; sample/take head.\n\n### Pandera vs custom checks\n\n* **Pandera**: declarative schema; optional dependency; good for **column existence + ranges**.\n* **Custom**: essential for **domain logic** (look‑ahead bans, exact rolling formulas).\n\n### Logging basics\n\n* Use `logging.getLogger(__name__)`; set level via env (`LOGLEVEL=INFO`).\n* Log **counts, ranges, and any data drops** inside build scripts.\n* In tests: use `caplog` to assert a warning is emitted for suspicious conditions.\n\n---\n\n## In‑class lab (35 min)\n\n> Run each block as its **own Colab cell**. Adjust `REPO_NAME` as needed.\n\n### 0) Setup: mount & folders\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # <- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"tests\",\"scripts\",\"reports\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n```\n\n### 1) (Optional) Install test‑time helpers (Pandera)\n\n```python\n!pip -q install pytest pandera pyarrow\n```\n\n### 2) Put a tiny **logging helper** in your repo (used by build scripts & tests)\n\n```python\n# scripts/logsetup.py\nfrom __future__ import annotations\nimport logging, os\n\ndef setup_logging(name: str = \"dspt\"):\n    level = os.getenv(\"LOGLEVEL\", \"INFO\").upper()\n    logger = logging.getLogger(name)\n    if not logger.handlers:\n        handler = logging.StreamHandler()\n        fmt = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n        handler.setFormatter(logging.Formatter(fmt))\n        logger.addHandler(handler)\n    logger.setLevel(level)\n    return logger\n```\n\n### 3) Create **pytest config** and a fixture (with safe fallback data)\n\n```python\n# pytest.ini\nfrom pathlib import Path\nPath(\"pytest.ini\").write_text(\"\"\"[pytest]\naddopts = -q\ntestpaths = tests\nfilterwarnings =\n    ignore::FutureWarning\n\"\"\")\n\n# tests/conftest.py\nfrom pathlib import Path\nimport pandas as pd, numpy as np, pytest\n\ndef _synth_features():\n    # minimal synthetic features for 3 tickers, 60 days\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2023-01-02\", periods=60)\n    frames=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\"]:\n        ret = rng.normal(0, 0.01, size=len(dates)).astype(\"float32\")\n        adj = 100 * np.exp(np.cumsum(ret))\n        df = pd.DataFrame({\n            \"date\": dates,\n            \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        # next-day label\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        # rolling\n        df[\"roll_mean_20\"] = df[\"log_return\"].rolling(20, min_periods=20).mean()\n        df[\"roll_std_20\"]  = df[\"log_return\"].rolling(20, min_periods=20).std()\n        df[\"zscore_20\"]    = (df[\"log_return\"]-df[\"roll_mean_20\"])/(df[\"roll_std_20\"]+1e-8)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        frames.append(df)\n    out = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    out[\"ticker\"] = out[\"ticker\"].astype(\"category\")\n    return out\n\n@pytest.fixture(scope=\"session\")\ndef features_df():\n    p = Path(\"data/processed/features_v1.parquet\")\n    if p.exists():\n        df = pd.read_parquet(p)\n        # Ensure expected minimal cols exist (compute light ones if missing)\n        if \"weekday\" not in df: df[\"weekday\"] = pd.to_datetime(df[\"date\"]).dt.weekday.astype(\"int8\")\n        if \"month\" not in df:   df[\"month\"] = pd.to_datetime(df[\"date\"]).dt.month.astype(\"int8\")\n        return df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    # fallback\n    return _synth_features().sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n```\n\n### 4) **High‑value tests**: shapes, nulls, look‑ahead ban (as requested)\n\n```python\n# tests/test_features.py\nimport numpy as np, pandas as pd\nimport pytest\n\nREQUIRED_COLS = [\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]\n\ndef test_required_columns_present(features_df):\n    missing = [c for c in REQUIRED_COLS if c not in features_df.columns]\n    assert not missing, f\"Missing required columns: {missing}\"\n\ndef test_key_no_duplicates(features_df):\n    dup = features_df[[\"ticker\",\"date\"]].duplicated().sum()\n    assert dup == 0, f\"Found {dup} duplicate (ticker,date) rows\"\n\ndef test_sorted_within_ticker(features_df):\n    for tkr, g in features_df.groupby(\"ticker\"):\n        assert g[\"date\"].is_monotonic_increasing, f\"Dates not sorted for {tkr}\"\n\ndef test_nulls_in_critical_columns(features_df):\n    crit = [\"log_return\",\"r_1d\"]\n    na = features_df[crit].isna().sum().to_dict()\n    assert all(v == 0 for v in na.values()), f\"NAs in critical cols: {na}\"\n\ndef test_calendar_dtypes(features_df):\n    assert str(features_df[\"weekday\"].dtype) in (\"int8\",\"Int8\"), \"weekday should be compact int\"\n    assert str(features_df[\"month\"].dtype)   in (\"int8\",\"Int8\"), \"month should be compact int\"\n\ndef test_ticker_is_categorical(features_df):\n    # allow object if reading from some parquet engines, but prefer category\n    assert features_df[\"ticker\"].dtype.name in (\"category\",\"CategoricalDtype\",\"object\")\n\ndef test_r1d_is_lead_of_log_return(features_df):\n    for tkr, g in features_df.groupby(\"ticker\"):\n        # r_1d at t equals log_return at t+1\n        assert g[\"r_1d\"].iloc[:-1].equals(g[\"log_return\"].iloc[1:]), f\"Lead/lag mismatch for {tkr}\"\n\n@pytest.mark.parametrize(\"W\", [20])\ndef test_rolling_mean_matches_definition(features_df, W):\n    if f\"roll_mean_{W}\" not in features_df.columns:\n        pytest.skip(f\"roll_mean_{W} not present\")\n    for tkr, g in features_df.groupby(\"ticker\"):\n        s = g[\"log_return\"]\n        rm = s.rolling(W, min_periods=W).mean()\n        # compare only where defined\n        mask = ~rm.isna()\n        diff = (g[f\"roll_mean_{W}\"][mask] - rm[mask]).abs().max()\n        assert float(diff) <= 1e-7, f\"roll_mean_{W} mismatch for {tkr} (max diff {diff})\"\n```\n\n### 5) **Optional** Pandera schema test (declarative)\n\n```python\n# tests/test_schema_pandera.py\nimport pytest, pandas as pd, numpy as np\ntry:\n    import pandera as pa\n    from pandera import Column, Check, DataFrameSchema\nexcept Exception:\n    pytest.skip(\"pandera not installed\", allow_module_level=True)\n\nschema = pa.DataFrameSchema({\n    \"date\":    Column(pa.DateTime, nullable=False),\n    \"ticker\":  Column(pa.String, nullable=False, coerce=True, checks=Check.str_length(1, 12)),\n    \"log_return\": Column(pa.Float, nullable=False, checks=Check.is_finite()),\n    \"r_1d\":       Column(pa.Float, nullable=False, checks=Check.is_finite()),\n    \"weekday\":    Column(pa.Int8,  checks=Check.in_range(0, 6)),\n    \"month\":      Column(pa.Int8,  checks=Check.in_range(1, 12)),\n}, coerce=True, strict=False)\n\ndef test_schema_validate(features_df):\n    # Cast ticker to string for schema validation; categorical is ok → string\n    df = features_df.copy()\n    df[\"ticker\"] = df[\"ticker\"].astype(str)\n    schema.validate(df[[\"date\",\"ticker\",\"log_return\",\"r_1d\",\"weekday\",\"month\"]])\n```\n\n### 6) **Logging** test: assert a warning is emitted on duplicates (toy demo)\n\n```python\n# tests/test_logging.py\nimport logging, pandas as pd, numpy as np, pytest\nfrom scripts.logsetup import setup_logging\n\ndef check_for_duplicates(df, logger=None):\n    logger = logger or setup_logging(\"dspt\")\n    dups = df[[\"ticker\",\"date\"]].duplicated().sum()\n    if dups > 0:\n        logger.warning(\"Found %d duplicate (ticker,date) rows\", dups)\n    return dups\n\ndef test_duplicate_warning(caplog):\n    caplog.set_level(logging.WARNING)\n    df = pd.DataFrame({\"ticker\":[\"AAPL\",\"AAPL\"], \"date\":pd.to_datetime([\"2024-01-02\",\"2024-01-02\"])})\n    dups = check_for_duplicates(df)\n    assert dups == 1\n    assert any(\"duplicate\" in rec.message for rec in caplog.records)\n```\n\n### 7) Run tests now\n\n```python\n!pytest -q\n```\n\n> If a test fails on your real data, fix your pipeline (e.g., regenerate `features_v1.parquet`) and re‑run. **Do not** relax the test without understanding the failure.\n\n---\n\n## Wrap‑up (10 min)\n\n* You now have **tests that fail loudly** if labels leak, required columns/keys break, or schemas drift.\n* Pandera provides a declarative baseline; custom tests encode your **domain logic**.\n* Logging helps you **debug data issues**; you can assert on log messages in tests.\n\n---\n\n## Homework (due before Session 14)\n\n**Goal:** Create a **Health Check** notebook that prints key diagnostics and is easy to include in your Quarto report.\n\n### Part A — Build a reusable **health** module\n\n```python\n# scripts/health.py\nfrom __future__ import annotations\nimport pandas as pd, numpy as np, json\nfrom pathlib import Path\n\ndef df_health(df: pd.DataFrame) -> dict:\n    out = {}\n    out[\"rows\"] = int(len(df))\n    out[\"cols\"] = int(df.shape[1])\n    out[\"date_min\"] = str(pd.to_datetime(df[\"date\"]).min().date())\n    out[\"date_max\"] = str(pd.to_datetime(df[\"date\"]).max().date())\n    out[\"tickers\"]  = int(df[\"ticker\"].nunique())\n    # Null counts (top 10)\n    na = df.isna().sum().sort_values(ascending=False)\n    out[\"nulls\"] = na[na>0].head(10).to_dict()\n    # Duplicates\n    out[\"dup_key_rows\"] = int(df[[\"ticker\",\"date\"]].duplicated().sum())\n    # Example numeric ranges for core cols\n    for c in [x for x in [\"log_return\",\"r_1d\",\"roll_std_20\"] if x in df.columns]:\n        s = pd.to_numeric(df[c], errors=\"coerce\")\n        out[f\"{c}_min\"] = float(np.nanmin(s))\n        out[f\"{c}_max\"] = float(np.nanmax(s))\n    return out\n\ndef write_health_report(in_parquet=\"data/processed/features_v1.parquet\",\n                        out_json=\"reports/health.json\", out_md=\"reports/health.md\"):\n    p = Path(in_parquet)\n    if not p.exists():\n        raise SystemExit(f\"Missing {in_parquet}.\")\n    df = pd.read_parquet(p)\n    h = df_health(df)\n    Path(out_json).write_text(json.dumps(h, indent=2))\n    # Render a small Markdown summary\n    lines = [\n        \"# Data Health Summary\",\n        \"\",\n        f\"- Rows: **{h['rows']}**; Cols: **{h['cols']}**; Tickers: **{h['tickers']}**\",\n        f\"- Date range: **{h['date_min']} → {h['date_max']}**\",\n        f\"- Duplicate (ticker,date) rows: **{h['dup_key_rows']}**\",\n    ]\n    if h.get(\"nulls\"):\n        lines += [\"\", \"## Top Null Counts\", \"\"]\n        lines += [f\"- **{k}**: {v}\" for k,v in h[\"nulls\"].items()]\n    Path(out_md).write_text(\"\\n\".join(lines))\n    print(\"Wrote\", out_json, \"and\", out_md)\n```\n\nRun once to generate the files:\n\n```python\n!python scripts/health.py\n```\n\n### Part B — **Health Check notebook** (`reports/health.ipynb`)\n\nCreate a new notebook `reports/health.ipynb` with **two cells**:\n\n**Cell 1 (setup):**\n\n```python\n%load_ext autoreload\n%autoreload 2\nfrom scripts.health import write_health_report\nwrite_health_report()  # writes reports/health.json and reports/health.md\n```\n\n**Cell 2 (display in notebook):**\n\n```python\nfrom pathlib import Path\nprint(Path(\"reports/health.md\").read_text())\n```\n\n> Commit the notebook. It will be light and re‑usable. You’ll include its output in Quarto below.\n\n### Part C — Include health output in your **Quarto report**\n\nIn `reports/eda.qmd`, add a section:\n\n````markdown\n## Data Health (auto-generated)\n\n::: {.cell execution_count=1}\n````` {.python .cell-code}\nfrom pathlib import Path\nprint(Path(\"reports/health.md\").read_text())\n````\n\n````\n\nRender EDA:\n```bash\nquarto render reports/eda.qmd\n````\n\n### Part D — Add a **Makefile** target and a quick test\n\n**Makefile append:**\n\n```make\n.PHONY: health test\nhealth: ## Generate health.json and health.md from the current features parquet\n\\tpython scripts/health.py\n\ntest: ## Run fast tests\n\\tpytest -q\n`````\n:::\n\n\n**Test that health files exist:**\n\n```python\n# tests/test_health_outputs.py\nimport os, json\n\ndef test_health_files_exist():\n    assert os.path.exists(\"reports/health.json\")\n    assert os.path.exists(\"reports/health.md\")\n    # json is valid\n    import json\n    json.load(open(\"reports/health.json\"))\n```\n\nRun:\n\n```bash\n%%bash\nmake health\npytest -q -k health\n```\n\n---\n\n## Instructor checklist (before class)\n\n* Ensure `features_v1.parquet` exists or the fixture’s synthetic fallback works.\n* Dry‑run `pytest -q` in a fresh runtime; keep total time < 5s.\n* Prepare 2–3 “expected failures” you can toggle (e.g., edit one feature column to NaN) to show tests catching bugs.\n\n## Emphasize while teaching\n\n* **Fast tests only** for CI; keep heavy, long recomputations out.\n* **No look‑ahead** and **unique (ticker,date)** are non‑negotiable contracts.\n* Logging is a first‑class tool—tests can assert on **warnings** you emit.\n\n## Grading (pass/revise)\n\n* `tests/test_features.py` present with **shapes, nulls, look‑ahead ban** (and rolling check).\n* Tests pass locally (`pytest -q`).\n* `reports/health.ipynb` and `reports/health.md/.json` exist and integrate into `eda.qmd`.\n* Makefile `health` and `test` targets work.\n\nYou now have a **safety net** around your data. In **Session 14**, we’ll enforce style with **pre‑commit** and bring your tests to **GitHub Actions CI**.\n\n",
    "supporting": [
      "lec13_files\\figure-pdf"
    ],
    "filters": []
  }
}