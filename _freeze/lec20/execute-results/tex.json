{
  "hash": "8b1f1a49ed2fa1fb113bd1d38a0732b7",
  "result": {
    "markdown": "---\ntitle: \"Session 20 — Multi‑asset training (unified model\"\n---\n\nBelow is a complete lecture package for **Session 20 — Multi‑asset training (unified model)** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll add a **`ticker_id` embedding** (and optional **sector** embedding) to a sequence model so **one model** learns across **all tickers**.\n\n> **Educational use only — not trading advice.**\n> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) and availability of `data/processed/features_v1.parquet` (or `features_v1_static.parquet` from Session 17) with columns like `ticker`, `date`, `log_return`, `r_1d`, and some features (`lag1..lag3`, `roll_std_20`, `zscore_20`, …). Cells include **safe fallbacks** so you can run end‑to‑end.\n\n---\n\n## Session 20 — Multi‑asset training (unified model) (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Train **one unified model** across **many tickers** instead of one model per ticker.\n2. Add a **`ticker_id` embedding** (and optional **sector** embedding) and concatenate it to sequence inputs.\n3. Batch **mixed tickers** safely (respecting the same time‑based splits and embargo from Session 15).\n4. Log overall **validation metrics** and **per‑ticker** metrics for fair comparison later.\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: unified vs per‑asset models; why embeddings; pitfalls\n* **(10 min)** Slides: batching mixed tickers; leakage guardrails; embedding size heuristics\n* **(35 min)** **In‑class lab**: dataset that returns `ticker_id` → GRU with `nn.Embedding` → train/evaluate → write per‑ticker metrics\n* **(10 min)** Wrap‑up + homework brief\n* **(10 min)** Buffer / Q\\&A\n\n---\n\n## Slide talking points (add to your deck)\n\n### Why unified?\n\n* **Data efficiency:** share statistical strength across assets.\n* **Personalization:** learn **asset‑specific biases** via **ID embeddings** (and optional sector embeddings).\n* **Simplicity:** one checkpoint, easier hyperparam search.\n* **Trade‑off:** can **overfit** to IDs if embedding too large; must compare to **per‑ticker** baselines.\n\n### Embeddings in a regression model\n\n* Treat categorical IDs (ticker, sector) as learnable vectors.\n* Concatenate the embedding to **every time step** features:\n  $x'_t = [x_t \\;\\|\\; e_{\\text{ticker}} \\;\\|\\; e_{\\text{sector}}]$.\n* Heuristic sizes: $d_\\text{ticker} \\in [8, 16]$; $d_\\text{sector} \\in [4, 8]$. Start small.\n\n### Batching mixed tickers without leakage\n\n* **Splits** are **by date** (same as Session 15); **do not** fit scalers or thresholds on validation.\n* The ID mapping is just **indices**; it’s **not** a data leak.\n* Keep **train‑fit scaler**, reuse on val/test.\n\n---\n\n## In‑class lab (35 min, Colab‑friendly)\n\n> Run each block as its **own cell** in Colab. Replace `REPO_NAME` if needed.\n\n### 0) Setup & device\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # <- change to your repo name\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys, platform, random\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\nimport numpy as np, pandas as pd, torch\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n\ndef seed_everything(seed=2025):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\nseed_everything(2025)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n```\n\n### 1) Load features (with safe fallback) and define a split\n\n```python\nfrom pathlib import Path\n\nf_static = Path(\"data/processed/features_v1_static.parquet\")\nf_base   = Path(\"data/processed/features_v1.parquet\")\n\nif f_static.exists():\n    df = pd.read_parquet(f_static)\nelif f_base.exists():\n    df = pd.read_parquet(f_base)\nelse:\n    # Fallback from returns\n    rpath = Path(\"data/processed/returns.parquet\")\n    if not rpath.exists():\n        # synthesize a tiny dataset to keep class flowing\n        rng = np.random.default_rng(0)\n        dates = pd.bdate_range(\"2022-01-03\", periods=340)\n        frames=[]\n        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\",\"META\"]:\n            eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n            adj = 100*np.exp(np.cumsum(eps))\n            g = pd.DataFrame({\n                \"date\": dates, \"ticker\": t,\n                \"adj_close\": adj.astype(\"float32\"),\n                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n            })\n            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n            frames.append(g)\n        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n        df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n        df.to_parquet(\"data/processed/returns.parquet\", index=False)\n    else:\n        df = pd.read_parquet(rpath)\n        df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    # ensure minimal features\n    for k in [1,2,3]:\n        df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n    df[\"roll_std_20\"]  = df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n    df[\"zscore_20\"]    = (df[\"log_return\"] - df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).mean().reset_index(level=0, drop=True)) / (df[\"roll_std_20\"] + 1e-8)\n    df = df.dropna().reset_index(drop=True)\n\n# Harmonize and subset for speed\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf[\"ticker\"] = df[\"ticker\"].astype(\"category\")\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nkeep = df[\"ticker\"].cat.categories.tolist()[:10]  # up to 10 tickers for class\ndf = df[df[\"ticker\"].isin(keep)].copy()\n\n# Choose features\nCAND = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\nFEATS = [c for c in CAND if c in df.columns]\nassert \"r_1d\" in df.columns and FEATS, \"Missing required columns.\"\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i=train_min-1; out=[]\n    while True:\n        if i>=len(u): break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\nsplits = make_splits(df[\"date\"], 252, 63, 63, 5)\nassert splits, \"Not enough history for split 1.\"\na,b,c,d = splits[0]\ntrain_df = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)].copy()\nval_df   = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)].copy()\nprint(\"Split1 Train:\", a.date(), \"→\", b.date(), \"| Val:\", c.date(), \"→\", d.date(),\n      \"| tickers train:\", train_df[\"ticker\"].nunique(), \"val:\", val_df[\"ticker\"].nunique())\n```\n\n### 2) Dataset with **ticker IDs** (+ optional sector)\n\n```python\nimport json\nfrom torch.utils.data import Dataset, DataLoader\n\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0, dtype=np.float64); self.std_=X.std(0, dtype=np.float64)+1e-8; return self\n    def transform(self, X): return (X - self.mean_) / self.std_\n    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\n\ndef build_sector_map(frame: pd.DataFrame):\n    # Optional: if a 'sector' column exists (from Session 12 scrape), use it; else all 'UNKNOWN'.\n    if \"sector\" in frame.columns:\n        sec = frame[[\"ticker\",\"sector\"]].drop_duplicates().copy()\n    else:\n        sec = frame[[\"ticker\"]].drop_duplicates().copy()\n        sec[\"sector\"] = \"UNKNOWN\"\n    sec[\"sector\"] = sec[\"sector\"].astype(\"category\")\n    return dict(zip(sec[\"ticker\"].astype(str), sec[\"sector\"].cat.codes.astype(int))), len(sec[\"sector\"].cat.categories)\n\nclass WindowedDatasetXID(Dataset):\n    \"\"\"\n    Sliding windows per ticker with ID features.\n    Returns: X_scaled (T,F), y (scalar), ticker_id (long), sector_id (long)\n    \"\"\"\n    def __init__(self, frame: pd.DataFrame, feature_cols, context_len=64,\n                 scaler: FeatureScaler|None=None,\n                 ticker2id: dict|None=None, sector2id: dict|None=None, n_sectors: int|None=None):\n        assert {\"ticker\",\"date\",\"r_1d\"}.issubset(frame.columns)\n        self.feature_cols = list(feature_cols); self.T = int(context_len)\n        self.groups, self.index = {}, []\n\n        # Build mappings on TRAIN; reuse on VAL\n        if ticker2id is None:\n            cats = frame[\"ticker\"].astype(\"category\").cat.categories.tolist()\n            self.ticker2id = {t:i for i,t in enumerate(cats)}\n        else:\n            self.ticker2id = dict(ticker2id)\n        sec_map, nsec = build_sector_map(frame)\n        if sector2id is None:\n            # Freeze sector ids according to sec_map order\n            uniq_secs = sorted(set(sec_map.values()))\n            self.sector2id = {s:i for i,s in enumerate(uniq_secs)}\n            self.n_sectors = len(self.sector2id)\n        else:\n            self.sector2id = dict(sector2id)\n            self.n_sectors = int(n_sectors)\n\n        # Build per-ticker arrays and global window index\n        for tkr, g in frame.groupby(\"ticker\"):\n            g = g.sort_values(\"date\").reset_index(drop=True)\n            X = g[self.feature_cols].to_numpy(\"float32\")\n            y = g[\"r_1d\"].to_numpy(\"float32\")\n            t_id = self.ticker2id[str(tkr)]\n            s_id = self.sector2id.get(build_sector_map(g)[0][str(tkr)], 0) if \"sector\" in g else 0\n            # Valid windows\n            for end in range(self.T-1, len(g)):\n                if np.isfinite(y[end]):\n                    self.index.append((str(tkr), end, t_id, s_id))\n            self.groups[str(tkr)] = {\"X\": X, \"y\": y}\n\n        # Fit or reuse scaler\n        self.scaler = scaler or FeatureScaler().fit(\n            np.concatenate([self.groups[t][\"X\"] for t in self.groups], axis=0)\n        )\n\n    def __len__(self): return len(self.index)\n\n    def __getitem__(self, i):\n        tkr, end, t_id, s_id = self.index[i]\n        g = self.groups[tkr]\n        xs = g[\"X\"][end-self.T+1:end+1]\n        xs = self.scaler.transform(xs)\n        y  = g[\"y\"][end]\n        return (torch.from_numpy(xs), torch.tensor(y, dtype=torch.float32),\n                torch.tensor(t_id, dtype=torch.long), torch.tensor(s_id, dtype=torch.long))\n\n# Build TRAIN/VAL datasets & loaders (reusing train-fitted scaler and ID maps)\nT = 64; BATCH=256; WORKERS=2; PIN=torch.cuda.is_available()\n\ntrain_ds = WindowedDatasetXID(train_df, FEATS, context_len=T)\nval_ds   = WindowedDatasetXID(val_df,   FEATS, context_len=T,\n                              scaler=train_ds.scaler,\n                              ticker2id=train_ds.ticker2id,\n                              sector2id=train_ds.sector2id,\n                              n_sectors=train_ds.n_sectors)\n\nfrom torch.utils.data import DataLoader\ndef _seed_worker(_):\n    ws = torch.initial_seed() % (2**32)\n    np.random.seed(ws); random.seed(ws)\n\ng = torch.Generator(); g.manual_seed(42)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),\n                          worker_init_fn=_seed_worker, generator=g)\nval_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, drop_last=False,\n                          num_workers=WORKERS, pin_memory=PIN, persistent_workers=(WORKERS>0),\n                          worker_init_fn=_seed_worker)\n\nlen(train_ds), len(val_ds), next(iter(train_loader))[0].shape, len(train_ds.ticker2id)\n```\n\n### 3) **Unified GRU** with `ticker_id` embedding (optional sector)\n\n```python\nimport torch.nn as nn, torch\nfrom torch.cuda.amp import autocast, GradScaler\n\nclass UnifiedGRUWithID(nn.Module):\n    def __init__(self, in_features: int, n_tickers: int, d_ticker: int = 12,\n                 n_sectors: int | None = None, d_sector: int = 0,\n                 hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.tok = nn.Embedding(n_tickers, d_ticker)\n        self.sec = nn.Embedding(n_sectors, d_sector) if (n_sectors and d_sector>0) else None\n        augmented_in = in_features + d_ticker + (d_sector if self.sec else 0)\n        self.gru = nn.GRU(input_size=augmented_in, hidden_size=hidden, num_layers=num_layers,\n                          batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden, 1))\n\n    def forward(self, x, ticker_ids, sector_ids=None):\n        # x: (B,T,F)\n        e = self.tok(ticker_ids)                          # (B, d_ticker)\n        if self.sec is not None and sector_ids is not None:\n            e = torch.cat([e, self.sec(sector_ids)], dim=-1)  # (B, d_ticker+d_sector)\n        e = e.unsqueeze(1).expand(-1, x.size(1), -1)      # repeat across time\n        x_aug = torch.cat([x, e], dim=-1)                 # (B,T,F+E)\n        _, hN = self.gru(x_aug)\n        h = hN[-1]                                        # (B, H)\n        return self.head(h).squeeze(-1)\n\ndef make_model():\n    return UnifiedGRUWithID(in_features=len(FEATS),\n                            n_tickers=len(train_ds.ticker2id),\n                            d_ticker=12,\n                            n_sectors=val_ds.n_sectors, d_sector=0,  # set >0 if you have sector\n                            hidden=64, num_layers=2, dropout=0.1)\n\nmodel = make_model().to(device)\nsum(p.numel() for p in model.parameters())/1e6, device\n```\n\n### 4) Training loop (AMP + early stopping) and evaluation (**per‑ticker** metrics)\n\n```python\nfrom torch.optim import AdamW\nimport time, math\n\ndef mae_t(y, yhat): return torch.mean(torch.abs(y - yhat))\ndef smape_t(y, yhat, eps=1e-8): return torch.mean(2*torch.abs(y - yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\ndef train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):\n    model.train(); tot=0.0; n=0\n    for xb, yb, tid, sid in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        tid = tid.to(device, non_blocking=True)\n        sid = sid.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if use_amp and device.type==\"cuda\":\n            with autocast(dtype=torch.float16):\n                pred = model(xb, tid, sid)\n                loss = mae_t(yb, pred)\n            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n        else:\n            pred = model(xb, tid, sid); loss = mae_t(yb, pred); loss.backward(); optimizer.step()\n        bs = xb.size(0); tot += loss.item()*bs; n += bs\n    return tot/max(n,1)\n\n@torch.no_grad()\ndef evaluate(model, loader, device, return_preds=False):\n    model.eval(); t_mae=t_smape=0.0; n=0\n    all_rows=[]\n    for xb, yb, tid, sid in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        tid = tid.to(device, non_blocking=True)\n        sid = sid.to(device, non_blocking=True)\n        pred = model(xb, tid, sid)\n        bs = xb.size(0); t_mae += mae_t(yb, pred).item()*bs; t_smape += smape_t(yb, pred).item()*bs; n+=bs\n        if return_preds:\n            all_rows.append((yb.detach().cpu().numpy(), pred.detach().cpu().numpy(), tid.detach().cpu().numpy()))\n    out = {\"mae\": t_mae/max(n,1), \"smape\": t_smape/max(n,1)}\n    if return_preds:\n        ys = np.concatenate([r[0] for r in all_rows]); yh = np.concatenate([r[1] for r in all_rows]); tids = np.concatenate([r[2] for r in all_rows])\n        out[\"y_true\"] = ys; out[\"y_pred\"] = yh; out[\"ticker_id\"] = tids\n    return out\n\ndef fit_unified(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    scaler = GradScaler(enabled=(use_amp and device.type==\"cuda\"))\n    best=math.inf; best_ep=-1; ckpt = Path(\"models/unified_gru_split1.pt\")\n    hist=[]\n    for ep in range(1, epochs+1):\n        t0=time.time()\n        tr = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n        val = evaluate(model, val_loader, device)\n        dt=time.time()-t0\n        hist.append({\"epoch\":ep,\"train_mae\":tr,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"],\"sec\":dt})\n        print(f\"Epoch {ep:02d}  train_mae={tr:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n        if val[\"mae\"] < best - 1e-6:\n            best = val[\"mae\"]; best_ep=ep\n            torch.save({\"model\": model.state_dict(),\n                        \"epoch\": ep,\n                        \"config\": {\"feats\": FEATS, \"T\": T, \"d_ticker\": 12}}, ckpt)\n        elif ep - best_ep >= patience:\n            print(f\"Early stopping at epoch {ep} (best {best:.5f} @ {best_ep})\"); break\n    return hist, best, best_ep, ckpt\n\nmodel = make_model().to(device)\nhist, best, best_ep, ckpt = fit_unified(model, train_loader, val_loader, epochs=10, lr=1e-3, wd=1e-5, patience=3, use_amp=True)\nprint(\"Best val_mae:\", best, \"| epoch:\", best_ep, \"| saved:\", ckpt.exists())\n```\n\n#### Per‑ticker metrics & CSVs\n\n```python\n# Reload best and compute per-ticker metrics\nckpt = torch.load(\"models/unified_gru_split1.pt\", map_location=device)\nmodel.load_state_dict(ckpt[\"model\"]); model.to(device)\n\n# Evaluate with predictions and map back to ticker symbols\nres = evaluate(model, val_loader, device, return_preds=True)\nid2ticker = {v:k for k,v in train_ds.ticker2id.items()}\npt_rows=[]\nfor tid in np.unique(res[\"ticker_id\"]):\n    m = res[\"ticker_id\"]==tid\n    y = res[\"y_true\"][m]; yhat = res[\"y_pred\"][m]\n    if len(y)==0: continue\n    pt_rows.append({\"ticker\": id2ticker[int(tid)], \"n\": int(len(y)),\n                    \"mae\": float(np.mean(np.abs(y - yhat))),\n                    \"smape\": float(np.mean(2*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+1e-8)))})\nper_ticker = pd.DataFrame(pt_rows).sort_values(\"mae\")\nper_ticker.to_csv(\"reports/unified_gru_split1_per_ticker.csv\", index=False)\n\n# Overall micro metrics (pooled)\noverall = pd.DataFrame([{\n    \"split\": 1, \"model\": \"unified_gru_id\", \"context\": T, \"feats\": \",\".join(FEATS),\n    \"val_mae\": float(np.mean(np.abs(res[\"y_true\"] - res[\"y_pred\"]))),\n    \"val_smape\": float(np.mean(2*np.abs(res[\"y_true\"] - res[\"y_pred\"])/(np.abs(res[\"y_true\"])+np.abs(res[\"y_pred\"])+1e-8))),\n    \"best_epoch\": ckpt.get(\"epoch\", None),\n    \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3)\n}])\noverall.to_csv(\"reports/unified_gru_split1_metrics.csv\", index=False)\n(per_ticker.head(), overall)\n```\n\n> **Time check:** With \\~8–10 tickers, `T=64`, and 10 epochs, this should finish in a couple of minutes on Colab CPU; faster on GPU.\n\n---\n\n## Wrap‑up (10 min) — key points\n\n* **Unified** models share information across assets and adapt via **ID embeddings**.\n* Embedding dims should be **small** (8–16) to avoid overfitting; they act like **bias + style** vectors.\n* Keep the **same splits** and **train‑fit scaler** to avoid leakage.\n* Always log **per‑ticker** metrics to check that no specific asset collapses.\n\n---\n\n## Homework (due before Session 21)\n\n**Goal:** Compare **unified (ID‑augmented)** vs **per‑ticker** models on **split 1**. Produce a single table `reports/unified_vs_per_ticker_split1.csv` and a short paragraph in your Quarto report.\n\n### Part A — Train per‑ticker GRU baselines and compare\n\nCreate **`scripts/compare_unified_vs_per_ticker.py`**:\n\n```python\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport numpy as np, pandas as pd, torch, torch.nn as nn\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\n\n# ---- Utilities reused from class ----\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n    while True:\n        if i>=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0); self.std_=X.std(0)+1e-8; return self\n    def transform(self, X): return (X-self.mean_)/self.std_\n\nclass WindowedDataset(Dataset):\n    def __init__(self, df, feats, T=64, scaler=None):\n        self.feats=feats; self.T=T; self.idx=[]; self.X=None; self.y=None\n        g=df.sort_values(\"date\").reset_index(drop=True)\n        self.X = g[feats].to_numpy(\"float32\"); self.y = g[\"r_1d\"].to_numpy(\"float32\")\n        for end in range(T-1, len(g)):\n            if np.isfinite(self.y[end]): self.idx.append(end)\n        self.scaler = scaler or FeatureScaler().fit(self.X)\n    def __len__(self): return len(self.idx)\n    def __getitem__(self,i):\n        end=self.idx[i]; X=self.scaler.transform(self.X[end-self.T+1:end+1])\n        return torch.from_numpy(X), torch.tensor(self.y[end], dtype=torch.float32)\n\nclass GRUReg(nn.Module):\n    def __init__(self, in_f, h=64, L=2, p=0.1):\n        super().__init__()\n        self.gru = nn.GRU(in_f, h, num_layers=L, batch_first=True, dropout=p if L>1 else 0.)\n        self.head= nn.Sequential(nn.Linear(h,h), nn.ReLU(), nn.Dropout(p), nn.Linear(h,1))\n    def forward(self, x):\n        _,hN = self.gru(x); return self.head(hN[-1]).squeeze(-1)\n\ndef mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\ndef smape(y,yhat,eps=1e-8):\n    y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef train_eval_one_ticker(df_t, df_v, feats, T=64, epochs=10, lr=1e-3, batch=128, device=\"cpu\"):\n    tr_ds = WindowedDataset(df_t, feats, T=T); va_ds = WindowedDataset(df_v, feats, T=T, scaler=tr_ds.scaler)\n    tr_ld = DataLoader(tr_ds, batch_size=batch, shuffle=True, drop_last=True)\n    va_ld = DataLoader(va_ds, batch_size=batch, shuffle=False)\n    net = GRUReg(len(feats)).to(device); opt = AdamW(net.parameters(), lr=lr, weight_decay=1e-5)\n    scaler = GradScaler(enabled=(device==\"cuda\"))\n    best=1e9\n    for ep in range(1, epochs+1):\n        net.train()\n        for xb,yb in tr_ld:\n            xb=xb.to(device).float(); yb=yb.to(device).float()\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device==\"cuda\"), dtype=torch.float16):\n                yhat = net(xb); loss = torch.mean(torch.abs(yb - yhat))\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n        # quick val\n        net.eval(); preds=[]; ys=[]\n        with torch.no_grad():\n            for xb,yb in va_ld:\n                xb=xb.to(device).float(); yb=yb.to(device).float()\n                preds.append(net(xb).cpu().numpy()); ys.append(yb.cpu().numpy())\n        y = np.concatenate(ys); yhat = np.concatenate(preds)\n        best = min(best, mae(y,yhat))\n    return best, smape(y,yhat)\n\ndef main():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    f_static = Path(\"data/processed/features_v1_static.parquet\")\n    df = pd.read_parquet(f_static if f_static.exists() else \"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    CAND = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\n    feats = [c for c in CAND if c in df.columns]; assert \"r_1d\" in df.columns\n    splits = make_splits(df[\"date\"]); a,b,c,d = splits[0]\n    tr_all = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)]; va_all = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)]\n    tickers = tr_all[\"ticker\"].astype(str).unique().tolist()\n    rows=[]\n    for t in tickers:\n        tr = tr_all[tr_all[\"ticker\"]==t]; va = va_all[va_all[\"ticker\"]==t]\n        if len(tr)<100 or len(va)<20: continue\n        vmae, vsmape = train_eval_one_ticker(tr, va, feats, T=64, epochs=10, device=device)\n        rows.append({\"ticker\":t, \"model\":\"per_ticker_gru\", \"val_mae\":vmae, \"val_smape\":vsmape})\n    per_ticker = pd.DataFrame(rows)\n    # Load unified results written in class\n    uni_pt = pd.read_csv(\"reports/unified_gru_split1_per_ticker.csv\").rename(columns={\"mae\":\"val_mae\",\"smape\":\"val_smape\"})\n    uni_pt[\"model\"] = \"unified_gru_id\"\n    # Join and compare\n    comp = per_ticker.merge(uni_pt[[\"ticker\",\"model\",\"val_mae\",\"val_smape\"]], on=\"ticker\", how=\"outer\", suffixes=(\"_per\",\"_uni\"))\n    comp.to_csv(\"reports/unified_vs_per_ticker_split1.csv\", index=False)\n    print(\"Wrote reports/unified_vs_per_ticker_split1.csv\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun:\n\n```bash\n%%bash\nchmod +x scripts/compare_unified_vs_per_ticker.py\npython scripts/compare_unified_vs_per_ticker.py\n```\n\n### Part B — Add a Makefile target and a minimal test\n\nAppend to **`Makefile`**:\n\n```make\n.PHONY: train-unified compare-unified\ntrain-unified: ## Train unified GRU with ticker embeddings on split 1\n\\tpython - <<'PY'\nfrom pathlib import Path\nimport runpy\nrunpy.run_module('scripts.__init__', run_name='__main__') if Path('scripts/__init__.py').exists() else None\nPY\n\ncompare-unified: ## Compare unified vs per-ticker baselines\n\\tpython scripts/compare_unified_vs_per_ticker.py\n```\n\n*(If you prefer, replace `train-unified` with a thin wrapper to re-run the in‑class cells as a script.)*\n\n**Test:** ensure per‑ticker comparison file exists and has required columns.\n\n```python\n# tests/test_unified_outputs.py\nimport os, pandas as pd\n\ndef test_unified_vs_per_ticker_table():\n    assert os.path.exists(\"reports/unified_vs_per_ticker_split1.csv\")\n    df = pd.read_csv(\"reports/unified_vs_per_ticker_split1.csv\")\n    need = {\"ticker\",\"val_mae_per\",\"val_mae_uni\"}\n    assert need.issubset(df.columns)\n```\n\nRun:\n\n```bash\n%%bash\npytest -q -k unified_outputs\n```\n\n### Part C — Add a short paragraph + table to your Quarto report\n\nIn `reports/eda.qmd` (or a new `reports/unified.qmd`), add:\n\n````markdown\n## Unified vs Per‑Ticker\n\nWe trained a **unified GRU with ticker embeddings** and compared it to **per‑ticker GRUs** on Split 1.\n\n::: {.cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\npd.read_csv(\"reports/unified_vs_per_ticker_split1.csv\").head(10)\n````\n\n**Note.** Unified models can outperform when data per asset is limited, but may underperform on idiosyncratic assets. Keep the embedding **small** to limit overfitting.\n`````\n:::\n\n\n---\n\n## Instructor notes / gotchas\n\n- **Embedding dimension**: start small (8–16). Larger dims can overfit and memorize IDs.  \n- **Scaler**: Fit on TRAIN only; reuse on VAL. It prevents target leakage through normalization.  \n- **Per‑ticker metrics**: Always report them to avoid hiding a few failing assets inside a good aggregate.  \n- **Sector embedding** (optional): If you have `sector` from Session 12, set `d_sector>0` in the model and pass `sector_ids` from the dataset.\n\n---\n\n## Grading (pass/revise)\n\n- Unified model with **ticker embeddings** trains and writes:\n  - `models/unified_gru_split1.pt`  \n  - `reports/unified_gru_split1_metrics.csv` (overall)  \n  - `reports/unified_gru_split1_per_ticker.csv` (per‑ticker)  \n- Comparison table `reports/unified_vs_per_ticker_split1.csv` exists.  \n- Quarto report updated with the comparison and a short discussion.\n\n---\n\n### Optional extensions (if students finish early)\n\n- **Ablate** embedding size: d=0 (no ID) vs 8 vs 16.  \n- Add **dropout** on embeddings or **L2** weight decay on embedding parameters only.  \n- Try **concatenating embeddings at the head** (after GRU) instead of at input, and compare.  \n- Try **sector embeddings** (`d_sector=4`) if you have sector metadata.  \n\nYou now have a single **multi‑asset** model with **tickers encoded as embeddings**—the foundation for Session 21, where you’ll implement attention and a **tiny GPT** on toy data before adapting it to time series.\n```\n\n",
    "supporting": [
      "lec20_files\\figure-pdf"
    ],
    "filters": []
  }
}