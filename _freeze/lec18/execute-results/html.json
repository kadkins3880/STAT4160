{
  "hash": "4d986471f86b9f73310eb60a7090746d",
  "result": {
    "markdown": "---\ntitle: \"Session 18 — Walk‑forward + Regime Analysi\"\n---\n\nBelow is a complete lecture package for **Session 18 — Walk‑forward + Regime Analysis** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. You’ll add **volatility regimes** to your rolling‑origin evaluation (with embargo), compute **metrics by regime**, and produce **calibration plots** that reveal where baselines over/under‑predict.\n\n> **Educational use only — not trading advice.**\n> Assumes your repo in Drive (e.g., `unified-stocks-teamX`) with `data/processed/returns.parquet` and `data/processed/features_v1.parquet`. If missing, the lab will synthesize a small fallback so you can run end‑to‑end.\n\n---\n\n## Session 18 — Walk‑forward + Regime Analysis (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Use **embargoed** rolling‑origin splits (Session 15) and apply a **static universe** (Session 17) consistently.\n2. Construct **volatility regimes** (low/med/high) from **rolling volatility** computed **causally** (≤ t), and set regime thresholds **using training‑only** data per split.\n3. Evaluate **MAE, sMAPE, MASE** **by regime**, with macro and micro aggregation.\n4. Make **calibration plots** (binned predicted vs. realized returns) **by regime** and interpret them.\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: walk‑forward recap (expanding vs sliding), embargo; regime intuition\n* **(10 min)** Slides: defining regimes (rolling std), training‑only thresholds, leakage pitfalls\n* **(35 min)** **In‑class lab**: add regime labels (train‑only quantiles) → evaluate naive & linear‑lags **by regime** → calibration plots\n* **(10 min)** Wrap‑up + homework brief\n* **(10 min)** Buffer / Q\\&A\n\n---\n\n## Slide talking points (paste into your deck)\n\n### Why regime analysis?\n\n* Model error is **not uniform**. Many models fail during **high‑volatility** periods.\n* Reporting **one global metric** hides when/where models break.\n* Regime‑aware metrics guide **feature/model design** and **risk controls**.\n\n### Splits & embargo refresher\n\n* **Rolling‑origin, expanding**: train grows, validation moves forward.\n* **Embargo**: gap (e.g., 5 business days) between train end and val start to reduce adjacency leakage.\n\n### Defining volatility regimes (avoid leakage)\n\n* Use **rolling standard deviation** of returns (e.g., `roll_std_20`) computed **up to and including t**.\n* **Thresholds**: choose quantiles (e.g., 33% and 66%) **on TRAIN ONLY** for each split; label both train & val using those fixed thresholds.\n* **Categories**: `low`, `med`, `high`. Treat labels as **categorical dtypes**.\n\n### Metrics & calibration by regime\n\n* Compute **MAE, sMAPE, MASE** **within each regime**. Aggregate macro/micro.\n* **Calibration (point forecasts)**: bin predictions into deciles; plot **mean predicted vs. mean realized** per bin.\n\n  * Perfect calibration ⇒ points on the 45° line.\n  * Plot one figure **overall** and one **per regime**.\n\n---\n\n## In‑class lab (35 min, Colab‑friendly)\n\n> Run each block as its **own cell**. Adjust `REPO_NAME` to your repo name.\n\n### 0) Setup & load (with safe fallbacks)\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # <- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd, json\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"data/static\",\"reports\",\"scripts\",\"tests\",\"docs/figs\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns; synthesize if missing\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n    frames=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n        eps = rng.normal(0,0.012,size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates, \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        frames.append(df)\n    returns = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Load features or generate minimal set with rolling std (causal)\nfpath = Path(\"data/processed/features_v1.parquet\")\nif fpath.exists():\n    feats = pd.read_parquet(fpath)\n    if \"roll_std_20\" not in feats.columns:\n        # ensure we have rolling volatility\n        feats = feats.sort_values([\"ticker\",\"date\"])\n        feats[\"roll_std_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\nelse:\n    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    feats[\"roll_std_20\"] = feats.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n\n# If static universe exists from Session 17, apply it\nuniv_files = sorted(Path(\"data/static\").glob(\"universe_*.csv\"))\nif univ_files:\n    univ = pd.read_csv(univ_files[-1])[\"ticker\"].astype(str)\n    feats = feats[feats[\"ticker\"].astype(str).isin(set(univ))]\n    returns = returns[returns[\"ticker\"].astype(str).isin(set(univ))]\n\n# Harmonize types & sort\nfor df in (returns, feats):\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\nfeats = feats.dropna(subset=[\"log_return\"]).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nreturns = returns.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfeats.head(3)\n```\n\n### 1) Rolling‑origin splits (expanding) with embargo\n\n```python\nimport numpy as np, pandas as pd\n\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i=train_min-1; n=len(u)\n    while True:\n        if i>=n: break\n        a,b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve>=n: break\n        splits.append((a,b,u[vs],u[ve]))\n        i += step\n    return splits\n\nsplits = make_rolling_origin_splits(feats[\"date\"], train_min=252, val_size=63, step=63, embargo=5)\nprint(\"Num splits:\", len(splits))\nsplits[:2]\n```\n\n### 2) Regime thresholds from **training‑only** (quantiles of rolling vol)\n\n```python\ndef regime_thresholds(train_df, vol_col=\"roll_std_20\", q_low=0.33, q_high=0.66):\n    v = train_df[vol_col].dropna().to_numpy()\n    if len(v) < 100:  # defensive: small train\n        q_low, q_high = 0.4, 0.8\n    return float(np.quantile(v, q_low)), float(np.quantile(v, q_high))\n\ndef label_regime(df, vol_col, lo, hi):\n    # low: <= lo, high: >= hi, else med; NaNs -> 'unknown'\n    out = df.copy()\n    vc = out[vol_col]\n    regime = pd.Series(pd.Categorical([\"unknown\"]*len(out), categories=[\"low\",\"med\",\"high\",\"unknown\"]), index=out.index)\n    regime[(vc.notna()) & (vc <= lo)] = \"low\"\n    regime[(vc.notna()) & (vc > lo) & (vc < hi)] = \"med\"\n    regime[(vc.notna()) & (vc >= hi)] = \"high\"\n    out[\"regime\"] = regime.astype(\"category\")\n    return out\n\n# Demonstrate on first split in class\na,b,c,d = splits[0]\ntr = feats[(feats[\"date\"]>=a) & (feats[\"date\"]<=b)]\nva = feats[(feats[\"date\"]>=c) & (feats[\"date\"]<=d)]\nlo, hi = regime_thresholds(tr, \"roll_std_20\", 0.33, 0.66)\ntr_lab = label_regime(tr, \"roll_std_20\", lo, hi)\nva_lab = label_regime(va, \"roll_std_20\", lo, hi)\nprint({\"lo\": lo, \"hi\": hi}, tr_lab[\"regime\"].value_counts().to_dict(), va_lab[\"regime\"].value_counts().to_dict())\n```\n\n### 3) Baseline predictions (naive & linear‑lags per ticker, fit on TRAIN only)\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# features we will use for linear baseline\nXCOLS = [c for c in [\"lag1\",\"lag2\",\"lag3\"] if c in feats.columns]\nif not XCOLS:\n    # create lags on the fly (causal)\n    feats = feats.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    XCOLS = [\"lag1\",\"lag2\",\"lag3\"]\n\ndef fit_predict_lin_per_ticker(train_df, val_df):\n    preds=[]\n    for tkr, trk in train_df.groupby(\"ticker\"):\n        vak = val_df[val_df[\"ticker\"]==tkr]\n        if len(trk)==0 or len(vak)==0: continue\n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(trk[XCOLS].dropna().values, trk.dropna(subset=XCOLS)[\"r_1d\"].values)\n        yhat = pipe.predict(vak[XCOLS].fillna(0).values)\n        out = vak[[\"date\",\"ticker\",\"r_1d\",\"log_return\",\"regime\"]].copy()\n        out[\"yhat_lin\"] = yhat.astype(\"float32\")\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()\n\ndef add_naive_preds(df):\n    out = df.copy()\n    out[\"yhat_naive\"] = out[\"log_return\"]  # r_{t+1} ~ log_return_t\n    return out\n\ntr_lab2 = add_naive_preds(tr_lab)\nva_lab2 = add_naive_preds(va_lab)\nval_lin = fit_predict_lin_per_ticker(tr_lab2, va_lab2)\nval = va_lab2.merge(val_lin[[\"date\",\"ticker\",\"yhat_lin\"]], on=[\"date\",\"ticker\"], how=\"left\")\nval.head(3)\n```\n\n### 4) Metrics **by regime** (MAE, sMAPE, MASE; macro & micro)\n\n```python\ndef mae(y, yhat): \n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(np.abs(y - yhat)))\n\ndef smape(y,yhat,eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2.0*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    scale = mae(y_train_true, y_train_naive) + 1e-12\n    return float(mae(y_true,y_pred)/scale)\n\ndef per_regime_metrics(val_df, train_df, pred_col):\n    rows=[]\n    for reg, g in val_df.groupby(\"regime\"):\n        if reg == \"unknown\" or len(g)==0: \n            continue\n        # build per-ticker MASE scales from TRAIN\n        per_t = []\n        for tkr, gv in g.groupby(\"ticker\"):\n            gt = train_df[train_df[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n            if len(gt)==0: continue\n            m = {\n                \"ticker\": tkr,\n                \"n\": int(gv[\"r_1d\"].notna().sum()),\n                \"mae\": mae(gv[\"r_1d\"], gv[pred_col]),\n                \"smape\": smape(gv[\"r_1d\"], gv[pred_col]),\n                \"mase\": mase(gv[\"r_1d\"], gv[pred_col], gt[\"r_1d\"], gt[\"log_return\"]),\n                \"regime\": reg\n            }\n            per_t.append(m)\n        per_t = pd.DataFrame(per_t)\n        if per_t.empty: \n            continue\n        # macro (mean of per-ticker)\n        macro = per_t[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n        # micro (weighted by n)\n        w = per_t[\"n\"].to_numpy()\n        micro = {\n            \"micro_mae\": float(np.average(per_t[\"mae\"], weights=w)),\n            \"micro_smape\": float(np.average(per_t[\"smape\"], weights=w)),\n            \"micro_mase\": float(np.average(per_t[\"mase\"], weights=w)),\n        }\n        rows.append({\"regime\":reg, **{f\"macro_{k}\":float(v) for k,v in macro.items()}, **micro})\n    return pd.DataFrame(rows)\n\nmet_naive = per_regime_metrics(val, tr_lab2, \"yhat_naive\")\nmet_lin   = per_regime_metrics(val.dropna(subset=[\"yhat_lin\"]), tr_lab2, \"yhat_lin\")\nprint(\"NAIVE by regime:\\n\", met_naive)\nprint(\"\\nLIN-LAGS by regime:\\n\", met_lin)\n# Save\npd.concat([\n    met_naive.assign(model=\"naive\"),\n    met_lin.assign(model=\"lin_lags\")\n], ignore_index=True).to_csv(\"reports/regime_metrics_split1.csv\", index=False)\n```\n\n### 5) **Calibration plots** overall and by regime (binned)\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd, pathlib\n\ndef calibration_by_bins(df, pred_col, y_col=\"r_1d\", n_bins=10):\n    d = df.dropna(subset=[pred_col, y_col]).copy()\n    d[\"bin\"] = pd.qcut(d[pred_col], q=n_bins, duplicates=\"drop\")\n    grp = d.groupby(\"bin\").agg(\n        mean_pred=(pred_col, \"mean\"),\n        mean_true=(y_col, \"mean\"),\n        count=(y_col, \"size\")\n    ).reset_index()\n    return grp\n\n# Overall calibration (lin_lags) on validation slice\ncal_overall = calibration_by_bins(val.dropna(subset=[\"yhat_lin\"]), \"yhat_lin\", \"r_1d\", n_bins=10)\n\nplt.figure(figsize=(5,4))\nplt.plot(cal_overall[\"mean_pred\"], cal_overall[\"mean_true\"], marker=\"o\")\nlim = max(abs(cal_overall[\"mean_pred\"]).max(), abs(cal_overall[\"mean_true\"]).max())\nplt.plot([-lim, lim], [-lim, lim], linestyle=\"--\")\nplt.xlabel(\"Mean predicted (bin)\"); plt.ylabel(\"Mean realized (bin)\")\nplt.title(\"Calibration (overall) — lin_lags\")\nplt.tight_layout()\nplt.savefig(\"docs/figs/calibration_overall_lin.png\", dpi=160)\n\"Saved docs/figs/calibration_overall_lin.png\"\n\n# By regime\nplt.figure(figsize=(6.5,4.5))\nfor i, reg in enumerate([\"low\",\"med\",\"high\"], start=1):\n    g = val[(val[\"regime\"]==reg) & (val[\"yhat_lin\"].notna())]\n    if len(g) < 50: \n        continue\n    cal = calibration_by_bins(g, \"yhat_lin\", \"r_1d\", n_bins=6)\n    plt.plot(cal[\"mean_pred\"], cal[\"mean_true\"], marker=\"o\", label=reg)\nlim = 0.02  # small returns\nplt.plot([-lim, lim], [-lim, lim], linestyle=\"--\")\nplt.xlabel(\"Mean predicted (bin)\"); plt.ylabel(\"Mean realized (bin)\")\nplt.title(\"Calibration by regime — lin_lags\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"docs/figs/calibration_by_regime_lin.png\", dpi=160)\n\"Saved docs/figs/calibration_by_regime_lin.png\"\n```\n\n---\n\n## Wrap‑up (10 min) — key points to emphasize\n\n* **Regime thresholds must be set on TRAIN ONLY** each split to avoid leakage.\n* Report **by‑regime** metrics alongside overall metrics; show **macro** & **micro**.\n* Calibration plots (binned predicted vs. realized) quickly show **systematic bias**; compare regimes.\n\n---\n\n## Homework (due before Session 19)\n\n**Goal:** Produce a **full regime‑aware evaluation** across **all splits** for **naive** and **linear‑lags** models and include the figures in your Quarto report.\n\n### A. Script: `scripts/regime_eval.py` — run across all splits\n\n```python\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, json, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i=train_min-1; n=len(u)\n    while True:\n        if i>=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=n: break\n        splits.append((a,b,u[vs],u[ve])); i+=step\n    return splits\n\ndef regime_thresholds(train_df, vol_col=\"roll_std_20\", q_low=0.33, q_high=0.66):\n    v = train_df[vol_col].dropna().to_numpy()\n    if len(v) < 100:\n        q_low, q_high = 0.4, 0.8\n    return float(np.quantile(v, q_low)), float(np.quantile(v, q_high))\n\ndef label_regime(df, vol_col, lo, hi):\n    out = df.copy()\n    vc = out[vol_col]\n    reg = pd.Series(pd.Categorical([\"unknown\"]*len(out), categories=[\"low\",\"med\",\"high\",\"unknown\"]), index=out.index)\n    reg[(vc.notna()) & (vc <= lo)] = \"low\"\n    reg[(vc.notna()) & (vc > lo) & (vc < hi)] = \"med\"\n    reg[(vc.notna()) & (vc >= hi)] = \"high\"\n    out[\"regime\"] = reg.astype(\"category\")\n    return out\n\ndef add_naive(df):\n    out = df.copy()\n    out[\"yhat_naive\"] = out[\"log_return\"]\n    return out\n\ndef fit_lin(tr, va, xcols):\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LinearRegression\n    preds=[]\n    for tkr, trk in tr.groupby(\"ticker\"):\n        vak = va[va[\"ticker\"]==tkr]\n        if len(trk)==0 or len(vak)==0: continue\n        Xtr = trk.dropna(subset=xcols); \n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(Xtr[xcols].values, Xtr[\"r_1d\"].values)\n        yhat = pipe.predict(vak[xcols].fillna(0).values)\n        out = vak[[\"date\",\"ticker\",\"r_1d\",\"log_return\",\"regime\"]].copy()\n        out[\"yhat_lin\"] = yhat\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()\n\ndef mae(y, yhat): y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(np.abs(y-yhat)))\ndef smape(y,yhat,eps=1e-8):\n    y=np.asarray(y); yhat=np.asarray(yhat); return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    return float(mae(y_true, y_pred)/(mae(y_train_true, y_train_naive)+1e-12))\n\ndef per_regime_metrics(val_df, train_df, pred_col):\n    rows=[]\n    for reg, g in val_df.groupby(\"regime\"):\n        if reg==\"unknown\" or len(g)==0: continue\n        per=[]\n        for tkr, gv in g.groupby(\"ticker\"):\n            gt = train_df[train_df[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n            if len(gt)==0: continue\n            per.append({\"ticker\":tkr,\"n\":int(gv[\"r_1d\"].notna().sum()),\n                        \"mae\": mae(gv[\"r_1d\"], gv[pred_col]),\n                        \"smape\": smape(gv[\"r_1d\"], gv[pred_col]),\n                        \"mase\": mase(gv[\"r_1d\"], gv[pred_col], gt[\"r_1d\"], gt[\"log_return\"]),\n                        \"regime\": reg})\n        pt = pd.DataFrame(per)\n        if pt.empty: continue\n        macro = pt[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n        w = pt[\"n\"].to_numpy()\n        micro = {\"micro_mae\": float(np.average(pt[\"mae\"], weights=w)),\n                 \"micro_smape\": float(np.average(pt[\"smape\"], weights=w)),\n                 \"micro_mase\": float(np.average(pt[\"mase\"], weights=w))}\n        rows.append({\"regime\":reg, **{f\"macro_{k}\":float(v) for k,v in macro.items()}, **micro})\n    return pd.DataFrame(rows)\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--train-min\", type=int, default=252)\n    ap.add_argument(\"--val-size\", type=int, default=63)\n    ap.add_argument(\"--step\", type=int, default=63)\n    ap.add_argument(\"--embargo\", type=int, default=5)\n    ap.add_argument(\"--vol-col\", default=\"roll_std_20\")\n    ap.add_argument(\"--xcols\", nargs=\"+\", default=[\"lag1\",\"lag2\",\"lag3\"])\n    ap.add_argument(\"--out-summary\", default=\"reports/regime_summary.csv\")\n    args = ap.parse_args()\n\n    df = pd.read_parquet(args.features).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    # Ensure vol col exists\n    if args.vol_col not in df.columns:\n        df[args.vol_col] = df.groupby(\"ticker\")[\"log_return\"].rolling(20, min_periods=20).std().reset_index(level=0, drop=True)\n\n    # Build lags if missing\n    for k in [1,2,3]:\n        col = f\"lag{k}\"\n        if col not in df.columns:\n            df[col] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n\n    splits = make_splits(df[\"date\"], args.train_min, args.val_size, args.step, args.embargo)\n    Path(\"reports\").mkdir(parents=True, exist_ok=True)\n    thresh_rec = {}\n\n    rows=[]\n    for sid,(a,b,c,d) in enumerate(splits, start=1):\n        tr = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)]\n        va = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)]\n        lo, hi = regime_thresholds(tr, args.vol_col)\n        thresh_rec[sid] = {\"lo\":lo, \"hi\":hi, \"train_range\":f\"{a.date()}→{b.date()}\"}\n        trL = label_regime(tr, args.vol_col, lo, hi)\n        vaL = label_regime(va, args.vol_col, lo, hi)\n\n        # predictions\n        trN, vaN = add_naive(trL), add_naive(vaL)\n        val_lin = fit_lin(trN, vaN, args.xcols)\n        vaN = vaN.merge(val_lin[[\"date\",\"ticker\",\"yhat_lin\"]], on=[\"date\",\"ticker\"], how=\"left\")\n\n        # metrics\n        m_naive = per_regime_metrics(vaN, trN, \"yhat_naive\").assign(split=sid, model=\"naive\")\n        m_lin   = per_regime_metrics(vaN.dropna(subset=[\"yhat_lin\"]), trN, \"yhat_lin\").assign(split=sid, model=\"lin_lags\")\n\n        out = pd.concat([m_naive, m_lin], ignore_index=True)\n        out.to_csv(f\"reports/regime_metrics_split{sid}.csv\", index=False)\n        rows.append(out)\n\n    pd.concat(rows, ignore_index=True).to_csv(args.out_summary, index=False)\n    Path(\"reports/regime_thresholds.json\").write_text(json.dumps(thresh_rec, indent=2))\n    print(\"Wrote\", args.out_summary, \"and per-split CSVs; thresholds saved to reports/regime_thresholds.json\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun:\n\n```bash\n%%bash\nchmod +x scripts/regime_eval.py\npython scripts/regime_eval.py\n```\n\n### B. Plot summary figures for your report\n\n```python\nimport pandas as pd, matplotlib.pyplot as plt, pathlib\npathlib.Path(\"docs/figs\").mkdir(parents=True, exist_ok=True)\n\ndf = pd.read_csv(\"reports/regime_summary.csv\")\n# Micro MAE by regime per model\npivot = df.pivot_table(index=[\"split\",\"regime\"], columns=\"model\", values=\"micro_mae\")\nplt.figure(figsize=(6,4))\nfor model in pivot.columns:\n    plt.plot(pivot.xs(\"low\", level=\"regime\").index, pivot.xs(\"low\", level=\"regime\")[model], marker=\"o\", label=f\"{model} — low\")\n    plt.plot(pivot.xs(\"high\", level=\"regime\").index, pivot.xs(\"high\", level=\"regime\")[model], marker=\"s\", label=f\"{model} — high\")\nplt.xlabel(\"Split\"); plt.ylabel(\"Micro MAE\")\nplt.title(\"Micro MAE by regime (low vs high)\")\nplt.legend(); plt.tight_layout()\nplt.savefig(\"docs/figs/regime_micro_mae.png\", dpi=160)\n\"Saved docs/figs/regime_micro_mae.png\"\n```\n\n### C. Add to Quarto report\n\nIn `reports/eda.qmd` (or a new `reports/regime.qmd`), include:\n\n````markdown\n## Regime‑aware Results\n\n::: {#8f00d8aa .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv(\"reports/regime_summary.csv\")\ndf.sort_values([\"split\",\"model\",\"regime\"]).head(12)\n````\n\n![](../docs/figs/regime_micro_mae.png)\n\n![](../docs/figs/calibration_overall_lin.png)\n\n![](../docs/figs/calibration_by_regime_lin.png)\n\n````\n\nRender:\n```bash\nquarto render reports/eda.qmd\n````\n\n### D. Quick test to protect **train‑only** thresholds\n\n```python\n# tests/test_regime_thresholds.py\nimport json, pandas as pd\n\ndef test_thresholds_exist_and_train_range():\n    data = json.load(open(\"reports/regime_thresholds.json\"))\n    assert len(data) >= 1\n    # basic sanity: low < high\n    for sid, rec in data.items():\n        assert float(rec[\"lo\"]) < float(rec[\"hi\"])\n        assert \"→\" in rec[\"train_range\"]\n`````\n:::\n\n\nRun:\n\n```bash\n%%bash\npytest -q -k regime_thresholds\n```\n\n---\n\n## Instructor checklist (before class)\n\n* Ensure `features_v1.parquet` contains `roll_std_20` (or let lab compute it).\n* Keep the in‑class run to the **first split** to finish inside the time box; homework runs all splits.\n* Have a slide explaining **why thresholds must not be learned on validation/test**.\n\n## Emphasize while teaching\n\n* Regime thresholds are **part of your training‑time state**—store them (JSON) and **do not recompute from validation/test**.\n* Report both **macro** and **micro** metrics **by regime** so small/large tickers don’t dominate silently.\n* Use calibration plots to diagnose **systematic bias**; e.g., under‑prediction in **high vol**.\n\n## Grading (pass/revise)\n\n* `scripts/regime_eval.py` runs and writes `reports/regime_summary.csv` + per‑split CSVs.\n* Thresholds saved to `reports/regime_thresholds.json`.\n* Figures exist under `docs/figs/` and are embedded in the report.\n* Short tests pass; report includes a paragraph discussing results by regime.\n\nYou now have a **regime‑aware evaluation** layered on your rolling‑origin pipeline—perfect preparation for Session 19, where you’ll implement PyTorch datasets and a minimal training loop.\n\n",
    "supporting": [
      "lec18_files"
    ],
    "filters": [],
    "includes": {}
  }
}