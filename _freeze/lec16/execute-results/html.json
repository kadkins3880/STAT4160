{
  "hash": "dbae99b0d3f16245b228457389d82724",
  "result": {
    "markdown": "---\ntitle: \"Session 16\"\n---\n\nBelow is a complete lecture package for **Session 16 — Classical Baselines** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. In class you’ll train a **lags‑only linear regressor per ticker** and compare it to the **naive** and **seasonal‑naive** baselines from Session 15. You’ll also see a short, optional **ARIMA** demo and log results in a consistent schema for future comparison.\n\n> **Educational use only — not trading advice.**\n> Assumes your repo (e.g., `unified-stocks-teamX`) with `data/processed/returns.parquet` and `data/processed/features_v1.parquet` from Sessions 9–10. Cells include safe fallbacks if some files are missing.\n\n---\n\n## Session 16 — Classical baselines (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Fit a **per‑ticker** lags‑only linear regressor to predict **next‑day log return** $r_{t+1}$.\n2. Evaluate models with **MAE, sMAPE, MASE** using the **rolling‑origin splits** (with embargo) from Session 15.\n3. Log results in a **consistent table schema** for per‑ticker and split‑level summaries.\n4. Understand **ARIMA** at a glance and its common pitfalls (optional demo).\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: where classical models fit; pitfalls with ARIMA; cross‑sectional regressors\n* **(10 min)** Slides: results table schema & comparison to baselines\n* **(35 min)** **In‑class lab**: train per‑ticker **Linear (lags‑only)** → evaluate across 2 splits → compare to naive/seasonal‑naive → log CSVs\n* **(10 min)** Wrap‑up + homework brief\n* **(10 min)** Buffer\n\n---\n\n## Slides / talking points\n\n### Why “classical” now?\n\n* Creates a **credible, strong baseline** against naive that’s still transparent.\n* Supports **fast iteration** and helps you debug feature definitions before deep models.\n\n### Lags‑only linear regressor\n\n* **Features** at time $t$: `lag1`, `lag2`, `lag3` (i.e., past returns), optionally a few stable stats (`roll_std_20`, `zscore_20`).\n* **Target**: `r_1d` (next‑day log return).\n* Fit **per ticker** to avoid cross‑sectional leakage for now.\n\n### ARIMA 60‑second pitfall tour\n\n* Stationarity: **fit on returns**, not prices (unless differencing).\n* Evaluation: **re‑fit only on train**; generate **one‑step‑ahead** forecasts on val, updating state **without peeking**.\n* Over‑differencing & mis‑specified seasonal terms → bad bias.\n* Computational cost grows with grid search; keep demo tiny.\n\n### Results table schema (consistent across sessions)\n\n* **Per‑split summary**:\n  `split, train_range, val_range, model, macro_mae, macro_smape, macro_mase, micro_mae, micro_smape, micro_mase`\n* **Per‑ticker metrics**:\n  `split, ticker, n, model, mae, smape, mase`\n\n---\n\n## In‑class lab (35 min, Colab‑friendly)\n\n> Run each block as its **own cell**. Update `REPO_NAME` if needed.\n\n### 0) Setup & data (with fallbacks)\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"  # <- change if needed\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, numpy as np, pandas as pd\nfrom pathlib import Path\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/raw\",\"data/processed\",\"reports\",\"models\",\"scripts\",\"tests\"]:\n    Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\n# Load returns; if missing, synthesize\nrpath = Path(\"data/processed/returns.parquet\")\nif rpath.exists():\n    returns = pd.read_parquet(rpath)\nelse:\n    rng = np.random.default_rng(0)\n    dates = pd.bdate_range(\"2022-01-03\", periods=360)\n    rows=[]\n    for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n        eps = rng.normal(0,0.012, size=len(dates)).astype(\"float32\")\n        adj = 100*np.exp(np.cumsum(eps))\n        df = pd.DataFrame({\n            \"date\": dates, \"ticker\": t,\n            \"adj_close\": adj.astype(\"float32\"),\n            \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n        })\n        df[\"r_1d\"] = df[\"log_return\"].shift(-1)\n        df[\"weekday\"] = df[\"date\"].dt.weekday.astype(\"int8\")\n        df[\"month\"]   = df[\"date\"].dt.month.astype(\"int8\")\n        rows.append(df)\n    returns = pd.concat(rows, ignore_index=True).dropna().reset_index(drop=True)\n    returns[\"ticker\"] = returns[\"ticker\"].astype(\"category\")\n    returns.to_parquet(rpath, index=False)\n\n# Load features_v1 or derive minimal lags from returns if missing\nfpath = Path(\"data/processed/features_v1.parquet\")\nif fpath.exists():\n    feats = pd.read_parquet(fpath)\nelse:\n    # Minimal lags derived just from returns\n    feats = returns.sort_values([\"ticker\",\"date\"]).copy()\n    for k in [1,2,3]:\n        feats[f\"lag{k}\"] = feats.groupby(\"ticker\")[\"log_return\"].shift(k)\n    feats = feats.dropna(subset=[\"lag1\",\"lag2\",\"lag3\",\"r_1d\"]).reset_index(drop=True)\n\n# Harmonize\nfeats[\"date\"] = pd.to_datetime(feats[\"date\"])\nfeats[\"ticker\"] = feats[\"ticker\"].astype(\"category\")\nfeats = feats.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\nfeats.head()\n```\n\n### 1) Rolling‑origin date splits (reuse Session 15 logic)\n\n```python\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i = train_min-1; n=len(u)\n    while True:\n        if i>=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=n: break\n        splits.append((a,b,u[vs],u[ve])); i+=step\n    return splits\n\nsplits = make_rolling_origin_splits(feats[\"date\"], 252, 63, 63, 5)\nlen(splits), splits[:2]\n```\n\n### 2) Metrics & baselines (from Session 15)\n\n```python\ndef mae(y, yhat): \n    y = np.asarray(y); yhat = np.asarray(yhat); \n    return float(np.mean(np.abs(y - yhat)))\n\ndef smape(y, yhat, eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2.0*np.abs(y - yhat)/(np.abs(y)+np.abs(yhat)+eps)))\n\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    scale = mae(y_train_true, y_train_naive) + 1e-12\n    return float(mae(y_true, y_pred)/scale)\n\ndef add_baseline_preds(df: pd.DataFrame, seasonality:int=5) -> pd.DataFrame:\n    out = df.copy()\n    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)\n    out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1)) if seasonality>1 else out[\"yhat_naive\"]\n    return out\n```\n\n### 3) **Per‑ticker lags‑only LinearRegression** (fit only on each split’s TRAIN)\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\n# Choose features (lags only for in-class lab)\nXCOLS = [c for c in [\"lag1\",\"lag2\",\"lag3\"] if c in feats.columns]\nassert XCOLS, \"No lag features found. Ensure features_v1 or fallback creation ran.\"\n\ndef fit_predict_lin_lags(train_df, val_df):\n    \"\"\"Fit per-ticker pipeline(StandardScaler, LinearRegression) on TRAIN; predict on VAL.\"\"\"\n    preds=[]\n    for tkr, tr in train_df.groupby(\"ticker\"):\n        va = val_df[val_df[\"ticker\"]==tkr]\n        if len(tr)==0 or len(va)==0: \n            continue\n        pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n                         (\"lr\", LinearRegression())])\n        pipe.fit(tr[XCOLS].values, tr[\"r_1d\"].values)\n        yhat = pipe.predict(va[XCOLS].values)\n        out = va[[\"date\",\"ticker\",\"r_1d\",\"log_return\"]].copy()\n        out[\"yhat_linlags\"] = yhat.astype(\"float32\")\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame(columns=[\"date\",\"ticker\",\"r_1d\",\"log_return\",\"yhat_linlags\"])\n```\n\n### 4) Evaluate **across the first 2 splits**; compare to naive/seasonal‑naive\n\n```python\nseasonality = 5\nfeats_baseline = add_baseline_preds(feats, seasonality=seasonality)\n\ndef per_ticker_metrics(df_val_pred, df_train, method_col):\n    rows=[]\n    for tkr, gv in df_val_pred.groupby(\"ticker\"):\n        if method_col not in gv: \n            continue\n        gv = gv.dropna(subset=[\"r_1d\", method_col])\n        if len(gv)==0: \n            continue\n        # TRAIN scale for MASE\n        gt = df_train[df_train[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n        gt_naive = gt[\"log_return\"] if \"yhat_s\" not in method_col else gt[\"log_return\"].shift(seasonality-1)\n        gt_naive = gt_naive.loc[gt.index]\n        rows.append({\n            \"ticker\": tkr,\n            \"n\": int(len(gv)),\n            \"mae\": mae(gv[\"r_1d\"], gv[method_col]),\n            \"smape\": smape(gv[\"r_1d\"], gv[method_col]),\n            \"mase\": mase(gv[\"r_1d\"], gv[method_col], gt[\"r_1d\"], gt_naive),\n        })\n    return pd.DataFrame(rows)\n\ndef summarize_split(feats_frame, sid, split, save_prefix=\"linlags\"):\n    a,b,c,d = split\n    tr = feats_frame[(feats_frame[\"date\"]>=a)&(feats_frame[\"date\"]<=b)].copy()\n    va = feats_frame[(feats_frame[\"date\"]>=c)&(feats_frame[\"date\"]<=d)].copy()\n    # Predictions\n    val_pred = fit_predict_lin_lags(tr, va)\n    # Attach baseline preds on val slice\n    va_base = add_baseline_preds(va, seasonality=seasonality)\n    val_pred = val_pred.merge(va_base[[\"date\",\"ticker\",\"yhat_naive\",\"yhat_s\"]], on=[\"date\",\"ticker\"], how=\"left\")\n\n    # Per-ticker metrics\n    pt_lin = per_ticker_metrics(val_pred, tr, \"yhat_linlags\"); pt_lin[\"model\"] = \"lin_lags\"\n    pt_nav = per_ticker_metrics(val_pred.rename(columns={\"yhat_naive\":\"yhat_linlags\"}), tr, \"yhat_linlags\"); pt_nav[\"model\"]=\"naive\"\n    pt_sea = per_ticker_metrics(val_pred.rename(columns={\"yhat_s\":\"yhat_linlags\"}), tr, \"yhat_linlags\"); pt_sea[\"model\"]=f\"s{seasonality}\"\n\n    # Save per-ticker\n    out_pt = pd.concat([pt_lin.assign(split=sid), pt_nav.assign(split=sid), pt_sea.assign(split=sid)], ignore_index=True)\n    out_pt.to_csv(f\"reports/{save_prefix}_per_ticker_split{sid}.csv\", index=False)\n\n    # Aggregate\n    def agg(df):\n        if df.empty: \n            return {\"macro_mae\":np.nan,\"macro_smape\":np.nan,\"macro_mase\":np.nan,\"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n        macro = df[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n        w = df[\"n\"].to_numpy()\n        micro = {\"micro_mae\": float(np.average(df[\"mae\"], weights=w)),\n                 \"micro_smape\": float(np.average(df[\"smape\"], weights=w)),\n                 \"micro_mase\": float(np.average(df[\"mase\"], weights=w))}\n        return {f\"macro_{k}\":float(v) for k,v in macro.items()} | micro\n\n    rows=[]\n    for name, pt in [(\"lin_lags\", pt_lin), (\"naive\", pt_nav), (f\"s{seasonality}\", pt_sea)]:\n        rows.append({\"split\":sid, \"train_range\": f\"{a.date()}→{b.date()}\",\n                     \"val_range\": f\"{c.date()}→{d.date()}\",\n                     \"model\":name, **agg(pt)})\n    return pd.DataFrame(rows)\n\n# Run on first 2 splits in class\nsummary_frames=[]\nfor sid, split in enumerate(splits[:2], start=1):\n    sf = summarize_split(feats_baseline, sid, split, save_prefix=\"linlags\")\n    summary_frames.append(sf)\n\nsummary = pd.concat(summary_frames, ignore_index=True)\nsummary.to_csv(\"reports/linlags_summary_splits12.csv\", index=False)\nsummary\n```\n\n### 5) (Optional) Tiny ARIMA demo on **one ticker** for the first split\n\n```python\n# Optional: quick ARIMA(1,0,0) demo predicting r_{t+1} on val for a single ticker\ntry:\n    from statsmodels.tsa.arima.model import ARIMA\n    import warnings; warnings.filterwarnings(\"ignore\")\n    a,b,c,d = splits[0]\n    tkr = feats[\"ticker\"].cat.categories[0]\n    tr = feats[(feats[\"ticker\"]==tkr) & (feats[\"date\"]>=a) & (feats[\"date\"]<=b)]\n    va = feats[(feats[\"ticker\"]==tkr) & (feats[\"date\"]>=c) & (feats[\"date\"]<=d)]\n    # Fit on TRAIN returns only (endog = log_return). Predict one-step ahead for VAL dates.\n    model = ARIMA(tr[\"log_return\"].to_numpy(), order=(1,0,0))\n    res = model.fit()\n    # Forecast length = len(va), one-step-ahead with dynamic=False updates internally\n    # (For strict no-peek rolling one-step, loop and append val true values; here we keep demo simple.)\n    fc = res.forecast(steps=len(va))\n    arima_mae = mae(va[\"r_1d\"], fc)  # compare against next-day return\n    float(arima_mae)\nexcept Exception as e:\n    print(\"ARIMA demo skipped:\", e)\n```\n\n> ⚠️ ARIMA is **optional** and **slow** on large loops. If you try it per ticker/per split, keep the dataset tiny.\n\n---\n\n## Wrap‑up (10 min)\n\n* You trained a **per‑ticker lags‑only linear** model and compared it fairly to **naive** and **seasonal‑naive** using the **same splits** and **MASE scale** (from the train window).\n* You logged results in a **stable schema** that you’ll reuse for future models (LSTM / Transformer).\n* ARIMA can be illustrative but is often **fragile + slower**; treat it as optional for your project scale.\n\n---\n\n## Homework (due before next session)\n\n**Goal:** 1) Run the linear lags baseline across **all splits**; 2) Write your **first model card** (Quarto) for the classical baseline.\n\n### Part A — CLI script to evaluate Linear‑Lags across *all* splits\n\n```python\n# scripts/eval_linlags.py\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, numpy as np, pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom pathlib import Path\n\ndef mae(y,yhat): return float(np.mean(np.abs(np.asarray(y)-np.asarray(yhat))))\ndef smape(y,yhat,eps=1e-8):\n    y = np.asarray(y); yhat = np.asarray(yhat)\n    return float(np.mean(2*np.abs(y-yhat)/(np.abs(y)+np.abs(yhat)+eps)))\ndef mase(y_true, y_pred, y_train_true, y_train_naive):\n    return float(mae(y_true, y_pred) / (mae(y_train_true, y_train_naive)+1e-12))\n\ndef make_splits(dates, train_min, val_size, step, embargo):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    splits=[]; i=train_min-1; n=len(u)\n    while True:\n        if i>=n: break\n        a,b = u[0], u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=n: break\n        splits.append((a,b,u[vs],u[ve])); i+=step\n    return splits\n\ndef add_baselines(df, seasonality):\n    out = df.copy()\n    out[\"yhat_naive\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s)\n    out[\"yhat_s\"] = out.groupby(\"ticker\")[\"log_return\"].transform(lambda s: s.shift(seasonality-1)) if seasonality>1 else out[\"yhat_naive\"]\n    return out\n\ndef fit_predict_lin(train_df, val_df, xcols):\n    from sklearn.linear_model import LinearRegression\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import Pipeline\n    preds=[]\n    for tkr, tr in train_df.groupby(\"ticker\"):\n        va = val_df[val_df[\"ticker\"]==tkr]\n        if len(tr)==0 or len(va)==0: continue\n        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n        pipe.fit(tr[xcols].values, tr[\"r_1d\"].values)\n        yhat = pipe.predict(va[xcols].values)\n        out = va[[\"date\",\"ticker\",\"r_1d\",\"log_return\"]].copy()\n        out[\"yhat_linlags\"] = yhat\n        preds.append(out)\n    return pd.concat(preds, ignore_index=True) if preds else pd.DataFrame()\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--seasonality\", type=int, default=5)\n    ap.add_argument(\"--train-min\", type=int, default=252)\n    ap.add_argument(\"--val-size\", type=int, default=63)\n    ap.add_argument(\"--step\", type=int, default=63)\n    ap.add_argument(\"--embargo\", type=int, default=5)\n    ap.add_argument(\"--xcols\", nargs=\"+\", default=[\"lag1\",\"lag2\",\"lag3\"])\n    ap.add_argument(\"--out-summary\", default=\"reports/linlags_summary.csv\")\n    ap.add_argument(\"--out-per-ticker\", default=\"reports/linlags_per_ticker_split{sid}.csv\")\n    args = ap.parse_args()\n\n    df = pd.read_parquet(args.features).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n    splits = make_splits(df[\"date\"], args.train_min, args.val_size, args.step, args.embargo)\n    df = add_baselines(df, args.seasonality)\n\n    rows=[]\n    for sid, (a,b,c,d) in enumerate(splits, start=1):\n        tr = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)]\n        va = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)]\n        val_pred = fit_predict_lin(tr, va, args.xcols)\n        va = va.merge(val_pred[[\"date\",\"ticker\",\"yhat_linlags\"]], on=[\"date\",\"ticker\"], how=\"left\")\n        # per-ticker\n        pts=[]\n        for tkr, gv in va.groupby(\"ticker\"):\n            gv = gv.dropna(subset=[\"r_1d\",\"yhat_linlags\"])\n            if len(gv)==0: continue\n            gt = tr[tr[\"ticker\"]==tkr].dropna(subset=[\"r_1d\"])\n            gt_naive = gt[\"log_return\"]  # scale comparator for MASE\n            pts.append({\"ticker\":tkr,\"n\":int(len(gv)),\n                        \"mae\": mae(gv[\"r_1d\"], gv[\"yhat_linlags\"]),\n                        \"smape\": smape(gv[\"r_1d\"], gv[\"yhat_linlags\"]),\n                        \"mase\": mase(gv[\"r_1d\"], gv[\"yhat_linlags\"], gt[\"r_1d\"], gt_naive)})\n        pt = pd.DataFrame(pts)\n        Path(\"reports\").mkdir(exist_ok=True)\n        pt.assign(split=sid, model=\"lin_lags\").to_csv(args.out_per_ticker.format(sid=sid), index=False)\n\n        # aggregate\n        if not pt.empty:\n            macro = pt[[\"mae\",\"smape\",\"mase\"]].mean().to_dict()\n            w = pt[\"n\"].to_numpy()\n            micro = {\"micro_mae\": float(np.average(pt[\"mae\"], weights=w)),\n                     \"micro_smape\": float(np.average(pt[\"smape\"], weights=w)),\n                     \"micro_mase\": float(np.average(pt[\"mase\"], weights=w))}\n        else:\n            macro = {\"mae\":np.nan,\"smape\":np.nan,\"mase\":np.nan}\n            micro = {\"micro_mae\":np.nan,\"micro_smape\":np.nan,\"micro_mase\":np.nan}\n        rows.append({\"split\":sid,\"train_range\":f\"{a.date()}→{b.date()}\",\"val_range\":f\"{c.date()}→{d.date()}\",\n                     \"model\":\"lin_lags\", \"macro_mae\":float(macro[\"mae\"]), \"macro_smape\":float(macro[\"smape\"]), \"macro_mase\":float(macro[\"mase\"]),\n                     **micro})\n\n    pd.DataFrame(rows).to_csv(args.out_summary, index=False)\n    print(\"Wrote\", args.out_summary)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nMake executable & run:\n\n```bash\n%%bash\nchmod +x scripts/eval_linlags.py\npython scripts/eval_linlags.py --xcols lag1 lag2 lag3\n```\n\n### Part B — Quarto **Model Card** for the Linear‑Lags baseline\n\nCreate `docs/model_card_linear.qmd`:\n\n````markdown\n---\ntitle: \"Model Card — Linear Lags (Per‑Ticker)\"\nformat:\n  html:\n    theme: cosmo\n    toc: true\nparams:\n  model_name: \"Linear Lags (per‑ticker)\"\n  data: \"features_v1.parquet\"\n---\n\n> **Educational use only — not trading advice.** Predicts next‑day log return \\(r_{t+1}\\) using past lags.\n\n## Overview\n\n- **Model:** Per‑ticker linear regression with features: `lag1`, `lag2`, `lag3`.\n- **Data:** `features_v1.parquet` (Session 10).  \n- **Splits:** Expanding, quarterly val, 5‑day embargo (Session 15).  \n- **Baselines:** Naive and seasonal‑naive \\(s=5\\).\n\n## Metrics (across splits)\n\n::: {#50e1d667 .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv(\"reports/linlags_summary.csv\")\ndf\n````\n\n## Discussion\n\n* **Assumptions:** Linear relation to recent returns; stationarity at return level.\n* **Strengths:** Fast, interpretable, leakage‑resistant with proper splits.\n* **Failure modes:** Regime shifts; volatility spikes; nonlinearity.\n* **Ethics:** Educational; not suitable for trading.\n\n````\n\nRender (if Quarto is available):\n```bash\nquarto render docs/model_card_linear.qmd\n````\n\n### Part C — Quick test to safeguard results shape\n\n```python\n# tests/test_linlags_results.py\nimport pandas as pd, os\n\ndef test_linlags_summary_exists_and_columns():\n    assert os.path.exists(\"reports/linlags_summary.csv\")\n    df = pd.read_csv(\"reports/linlags_summary.csv\")\n    need = {\"split\",\"model\",\"macro_mae\",\"micro_mae\"}\n    assert need.issubset(df.columns)\n`````\n:::\n\n\nRun:\n\n```bash\n%%bash\npytest -q -k linlags_results\n```\n\n### Part D — (Optional) Extend features or add Ridge\n\n* Try `--xcols lag1 lag2 lag3 roll_std_20 zscore_20` (if present in `features_v1`).\n* Swap `LinearRegression` for `Ridge(alpha=1.0)`; log and compare.\n\n---\n\n## Instructor checklist (before class)\n\n* Verify `features_v1.parquet` has `lag1..lag3` or the fallback cell creates them.\n* Dry‑run the 2‑split demo; ensure total runtime < 5–6 minutes.\n* Optionally prepare an ARIMA demo on **one** ticker to illustrate pitfalls.\n\n## Emphasize while teaching\n\n* Keep **splits identical** across models for fair comparison.\n* **MASE < 1** ⇒ your model beats naive on train‑scale; report macro & micro.\n* Linear lags are a **transparent baseline**—use them to validate your entire pipeline.\n\n## Grading (pass/revise)\n\n* `scripts/eval_linlags.py` runs and writes `reports/linlags_summary.csv` + per‑ticker CSVs.\n* Model card exists and renders (locally or in CI artifact).\n* Tests for results table shape pass.\n* Results show a reasonable comparison against naive/seasonal‑naive.\n\nYou now have a **solid classical baseline** with a reproducible evaluation and reporting workflow—perfect for benchmarking upcoming neural models.\n\n",
    "supporting": [
      "lec16_files"
    ],
    "filters": [],
    "includes": {}
  }
}