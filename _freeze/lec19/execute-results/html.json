{
  "hash": "12af2300ce5db7bffda2b39ebb95a3ac",
  "result": {
    "markdown": "---\ntitle: \"Session 19 — Tensors, Datasets, Training Loop\"\n---\n\nBelow is a complete lecture package for **Session 19 — Tensors, Datasets, Training Loop** (75 minutes). It includes a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. You will build a **windowed multi‑ticker dataset** for next‑day return prediction, write a **minimal PyTorch training loop** with **early stopping**, and **save/load checkpoints**. We’ll keep it CPU‑friendly and optionally accelerate with GPU/Mixed Precision on Colab.\n\n> **Educational use only — not trading advice.**\n> Assumes your Drive‑mounted repo (e.g., `unified-stocks-teamX`) and `data/processed/features_v1.parquet` (or `features_v1_static.parquet`) with columns like: `ticker`, `date`, `log_return`, `r_1d` (label), and some features (`lag1..lag3`, `roll_std_20`, `zscore_20`, …). Cells include **safe fallbacks** if files are missing.\n\n---\n\n## Session 19 — Tensors, Datasets, Training Loop (75 min)\n\n### Learning goals\n\nBy the end of class, students can:\n\n1. Create a **windowed sequence dataset** across **multiple tickers** with shape `(B, T, F)` → predict `r_1d` at time `t+1`.\n2. Use `Dataset`/`DataLoader` correctly: **pin memory**, worker seeding, and efficient slicing.\n3. Write a **minimal training loop** with **early stopping**, **AMP** (mixed precision on GPU), and **checkpoint save/load**.\n4. Produce a tidy **validation metrics CSV** to compare later models.\n\n---\n\n## Agenda (75 min)\n\n* **(10 min)** Slides: tensors & batching; `Dataset`/`DataLoader`; pinning memory; reproducibility with seeds\n* **(10 min)** Slides: training loop anatomy; early stopping; AMP; checkpoints\n* **(35 min)** **In‑class lab**: build `WindowedDataset` → DataLoaders → tiny GRU regressor → train w/ early stopping → save best checkpoint; evaluate\n* **(10 min)** Wrap‑up + homework brief\n* **(10 min)** Buffer\n\n---\n\n## Slides / talking points (drop into your deck)\n\n### Tensors & batching\n\n* Tensors are N‑D arrays on **CPU** or **GPU** (`.to(device)`); keep everything `float32` unless using AMP.\n* For sequences: **batch** × **time** × **features** ⇒ `(B, T, F)`. Predict a scalar per sequence end (`r_1d` at time `t+1`).\n\n### `Dataset`/`DataLoader` patterns\n\n* **Precompute an index** of windows: for each ticker, sliding windows end at `i` with context `T`; target is `r_1d[i]`.\n* `DataLoader` tips:\n\n  * `pin_memory=True` and `non_blocking=True` on `.to(device)` speed H2D copies (when using GPU).\n  * Seed workers for reproducibility; keep `num_workers=2` (Colab stable).\n\n### Seeds & determinism\n\n* Set `python`, `numpy`, `torch` seeds; disable CuDNN benchmarking for reproducibility; prefer small batch sizes that fit CPU/GPU.\n\n### Training loop w/ early stopping\n\n* Loop: `train_step` (model in `train()`), `val_step` (model in `eval()` + `no_grad()`).\n* Track **best val loss**; stop after `patience` epochs without improvement.\n* Save **checkpoint**: `state_dict`, optimizer state, epoch, metrics. Load with `load_state_dict`.\n\n### AMP & checkpoints\n\n* On CUDA, wrap forward in `torch.cuda.amp.autocast()` and use `GradScaler` to scale loss.\n* Save the best checkpoint to `models/…pt`; log a CSV under `reports/`.\n\n---\n\n## In‑class lab (35 min, Colab‑friendly)\n\n> Run each block as its own **separate cell** in Colab. Replace `REPO_NAME` as needed.\n\n### 0) Setup, mount, and check device\n\n```python\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nREPO_NAME  = \"unified-stocks-teamX\"   # <-- change to your repo\nBASE_DIR   = \"/content/drive/MyDrive/dspt25\"\nREPO_DIR   = f\"{BASE_DIR}/{REPO_NAME}\"\n\nimport os, pathlib, sys\npathlib.Path(REPO_DIR).mkdir(parents=True, exist_ok=True)\nos.chdir(REPO_DIR)\nfor p in [\"data/processed\",\"models\",\"reports\",\"scripts\",\"tests\"]:\n    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\nprint(\"Working dir:\", os.getcwd())\n\nimport torch, platform\nprint(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| Python:\", sys.version.split()[0], \"| OS:\", platform.system())\n```\n\n### 1) Load features and pick columns (with fallbacks)\n\n```python\nimport pandas as pd, numpy as np\nfrom pathlib import Path\n\n# Prefer static universe file if present (from Session 17)\nf_static = Path(\"data/processed/features_v1_static.parquet\")\nf_base   = Path(\"data/processed/features_v1.parquet\")\n\nif f_static.exists():\n    df = pd.read_parquet(f_static)\nelif f_base.exists():\n    df = pd.read_parquet(f_base)\nelse:\n    # Minimal fallback from returns\n    rpath = Path(\"data/processed/returns.parquet\")\n    if not rpath.exists():\n        # synthesize small dataset\n        rng = np.random.default_rng(0)\n        dates = pd.bdate_range(\"2022-01-03\", periods=320)\n        frames=[]\n        for t in [\"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"NVDA\"]:\n            eps = rng.normal(0, 0.012, size=len(dates)).astype(\"float32\")\n            adj = 100*np.exp(np.cumsum(eps))\n            g = pd.DataFrame({\n                \"date\": dates, \"ticker\": t,\n                \"adj_close\": adj.astype(\"float32\"),\n                \"log_return\": np.r_[np.nan, np.diff(np.log(adj))].astype(\"float32\")\n            })\n            g[\"r_1d\"] = g[\"log_return\"].shift(-1)\n            frames.append(g)\n        df = pd.concat(frames, ignore_index=True).dropna().reset_index(drop=True)\n        df[\"ticker\"] = df[\"ticker\"].astype(\"category\")\n    else:\n        df = pd.read_parquet(rpath)\n        df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n        # add minimal lags\n        for k in [1,2,3]:\n            df[f\"lag{k}\"] = df.groupby(\"ticker\")[\"log_return\"].shift(k)\n        df = df.dropna().reset_index(drop=True)\n\n# Ensure minimal features exist\ncand_feats = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\nFEATS = [c for c in cand_feats if c in df.columns]\nassert \"r_1d\" in df.columns, \"Label r_1d missing; rebuild returns/features pipeline.\"\nassert \"log_return\" in df.columns, \"log_return missing.\"\n\n# Keep a small subset of tickers for speed (5–10 tickers)\nsubset = df[\"ticker\"].astype(str).unique().tolist()[:8]\ndf = df[df[\"ticker\"].astype(str).isin(subset)].copy()\n\n# Harmonize types & sort\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf[\"ticker\"] = df[\"ticker\"].astype(\"category\")\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\nprint(\"Using features:\", FEATS, \"| tickers:\", df[\"ticker\"].nunique(), \"| rows:\", len(df))\ndf.head()\n```\n\n### 2) Time‑based split (first rolling‑origin split with embargo)\n\n```python\ndef make_rolling_origin_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u = np.array(sorted(pd.to_datetime(pd.Series(dates).unique())))\n    i = train_min - 1; out=[]\n    while True:\n        if i >= len(u): break\n        a,b = u[0], u[i]\n        vs = i + embargo + 1\n        ve = vs + val_size - 1\n        if ve >= len(u): break\n        out.append((a,b,u[vs],u[ve]))\n        i += step\n    return out\n\nsplits = make_rolling_origin_splits(df[\"date\"], 252, 63, 63, 5)\nassert splits, \"Not enough history for a first split.\"\na,b,c,d = splits[0]\ntrain_df = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)].copy()\nval_df   = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)].copy()\nprint(\"Split 1 - Train:\", a.date(), \"→\", b.date(), \"| Val:\", c.date(), \"→\", d.date(), \n      \"| train rows:\", len(train_df), \"| val rows:\", len(val_df))\n```\n\n### 3) Reproducibility helpers, `FeatureScaler`, and `WindowedDataset`\n\n```python\nimport random, math, json\n\ndef seed_everything(seed=1337):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cudnn.deterministic = True\nseed_everything(1337)\n\nclass FeatureScaler:\n    \"\"\"Train-only mean/std scaler for numpy arrays.\"\"\"\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X: np.ndarray):\n        self.mean_ = X.mean(axis=0, dtype=np.float64)\n        self.std_  = X.std(axis=0, dtype=np.float64) + 1e-8\n        return self\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        return (X - self.mean_) / self.std_\n    def state_dict(self): \n        return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): \n        self.mean_ = np.array(d[\"mean\"], dtype=np.float64)\n        self.std_  = np.array(d[\"std\"],  dtype=np.float64)\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass WindowedDataset(Dataset):\n    \"\"\"\n    Sliding windows over time per ticker (multi-ticker, fixed context_len).\n    Each item: X in shape (T, F), y scalar: r_1d at window end.\n    \"\"\"\n    def __init__(self, frame: pd.DataFrame, feature_cols, context_len=64, scaler: FeatureScaler|None=None):\n        assert \"ticker\" in frame and \"date\" in frame and \"r_1d\" in frame\n        self.feature_cols = feature_cols\n        self.T = int(context_len)\n        self.groups = {}   # ticker -> dict('X': np.ndarray [N,F], 'y': np.ndarray [N])\n        self.index  = []   # list of (ticker, end_idx)\n        # Build groups (per ticker)\n        for tkr, g in frame.groupby(\"ticker\"):\n            g = g.sort_values(\"date\").reset_index(drop=True)\n            X = g[feature_cols].to_numpy(dtype=np.float32)\n            y = g[\"r_1d\"].to_numpy(dtype=np.float32)\n            # valid windows end where we have T steps and y is finite\n            for end in range(self.T-1, len(g)):\n                if not np.isfinite(y[end]): \n                    continue\n                self.index.append((tkr, end))\n            self.groups[tkr] = {\"X\": X, \"y\": y}\n        # Fit scaler on all TRAIN rows (only when building train dataset)\n        self.scaler = scaler or FeatureScaler().fit(\n            np.concatenate([self.groups[t][\"X\"] for t in self.groups], axis=0)\n        )\n    def __len__(self): return len(self.index)\n    def __getitem__(self, i):\n        tkr, end = self.index[i]\n        g = self.groups[tkr]\n        xs = g[\"X\"][end-self.T+1:end+1]        # (T, F) context\n        xs = self.scaler.transform(xs)         # scale using train stats\n        y  = g[\"y\"][end]                       # scalar target\n        return torch.from_numpy(xs), torch.tensor(y), str(tkr)\n\ndef make_loaders(train_df, val_df, feature_cols, context_len=64, batch_size=256, num_workers=2):\n    # Train dataset fits scaler; Val shares it\n    train_ds = WindowedDataset(train_df, feature_cols, context_len=context_len, scaler=None)\n    val_ds   = WindowedDataset(val_df,   feature_cols, context_len=context_len, scaler=train_ds.scaler)\n    # Persist scaler for reuse\n    Path(\"models\").mkdir(exist_ok=True)\n    Path(\"models/scaler_split1.json\").write_text(json.dumps(train_ds.scaler.state_dict()))\n    pin = torch.cuda.is_available()\n    g = torch.Generator()\n    g.manual_seed(42)\n    def _seed_worker(_):\n        worker_seed = torch.initial_seed() % (2**32)\n        np.random.seed(worker_seed); random.seed(worker_seed)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0),\n                              worker_init_fn=_seed_worker, generator=g)\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False,\n                              num_workers=num_workers, pin_memory=pin, persistent_workers=(num_workers>0),\n                              worker_init_fn=_seed_worker)\n    return train_ds, val_ds, train_loader, val_loader\n\ntrain_ds, val_ds, train_loader, val_loader = make_loaders(train_df, val_df, FEATS, context_len=64, batch_size=256)\nlen(train_ds), len(val_ds), next(iter(train_loader))[0].shape\n```\n\n### 4) Define a tiny GRU regressor\n\n```python\nimport torch.nn as nn, torch\n\nclass GRURegressor(nn.Module):\n    def __init__(self, in_features: int, hidden: int = 64, num_layers: int = 2, dropout: float = 0.1):\n        super().__init__()\n        self.gru = nn.GRU(input_size=in_features, hidden_size=hidden, num_layers=num_layers,\n                          batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n        self.head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, 1)\n        )\n    def forward(self, x):        # x: (B, T, F)\n        _, hN = self.gru(x)      # hN: (num_layers, B, H); take last layer\n        h = hN[-1]               # (B, H)\n        return self.head(h).squeeze(-1)  # (B,)\n\ndef make_model():\n    return GRURegressor(in_features=len(FEATS), hidden=64, num_layers=2, dropout=0.1)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = make_model().to(device)\nsum(p.numel() for p in model.parameters())/1e6, device\n```\n\n### 5) Training loop with AMP, early stopping, checkpointing\n\n```python\nfrom torch.optim import AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nimport math, time\n\ndef mae_t(y_true, y_pred): return torch.mean(torch.abs(y_true - y_pred))\ndef smape_t(y_true, y_pred, eps=1e-8):\n    return torch.mean(2.0*torch.abs(y_true - y_pred)/(torch.abs(y_true)+torch.abs(y_pred)+eps))\n\ndef train_one_epoch(model, loader, optimizer, scaler, device, use_amp=True):\n    model.train(); total=0.0; n=0\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        optimizer.zero_grad(set_to_none=True)\n        if use_amp and device.type==\"cuda\":\n            with autocast(dtype=torch.float16):\n                pred = model(xb)\n                loss = mae_t(yb, pred)  # train with MAE (robust)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer); scaler.update()\n        else:\n            pred = model(xb); loss = mae_t(yb, pred)\n            loss.backward(); optimizer.step()\n        bs = xb.size(0); total += loss.item()*bs; n += bs\n    return total/max(n,1)\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval(); tot_mae=tot_smape=0.0; n=0\n    for xb, yb, _ in loader:\n        xb = xb.to(device, non_blocking=True).float()\n        yb = yb.to(device, non_blocking=True).float()\n        pred = model(xb)\n        bs = xb.size(0)\n        tot_mae   += mae_t(yb, pred).item()*bs\n        tot_smape += smape_t(yb, pred).item()*bs\n        n += bs\n    return {\"mae\": tot_mae/max(n,1), \"smape\": tot_smape/max(n,1)}\n\ndef fit(model, train_loader, val_loader, epochs=12, lr=1e-3, wd=1e-5, patience=3, use_amp=True):\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n    scaler = GradScaler(enabled=use_amp and (device.type==\"cuda\"))\n    best = math.inf; best_metrics=None; best_epoch=-1\n    ckpt_path = Path(\"models/gru_split1.pt\")\n    history=[]\n    for epoch in range(1, epochs+1):\n        t0=time.time()\n        tr_loss = train_one_epoch(model, train_loader, opt, scaler, device, use_amp)\n        val = evaluate(model, val_loader, device)\n        dt=time.time()-t0\n        history.append({\"epoch\":epoch,\"train_mae\":tr_loss,\"val_mae\":val[\"mae\"],\"val_smape\":val[\"smape\"],\"seconds\":dt})\n        print(f\"Epoch {epoch:02d}  train_mae={tr_loss:.5f}  val_mae={val['mae']:.5f}  val_sMAPE={val['smape']:.5f}  ({dt:.1f}s)\")\n        # early stopping on val mae\n        if val[\"mae\"] < best - 1e-6:\n            best = val[\"mae\"]; best_metrics=val; best_epoch=epoch\n            torch.save({\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": opt.state_dict(),\n                \"epoch\": epoch,\n                \"val\": val,\n                \"config\": {\"lr\":lr,\"wd\":wd,\"epochs\":epochs,\"context_len\":train_loader.dataset.T,\"feats\":FEATS}\n            }, ckpt_path)\n        elif epoch - best_epoch >= patience:\n            print(f\"Early stopping at epoch {epoch} (best {best:.5f} @ {best_epoch})\")\n            break\n    return history, best, best_epoch, ckpt_path\n\nhistory, best, best_epoch, ckpt_path = fit(model, train_loader, val_loader,\n                                           epochs=10, lr=1e-3, wd=1e-5, patience=3, use_amp=True)\nprint(\"Best val_mae:\", best, \"at epoch\", best_epoch, \"| saved:\", ckpt_path.exists())\n```\n\n### 6) Evaluate best checkpoint & write a small report\n\n```python\n# Reload best checkpoint and compute final validation metrics + save CSV\nckpt = torch.load(\"models/gru_split1.pt\", map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.to(device)\nfinal = evaluate(model, val_loader, device)\nimport pandas as pd\nrep = pd.DataFrame([{\n    \"split\": 1,\n    \"context_len\": train_loader.dataset.T,\n    \"feats\": \",\".join(FEATS),\n    \"val_mae\": final[\"mae\"],\n    \"val_smape\": final[\"smape\"],\n    \"best_epoch\": ckpt.get(\"epoch\", None),\n    \"params_M\": round(sum(p.numel() for p in model.parameters())/1e6, 3)\n}])\nrep.to_csv(\"reports/gru_split1_metrics.csv\", index=False)\nrep\n```\n\n> **Time check:** With \\~5–8 tickers, `T=64`, and 10 epochs, this should finish in a couple of minutes on Colab CPU; faster on GPU.\n\n---\n\n## Wrap‑up (10 min) — emphasize these points\n\n* **WindowedDataset** emits causal windows `(≤ t)` and targets `r_1d[t]` (i.e., `t+1` return).\n* Use **train‑fit scaler** and **reuse it** on validation to avoid leakage.\n* Keep the training loop **simple**: MAE training objective, AMP on CUDA, **early stopping** on validation MAE, and save a **checkpoint**.\n* Produce a **CSV** with validation metrics to track progress and compare future models.\n\n---\n\n## Homework (due before Session 20)\n\n**Goal:** Train a stronger **sequence baseline** (choose **LSTM** or **TCN**) on a subset (5–10 tickers). Log metrics and push your checkpoint + report.\n\n### Part A — Script `scripts/train_seq.py` (LSTM or TCN)\n\n```python\n#!/usr/bin/env python\nfrom __future__ import annotations\nimport argparse, json, math\nfrom pathlib import Path\nimport numpy as np, pandas as pd, torch, torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\n\n# --- (reuse minimal dataset/scaler from class; compact copy here) ---\nclass FeatureScaler:\n    def __init__(self): self.mean_=None; self.std_=None\n    def fit(self, X): self.mean_=X.mean(0); self.std_=X.std(0)+1e-8; return self\n    def transform(self, X): return (X-self.mean_)/self.std_\n    def state_dict(self): return {\"mean\": self.mean_.tolist(), \"std\": self.std_.tolist()}\n    def load_state_dict(self, d): import numpy as np; self.mean_=np.array(d[\"mean\"]); self.std_=np.array(d[\"std\"])\nclass WindowedDataset(Dataset):\n    def __init__(self, df, feats, T=64, scaler=None):\n        self.feats=feats; self.T=T; self.idx=[]; self.g={}\n        for tkr,g in df.groupby(\"ticker\"):\n            g=g.sort_values(\"date\").reset_index(drop=True)\n            X=g[feats].to_numpy(\"float32\"); y=g[\"r_1d\"].to_numpy(\"float32\")\n            for end in range(T-1,len(g)):\n                if np.isfinite(y[end]): self.idx.append((tkr,end))\n            self.g[tkr]={\"X\":X,\"y\":y}\n        self.scaler = scaler or FeatureScaler().fit(np.concatenate([self.g[t][\"X\"] for t in self.g],0))\n    def __len__(self): return len(self.idx)\n    def __getitem__(self,i):\n        tkr,end=self.idx[i]; g=self.g[tkr]\n        X=g[\"X\"][end-self.T+1:end+1]; X=self.scaler.transform(X)\n        y=g[\"y\"][end]\n        return torch.from_numpy(X), torch.tensor(y)\n\ndef make_splits(dates, train_min=252, val_size=63, step=63, embargo=5):\n    u=np.array(sorted(pd.to_datetime(pd.Series(dates).unique()))); i=train_min-1; out=[]\n    while True:\n        if i>=len(u): break\n        a,b=u[0],u[i]; vs=i+embargo+1; ve=vs+val_size-1\n        if ve>=len(u): break\n        out.append((a,b,u[vs],u[ve])); i+=step\n    return out\n\n# --- Models ---\nclass LSTMReg(nn.Module):\n    def __init__(self, in_f, hidden=64, layers=2, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(in_f, hidden, num_layers=layers, batch_first=True, dropout=dropout if layers>1 else 0.)\n        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout), nn.Linear(hidden,1))\n    def forward(self, x):\n        out,_ = self.lstm(x)\n        h = out[:,-1,:]\n        return self.head(h).squeeze(-1)\n\nclass TCNBlock(nn.Module):\n    def __init__(self, in_c, out_c, k=3, d=1, dropout=0.1):\n        super().__init__()\n        pad = (k-1)*d\n        self.net = nn.Sequential(\n            nn.Conv1d(in_c, out_c, k, padding=pad, dilation=d),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(out_c, out_c, k, padding=pad, dilation=d),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n        )\n        self.down = nn.Conv1d(in_c, out_c, 1) if in_c!=out_c else nn.Identity()\n    def forward(self, x):        # x: (B, F, T)\n        y = self.net(x)\n        # Causal crop to ensure output aligns with last time step\n        crop = y.shape[-1]-x.shape[-1]\n        if crop>0: y = y[..., :-crop]\n        return y + self.down(x)\n\nclass TCNReg(nn.Module):\n    def __init__(self, in_f, ch=64, blocks=3, k=3, dropout=0.1):\n        super().__init__()\n        layers=[]; c=in_f\n        for b in range(blocks):\n            layers.append(TCNBlock(c, ch, k=k, d=2**b, dropout=dropout)); c=ch\n        self.tcn = nn.Sequential(*layers)\n        self.head = nn.Sequential(nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(ch,1))\n    def forward(self, x):\n        # x: (B,T,F) -> (B,F,T) for Conv1d\n        x = x.transpose(1,2)\n        y = self.tcn(x)              # (B, C, T)\n        return self.head(y).squeeze(-1)\n\ndef mae_t(y,yhat): return torch.mean(torch.abs(y - yhat))\ndef smape_t(y,yhat,eps=1e-8): return torch.mean(2*torch.abs(y-yhat)/(torch.abs(y)+torch.abs(yhat)+eps))\n\ndef main():\n    ap=argparse.ArgumentParser()\n    ap.add_argument(\"--features\", default=\"data/processed/features_v1.parquet\")\n    ap.add_argument(\"--context\", type=int, default=64)\n    ap.add_argument(\"--model\", choices=[\"lstm\",\"tcn\"], default=\"lstm\")\n    ap.add_argument(\"--epochs\", type=int, default=12)\n    ap.add_argument(\"--batch\", type=int, default=256)\n    ap.add_argument(\"--lr\", type=float, default=1e-3)\n    ap.add_argument(\"--patience\", type=int, default=3)\n    ap.add_argument(\"--tickers\", type=int, default=8)\n    args=ap.parse_args()\n\n    df = pd.read_parquet(\"data/processed/features_v1_static.parquet\") if Path(\"data/processed/features_v1_static.parquet\").exists() else pd.read_parquet(args.features)\n    df = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    cand = [\"log_return\",\"lag1\",\"lag2\",\"lag3\",\"zscore_20\",\"roll_std_20\"]\n    feats = [c for c in cand if c in df.columns]\n    assert \"r_1d\" in df.columns\n    # subset tickers\n    keep = df[\"ticker\"].astype(str).unique().tolist()[:args.tickers]\n    df = df[df[\"ticker\"].astype(str).isin(keep)].copy()\n\n    splits = make_splits(df[\"date\"])\n    a,b,c,d = splits[0]\n    tr = df[(df[\"date\"]>=a)&(df[\"date\"]<=b)]\n    va = df[(df[\"date\"]>=c)&(df[\"date\"]<=d)]\n\n    train_ds = WindowedDataset(tr, feats, T=args.context, scaler=None)\n    val_ds   = WindowedDataset(va, feats, T=args.context, scaler=train_ds.scaler)\n\n    pin = torch.cuda.is_available()\n    g = torch.Generator(); g.manual_seed(42)\n    def _seed_worker(_): import numpy as np, random, torch; ws=torch.initial_seed()%2**32; np.random.seed(ws); random.seed(ws)\n    train_ld = DataLoader(train_ds, batch_size=args.batch, shuffle=True, drop_last=True,\n                          num_workers=2, pin_memory=pin, worker_init_fn=_seed_worker, generator=g)\n    val_ld   = DataLoader(val_ds, batch_size=args.batch, shuffle=False, drop_last=False,\n                          num_workers=2, pin_memory=pin, worker_init_fn=_seed_worker)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    net = (LSTMReg(len(feats)) if args.model==\"lstm\" else TCNReg(len(feats))).to(device)\n    opt = torch.optim.AdamW(net.parameters(), lr=args.lr, weight_decay=1e-5)\n    scaler = GradScaler(enabled=(device.type==\"cuda\"))\n    best = 1e9; best_epoch=0\n    ckpt = Path(f\"models/{args.model}_split1.pt\")\n\n    for epoch in range(1, args.epochs+1):\n        net.train(); tmae=0; n=0\n        for xb,yb in train_ld:\n            xb=xb.to(device).float(); yb=yb.to(device).float()\n            opt.zero_grad(set_to_none=True)\n            with autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n                yhat = net(xb)\n                loss = mae_t(yb, yhat)\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n            bs=xb.size(0); tmae += loss.item()*bs; n+=bs\n        tr_mae=tmae/n\n        # val\n        net.eval(); vmae=vsm=0; n=0\n        with torch.no_grad():\n            for xb,yb in val_ld:\n                xb=xb.to(device).float(); yb=yb.to(device).float()\n                yhat = net(xb)\n                bs=xb.size(0); vmae += mae_t(yb,yhat).item()*bs; vsm += smape_t(yb,yhat).item()*bs; n+=bs\n        vmae/=n; vsm/=n\n        print(f\"Epoch {epoch:02d}  tr_mae={tr_mae:.5f}  val_mae={vmae:.5f}  val_sMAPE={vsm:.5f}\")\n        if vmae < best-1e-6:\n            best=vmae; best_epoch=epoch\n            torch.save({\"model\": net.state_dict(), \"epoch\": epoch, \"feats\": feats, \"context\": args.context}, ckpt)\n        elif epoch - best_epoch >= args.patience:\n            print(\"Early stopping.\")\n            break\n\n    Path(\"reports\").mkdir(exist_ok=True)\n    pd.DataFrame([{\"model\":args.model,\"context\":args.context,\"val_mae\":best,\"best_epoch\":best_epoch,\"feats\":\",\".join(feats)}]).to_csv(\n        f\"reports/{args.model}_split1_metrics.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun (from repo root):\n\n```bash\n%%bash\nchmod +x scripts/train_seq.py\npython scripts/train_seq.py --model lstm --context 64 --tickers 8 --epochs 12\n```\n\n### Part B — Add a quick **Makefile** target and a tiny test\n\n**Append to `Makefile`:**\n\n```make\n.PHONY: train-lstm\ntrain-lstm: ## Train LSTM baseline on split 1 (subset of tickers)\n\\tpython scripts/train_seq.py --model lstm --context 64 --tickers 8 --epochs 12\n```\n\n**Basic shape test for dataset windows:**\n\n```python\n# tests/test_windowed_dataset.py\nimport pandas as pd, numpy as np, os\ndef test_window_shapes():\n    import scripts.train_seq as T\n    df = pd.read_parquet(\"data/processed/features_v1.parquet\").sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n    feats = [c for c in [\"log_return\",\"lag1\",\"lag2\",\"lag3\"] if c in df.columns]\n    splits = T.make_splits(df[\"date\"])\n    a,b,c,d = splits[0]\n    ds = T.WindowedDataset(df[(df[\"date\"]>=a)&(df[\"date\"]<=b)], feats, T=32, scaler=None)\n    X,y = ds[0]\n    assert X.shape == (32, len(feats))\n    assert np.isfinite(y.item())\n```\n\nRun:\n\n```bash\n%%bash\npytest -q -k windowed_dataset\n```\n\n### Part C — Report\n\nAdd to your Quarto report (e.g., `reports/eda.qmd`):\n\n````markdown\n## PyTorch Baselines\n\n::: {#d083c67a .cell execution_count=1}\n````` {.python .cell-code}\nimport pandas as pd\nprint(pd.read_csv(\"reports/gru_split1_metrics.csv\"))\ntry:\n    print(pd.read_csv(\"reports/lstm_split1_metrics.csv\"))\nexcept Exception as e:\n    print(\"lstm metrics not found yet\")\n````\n`````\n:::\n\n\n---\n\n## Instructor checklist (before class)\n- Confirm `features_v1.parquet` exists with `r_1d`. If not, ensure the fallback creates minimal lags and `r_1d`.  \n- Dry‑run the GRU training in a fresh Colab (~2–5 min).  \n- Prepare a slide on **why** we use **MAE** for training (robust to outliers) and **sMAPE** for reporting.\n\n## Emphasize while teaching\n- **Causality:** windows contain only information up to time `t`; the label is `t+1`.  \n- **No leakage:** scaler fit on **train only**; reuse for val/test.  \n- **Reproducibility:** seeds, deterministic flags, and saving checkpoints.  \n- **Efficiency:** pin memory, small batch size, AMP on CUDA.\n\n## Grading (pass/revise)\n- `WindowedDataset` works and yields `(B, T, F)` batches.  \n- Training loop runs with **early stopping** and writes `models/gru_split1.pt`.  \n- `reports/gru_split1_metrics.csv` exists with `val_mae` and `val_smape`.  \n- Homework script `scripts/train_seq.py` runs and writes its metrics CSV; optional test passes.\n\nYou now have a **clean PyTorch scaffold**: deterministic dataset windows, a minimal training loop with early stopping, and saved checkpoints—ready for **Session 20**, where you’ll build a **unified multi‑asset model** (ticker embeddings, mixed batches) on the same pipeline.\n```\n\n",
    "supporting": [
      "lec19_files"
    ],
    "filters": [],
    "includes": {}
  }
}