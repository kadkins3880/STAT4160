---
title: "Session 10"
---

Below is a complete lecture package for **Session 10 --- PyTorch Fundamentals for Time‑Series Forecasting** (75 minutes). It includes: a timed agenda, slide talking points, a **Colab‑friendly in‑class lab with copy‑paste code**, and **homework with copy‑paste code**. We'll build a small, fast **LSTM baseline** that predicts **next‑day log return** from a sliding window of past features---keeping everything **leakage‑free** and Colab‑friendly.

> **Educational use only --- not trading advice.** Assumes the same Drive‑mounted repo (e.g., `unified-stocks-teamX`) and a features file from Session 8: `data/processed/features_sql.parquet`. If missing, the lab includes a minimal fallback.

------------------------------------------------------------------------

## Session 10 --- PyTorch Fundamentals (75 min)

### Learning goals

By the end of class, students can:

1.  Use **tensors** on CPU/GPU and control **reproducibility** (seeds, deterministic flags).
2.  Build a **windowed Dataset** and **DataLoader** for multi‑ticker time series without leakage.
3.  Implement a minimal **training loop** with gradient clipping, **early stopping**, and checkpoint save/load.
4.  Train a compact **LSTM regressor** to beat trivial baselines on a small validation slice.

------------------------------------------------------------------------

## Agenda (75 min)

-   **(8 min)** Why sequence models; CPU/GPU, dtype, seeding
-   **(12 min)** Datasets/Dataloaders for sliding windows (no leakage); normalization from **train only**
-   **(35 min)** **In‑class lab**: prepare data → LSTM model → train/validate with early stopping → save predictions & checkpoint
-   **(10 min)** Wrap‑up + homework briefing
-   **(10 min)** Buffer for runtime hiccups

------------------------------------------------------------------------

## Slides / talking points (paste into your deck)

**Why PyTorch now**

-   We've built robust features & evaluation---time to learn a learnable model.
-   LSTM/TCN ≈ strong baselines for short‑horizon sequence forecasting with low compute.

**Reproducibility checklist**

-   Fix **random**, **NumPy**, **Torch** seeds; turn on deterministic flags (minor speed hit).
-   Always record: model config, context length, feature list, split dates.

**No‑leakage windowing**

-   Input window: features at **t−L+1 ... t** → **predict r(t+1)**.
-   Never let features at *t+1* enter inputs; **normalize using train stats only**.

**Training loop essentials**

-   `model.train()` / `model.eval()`; zero grad; forward; loss; backward; step.
-   Clip gradients (e.g., 1.0).
-   **Early stopping** on validation loss; save best `state_dict`.

------------------------------------------------------------------------

## In‑class lab (35 min)

> Run each block as its own Colab cell. Adjust `REPO_OWNER/REPO_NAME` first. We keep the model **small** for quick runs.

### 0) Mount Drive, set path, imports, and seeding

``` python
# Colab: setup
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

REPO_OWNER = "YOUR_GITHUB_USERNAME_OR_ORG"   # <- change
REPO_NAME  = "unified-stocks-teamX"          # <- change
BASE_DIR   = "/content/drive/MyDrive/dspt25"
REPO_DIR   = f"{BASE_DIR}/{REPO_NAME}"

import os, pathlib, math, time, random
import numpy as np
import pandas as pd
import torch
from torch import nn

pathlib.Path(BASE_DIR).mkdir(parents=True, exist_ok=True)
assert pathlib.Path(REPO_DIR).exists(), "Repo not found. Clone it first (Session 2/3)."
os.chdir(REPO_DIR)
print("Working dir:", os.getcwd())

def set_seed(seed=2025, deterministic=True):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        try:
            torch.use_deterministic_algorithms(True)
        except Exception:
            pass

set_seed(2025)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
```

### 1) Load features (or build a tiny fallback), choose feature set & split dates

``` python
# Load features built in Session 8 (SQL windows), or fallback from raw
from pathlib import Path

feats_path = Path("data/processed/features_sql.parquet")
raw_path   = Path("data/raw/prices.csv")

if feats_path.exists():
    df = pd.read_parquet(feats_path)
else:
    assert raw_path.exists(), "Missing data/raw/prices.csv and features_sql.parquet."
    raw = pd.read_csv(raw_path, parse_dates=["date"]).sort_values(["ticker","date"])
    # Minimal features (safe): lag returns + rolling std
    raw["r_1d"] = raw.groupby("ticker")["log_return"].shift(-1)  # label t+1
    raw["lag1"] = raw.groupby("ticker")["log_return"].shift(1)
    raw["lag2"] = raw.groupby("ticker")["log_return"].shift(2)
    raw["lag3"] = raw.groupby("ticker")["log_return"].shift(3)
    raw["roll_std_20"] = (raw.groupby("ticker")["log_return"]
                          .rolling(20, min_periods=10).std()
                          .reset_index(level=0, drop=True))
    df = raw[["ticker","date","r_1d","lag1","lag2","lag3","roll_std_20"]]

# Use a modest subset for speed (edit as desired)
keep_tickers = None  # e.g., ["AAPL","MSFT","NVDA","AMZN","GOOGL"]
if keep_tickers:
    df = df[df["ticker"].isin(keep_tickers)]

# Ensure ordering and cleanliness
df["date"] = pd.to_datetime(df["date"])
df = df.dropna().sort_values(["ticker","date"]).reset_index(drop=True)

# Feature columns for the sequence (inputs at time t)
FEATURES = [c for c in df.columns if c in ["r_1d","roll_mean_20","roll_std_20","zscore_20","lag1","lag2","lag3"]]
if "r_1d" not in FEATURES:
    FEATURES.insert(0, "r_1d")  # include past returns

print("Feature columns:", FEATURES)

# Define a simple contiguous split: last ~63 business days for validation
u_dates = np.array(sorted(df["date"].unique()))
val_days = 63
if len(u_dates) < val_days + 40:
    val_days = max(20, len(u_dates)//5)
train_end = u_dates[-val_days-1]
train_mask = df["date"] <= train_end
val_mask   = df["date"] >  train_end

print("Train end:", train_end.date(), "| Train rows:", train_mask.sum(), "Val rows:", val_mask.sum())
```

### 2) Windowed Dataset & DataLoader (no leakage), train‑only normalization

``` python
# Build a sliding-window dataset: X = [t-L+1 ... t] -> y = r_1d at (t+1)
from typing import List, Tuple, Optional

class WindowedTS(torch.utils.data.Dataset):
    def __init__(self, frame: pd.DataFrame, feature_cols: List[str], context: int = 32,
                 horizon: int = 1, norm_stats: Optional[Tuple[np.ndarray,np.ndarray]] = None):
        self.feature_cols = feature_cols
        self.context = int(context)
        self.horizon = int(horizon)
        # Build index of windows per ticker (prevent cross-ticker windows)
        self.rows = []
        for tkr, g in frame.groupby("ticker"):
            g = g.sort_values("date").reset_index(drop=True)
            # We need at least context + horizon rows
            T = len(g)
            for i in range(0, T - self.context - self.horizon + 1):
                j = i + self.context - 1
                y_idx = j + self.horizon
                # X uses rows [i..j], y is row y_idx (next day after j)
                self.rows.append((tkr, g.loc[i:j, feature_cols].values.astype("float32"),
                                  float(g.loc[y_idx, "r_1d"])))
        self.rows = np.array(self.rows, dtype=object)
        # normalization stats (mean/std) computed on TRAIN ONLY
        self.mu, self.sigma = (None, None)
        if norm_stats is not None:
            self.mu = np.asarray(norm_stats[0], dtype="float32")
            self.sigma = np.asarray(norm_stats[1], dtype="float32")

    def __len__(self): return len(self.rows)

    def __getitem__(self, idx: int):
        tkr, X, y = self.rows[idx]
        if self.mu is not None:
            X = (X - self.mu) / (self.sigma + 1e-8)
        return torch.from_numpy(X), torch.tensor([y], dtype=torch.float32), tkr

def compute_norm_stats(train_df: pd.DataFrame, feature_cols: List[str]):
    mu = train_df[feature_cols].mean().values.astype("float32")
    sg = train_df[feature_cols].std(ddof=0).replace(0, np.nan).fillna(1.0).values.astype("float32")
    return mu, sg

CONTEXT = 32
mu, sg = compute_norm_stats(df[train_mask], FEATURES)
train_ds = WindowedTS(df[train_mask], FEATURES, context=CONTEXT, horizon=1, norm_stats=(mu, sg))
val_ds   = WindowedTS(df[val_mask],   FEATURES, context=CONTEXT, horizon=1, norm_stats=(mu, sg))

BATCH = 256
train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True)
val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=BATCH, shuffle=False, drop_last=False)

len(train_ds), len(val_ds)
```

### 3) Define a compact LSTM regressor

``` python
class LSTMRegressor(nn.Module):
    def __init__(self, in_dim: int, hidden: int = 64, layers: int = 2, dropout: float = 0.1):
        super().__init__()
        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=hidden, num_layers=layers,
                            batch_first=True, dropout=(dropout if layers > 1 else 0.0))
        self.head = nn.Sequential(
            nn.Linear(hidden, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        # x: [B, T, F]
        out, _ = self.lstm(x)
        last = out[:, -1, :]           # last time step
        return self.head(last)         # [B, 1]

model = LSTMRegressor(in_dim=len(FEATURES), hidden=64, layers=2, dropout=0.1).to(device)
sum(p.numel() for p in model.parameters())/1e3, "K params"
```

### 4) Training loop with early stopping & gradient clipping

``` python
from torch.cuda.amp import autocast, GradScaler

def train_one_epoch(model, loader, opt, loss_fn, scaler=None, max_grad_norm=1.0):
    model.train()
    total = 0.0; n = 0
    for X, y, _ in loader:
        X = X.to(device); y = y.to(device)
        opt.zero_grad(set_to_none=True)
        if scaler:
            with autocast():
                yhat = model(X)
                loss = loss_fn(yhat, y)
            scaler.scale(loss).backward()
            if max_grad_norm:
                scaler.unscale_(opt); torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            scaler.step(opt); scaler.update()
        else:
            yhat = model(X)
            loss = loss_fn(yhat, y)
            loss.backward()
            if max_grad_norm:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            opt.step()
        total += float(loss.item()) * X.size(0); n += X.size(0)
    return total / max(n,1)

@torch.no_grad()
def eval_loss(model, loader, loss_fn):
    model.eval()
    total = 0.0; n = 0
    for X, y, _ in loader:
        X = X.to(device); y = y.to(device)
        yhat = model(X)
        loss = loss_fn(yhat, y)
        total += float(loss.item()) * X.size(0); n += X.size(0)
    return total / max(n,1)

def fit(model, train_loader, val_loader, epochs=15, lr=1e-3, patience=3, use_amp=True):
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.SmoothL1Loss()   # Huber loss: robust for returns
    scaler = GradScaler(enabled=(use_amp and device.type=="cuda"))
    best = {"epoch": -1, "val": math.inf, "state": None, "train": None}
    history = []
    for epoch in range(1, epochs+1):
        t0 = time.time()
        tr = train_one_epoch(model, train_loader, opt, loss_fn, scaler=scaler)
        va = eval_loss(model, val_loader, loss_fn)
        history.append({"epoch":epoch,"train_loss":tr,"val_loss":va})
        if va + 1e-8 < best["val"]:
            best.update({"epoch":epoch, "val":va, "state":{k:v.detach().cpu().clone() for k,v in model.state_dict().items()}, "train":tr})
        dur = time.time()-t0
        print(f"Epoch {epoch:02d} | train {tr:.6f} | val {va:.6f} | best {best['val']:.6f} (ep {best['epoch']}) | {dur:.1f}s")
        if epoch - best["epoch"] >= patience:
            print("Early stopping.")
            break
    # load best
    if best["state"] is not None:
        model.load_state_dict(best["state"])
    return history, best

history, best = fit(model, train_loader, val_loader, epochs=15, lr=1e-3, patience=3, use_amp=True)
history[:3], best["epoch"], best["val"]
```

### 5) Evaluate (MAE, sMAPE, directional accuracy), save predictions & checkpoint

``` python
from sklearn.metrics import mean_absolute_error

def smape(y_true, y_pred, eps=1e-8):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    return float(np.mean(2.0*np.abs(y_pred - y_true) / (np.abs(y_true)+np.abs(y_pred)+eps)))

def dir_acc(y_true, y_pred):
    yt = np.asarray(y_true); yp = np.asarray(y_pred)
    return float(np.mean(np.sign(yt) == np.sign(yp)))

@torch.no_grad()
def predict_loader(model, loader):
    model.eval()
    ys=[]; yh=[]; tkrs=[]; dts=[]
    for X, y, tkr in loader:
        X = X.to(device)
        yhat = model(X).cpu().numpy().reshape(-1)
        ys.append(y.numpy().reshape(-1))
        yh.append(yhat)
        tkrs += list(tkr)
        # We do not return dates here (window end + 1), but for reporting it's fine to omit
    y = np.concatenate(ys); yhat = np.concatenate(yh)
    return y, yhat, tkrs

y_val, yhat_val, tickers_val = predict_loader(model, val_loader)
metrics = {
    "val_mae": float(mean_absolute_error(y_val, yhat_val)),
    "val_smape": smape(y_val, yhat_val),
    "val_diracc": dir_acc(y_val, yhat_val),
    "n": int(len(y_val))
}
metrics
```

``` python
# Save artifacts: predictions, training curve, and best checkpoint
import json, pathlib
pathlib.Path("reports").mkdir(exist_ok=True)
pathlib.Path("models").mkdir(exist_ok=True)

pd.DataFrame(history).to_csv("reports/lstm_training_curve.csv", index=False)
pd.DataFrame({"y":y_val, "yhat":yhat_val, "ticker":tickers_val}).to_csv("reports/lstm_val_preds.csv", index=False)

ckpt = {
    "model":"LSTMRegressor",
    "features": FEATURES,
    "context": CONTEXT,
    "hidden": 64,
    "layers": 2,
    "dropout": 0.1,
    "norm_mu": mu.tolist(),
    "norm_sigma": sg.tolist(),
    "state_dict": {k:v.tolist() for k,v in model.state_dict().items()},
    "metrics": metrics,
    "train_end": str(train_end.date())
}
with open("models/lstm_baseline.json","w") as f:
    json.dump(ckpt, f)  # JSON for portability; small models only. (For larger, use torch.save with state_dict.)
print("Wrote reports/lstm_training_curve.csv, reports/lstm_val_preds.csv, models/lstm_baseline.json")
```

> **Note:** For larger models, prefer `torch.save({"state_dict": model.state_dict(), ...}, "models/lstm_baseline.pt")` and track small checkpoints with Git‑LFS.

------------------------------------------------------------------------

## Wrap‑up (10 min)

-   You built a **windowed** dataset (no leakage), trained a compact **LSTM**, and logged metrics & artifacts.
-   Keys to stability: **seeds**, **train‑only normalization**, **early stopping**, and **small, interpretable configs**.
-   Next time we'll compare classical baselines vs this LSTM in your **walk‑forward** protocol and extend toward **Transformer** models.

------------------------------------------------------------------------

## Homework (due before Session 11)

**Goal:** Productionize the baseline and explore one ablation. You will create (A) a reusable module for datasets/models, (B) a training script, (C) a Makefile target, (D) 2 tests, and (E) one ablation (context length).

### Part A --- Project modules

**`src/projectname/datasets.py`**

``` python
# src/projectname/datasets.py
from __future__ import annotations
import numpy as np, pandas as pd, torch
from typing import List, Optional, Tuple

class WindowedTS(torch.utils.data.Dataset):
    def __init__(self, frame: pd.DataFrame, feature_cols: List[str], context: int = 32,
                 horizon: int = 1, norm_stats: Optional[Tuple[np.ndarray,np.ndarray]] = None):
        self.feature_cols = feature_cols; self.context=int(context); self.horizon=int(horizon)
        self.rows = []
        for tkr, g in frame.groupby("ticker"):
            g = g.sort_values("date").reset_index(drop=True)
            T = len(g)
            for i in range(0, T - self.context - self.horizon + 1):
                j = i + self.context - 1
                y_idx = j + self.horizon
                X = g.loc[i:j, self.feature_cols].values.astype("float32")
                y = float(g.loc[y_idx, "r_1d"])
                self.rows.append((tkr, X, y))
        self.rows = np.array(self.rows, dtype=object)
        self.mu, self.sigma = (None, None)
        if norm_stats is not None:
            self.mu = np.asarray(norm_stats[0], dtype="float32")
            self.sigma = np.asarray(norm_stats[1], dtype="float32")

    def __len__(self): return len(self.rows)

    def __getitem__(self, idx: int):
        tkr, X, y = self.rows[idx]
        if self.mu is not None:
            X = (X - self.mu) / (self.sigma + 1e-8)
        return torch.from_numpy(X), torch.tensor([y], dtype=torch.float32), tkr

def compute_norm_stats(train_df: pd.DataFrame, feature_cols: List[str]):
    mu = train_df[feature_cols].mean().values.astype("float32")
    sg = train_df[feature_cols].std(ddof=0).replace(0, np.nan).fillna(1.0).values.astype("float32")
    return mu, sg
```

**`src/projectname/models/lstm.py`**

``` python
# src/projectname/models/lstm.py
from __future__ import annotations
import torch
from torch import nn

class LSTMRegressor(nn.Module):
    def __init__(self, in_dim: int, hidden: int = 64, layers: int = 2, dropout: float = 0.1):
        super().__init__()
        self.lstm = nn.LSTM(in_dim, hidden, num_layers=layers, batch_first=True,
                            dropout=(dropout if layers > 1 else 0.0))
        self.head = nn.Sequential(nn.Linear(hidden, 64), nn.ReLU(), nn.Linear(64, 1))

    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        return self.head(last)
```

### Part B --- Training script

**`scripts/train_lstm.py`**

``` python
#!/usr/bin/env python
import argparse, json, math, time, numpy as np, pandas as pd, torch
from torch import nn
from torch.cuda.amp import autocast, GradScaler
from pathlib import Path
from src.projectname.datasets import WindowedTS, compute_norm_stats
from src.projectname.models.lstm import LSTMRegressor

def set_seed(seed=2025, deterministic=True):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        try: torch.use_deterministic_algorithms(True)
        except Exception: pass

def smape(y_true, y_pred, eps=1e-8):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    return float(np.mean(2.0*np.abs(y_pred - y_true) / (np.abs(y_true)+np.abs(y_pred)+eps)))

def diracc(y_true, y_pred):
    yt = np.asarray(y_true); yp = np.asarray(y_pred)
    return float(np.mean(np.sign(yt) == np.sign(yp)))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--features", default="data/processed/features_sql.parquet")
    ap.add_argument("--features-cols", nargs="+", default=["r_1d","roll_mean_20","roll_std_20","zscore_20","lag1","lag2","lag3"])
    ap.add_argument("--context", type=int, default=32)
    ap.add_argument("--val-days", type=int, default=63)
    ap.add_argument("--batch", type=int, default=256)
    ap.add_argument("--epochs", type=int, default=15)
    ap.add_argument("--patience", type=int, default=3)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--hidden", type=int, default=64)
    ap.add_argument("--layers", type=int, default=2)
    ap.add_argument("--dropout", type=float, default=0.1)
    ap.add_argument("--out-metrics", default="reports/lstm_metrics.json")
    ap.add_argument("--out-preds", default="reports/lstm_val_preds.csv")
    ap.add_argument("--out-ckpt", default="models/lstm_baseline.pt")
    args = ap.parse_args()

    set_seed(2025)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    df = pd.read_parquet(args.features).dropna().sort_values(["ticker","date"]).reset_index(drop=True)
    df["date"] = pd.to_datetime(df["date"])
    u = np.array(sorted(df["date"].unique()))
    val_days = min(args.val_days, max(20, len(u)//5))
    train_end = u[-val_days-1]
    tr, va = df[df["date"] <= train_end], df[df["date"] > train_end]

    feats = [c for c in args.features_cols if c in df.columns]
    mu, sg = compute_norm_stats(tr, feats)
    tr_ds = WindowedTS(tr, feats, context=args.context, horizon=1, norm_stats=(mu, sg))
    va_ds = WindowedTS(va, feats, context=args.context, horizon=1, norm_stats=(mu, sg))
    tr_ld = torch.utils.data.DataLoader(tr_ds, batch_size=args.batch, shuffle=True, drop_last=True)
    va_ld = torch.utils.data.DataLoader(va_ds, batch_size=args.batch, shuffle=False)

    model = LSTMRegressor(in_dim=len(feats), hidden=args.hidden, layers=args.layers, dropout=args.dropout).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=args.lr)
    loss_fn = nn.SmoothL1Loss()
    scaler = GradScaler(enabled=(device.type=="cuda"))

    def epoch_pass(loader, train=True):
        total=0; n=0
        if train: model.train()
        else: model.eval()
        for X, y, _ in loader:
            X=X.to(device); y=y.to(device)
            if train:
                opt.zero_grad(set_to_none=True)
                with autocast(enabled=(device.type=="cuda")):
                    yhat = model(X); loss = loss_fn(yhat, y)
                scaler.scale(loss).backward()
                scaler.unscale_(opt); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                scaler.step(opt); scaler.update()
            else:
                with torch.no_grad():
                    yhat = model(X); loss = loss_fn(yhat, y)
            total += float(loss.item())*X.size(0); n += X.size(0)
        return total/max(n,1)

    best = {"epoch":-1, "val":math.inf, "state":None}
    hist=[]
    for ep in range(1, args.epochs+1):
        tr_loss = epoch_pass(tr_ld, train=True)
        va_loss = epoch_pass(va_ld, train=False)
        hist.append({"epoch":ep, "train_loss":tr_loss, "val_loss":va_loss})
        if va_loss < best["val"]-1e-12:
            best={"epoch":ep,"val":va_loss,"state":{k:v.detach().cpu() for k,v in model.state_dict().items()}}
        print(f"Epoch {ep:02d} | train {tr_loss:.6f} | val {va_loss:.6f} | best {best['val']:.6f} (ep {best['epoch']})")
        if ep - best["epoch"] >= args.patience:
            print("Early stopping.")
            break

    if best["state"] is not None:
        model.load_state_dict(best["state"])

    # Evaluate and save
    model.eval(); yh=[]; y=[]; tkr=[]
    with torch.no_grad():
        for X, yy, tk in va_ld:
            y.append(yy.numpy().reshape(-1))
            yh.append(model(X.to(device)).cpu().numpy().reshape(-1))
            tkr += list(tk)
    y = np.concatenate(y); yh = np.concatenate(yh)

    metrics = {
        "val_mae": float(np.mean(np.abs(y - yh))),
        "val_smape": smape(y, yh),
        "val_diracc": diracc(y, yh),
        "n": int(len(y)),
        "best_epoch": int(best["epoch"]),
        "best_val_loss": float(best["val"]),
        "train_end": str(train_end)
    }

    Path("reports").mkdir(exist_ok=True); Path("models").mkdir(exist_ok=True)
    pd.DataFrame({"y":y, "yhat":yh, "ticker":tkr}).to_csv(args.out_preds, index=False)
    with open(args.out_metrics, "w") as f: json.dump(metrics, f, indent=2)
    torch.save({"config":vars(args), "state_dict":model.state_dict(), "mu":mu, "sigma":sg}, args.out_ckpt)
    print("Saved:", args.out_preds, args.out_metrics, args.out_ckpt)

if __name__ == "__main__":
    main()
```

Make executable:

``` python
import os, stat, pathlib
p = pathlib.Path("scripts/train_lstm.py")
os.chmod(p, os.stat(p).st_mode | stat.S_IEXEC)
print("Ready:", p)
```

### Part C --- Makefile target

Append to your `Makefile`:

``` make
.PHONY: train-lstm
train-lstm: ## Train LSTM baseline and save metrics/preds/checkpoint
\tpython scripts/train_lstm.py --features data/processed/features_sql.parquet --context 32 --val-days 63 --epochs 12 --patience 3 --batch 256
```

Run:

``` bash
%%bash
set -euo pipefail
cd "/content/drive/MyDrive/dspt25/unified-stocks-teamX"
make train-lstm
head -n 5 reports/lstm_val_preds.csv
cat reports/lstm_metrics.json
```

### Part D --- Two lightweight tests

**`tests/test_windowed_ds.py`**

``` python
import pandas as pd, numpy as np
from src.projectname.datasets import WindowedTS, compute_norm_stats

def test_window_alignment():
    df = pd.DataFrame({
        "ticker":["A"]*8,
        "date":pd.date_range("2020-01-01", periods=8, freq="B"),
        "r_1d":np.arange(8)*0.1,
        "lag1":np.arange(8)*0.1,
    })
    mu, sg = compute_norm_stats(df.iloc[:5], ["r_1d","lag1"])
    ds = WindowedTS(df, ["r_1d","lag1"], context=3, horizon=1, norm_stats=(mu,sg))
    X,y,_ = ds[0]
    # Input spans rows 0..2; target is row 3
    assert X.shape == (3,2)
    assert abs(y.item() - 0.3) < 1e-9
```

**`tests/test_lstm_forward.py`**

``` python
import torch
from src.projectname.models.lstm import LSTMRegressor

def test_forward_shape():
    model = LSTMRegressor(in_dim=4, hidden=8, layers=2, dropout=0.0)
    x = torch.randn(5, 32, 4)  # B,T,F
    y = model(x)
    assert y.shape == (5,1)
```

Run:

``` bash
%%bash
set -euo pipefail
cd "/content/drive/MyDrive/dspt25/unified-stocks-teamX"
pytest -q
```

### Part E --- One ablation: context length

-   Train with `--context 16`, `--context 32`, and `--context 64`.
-   Save metrics to `reports/lstm_ablation.csv` with columns: `context, val_mae, val_smape, val_diracc`.

**Template snippet:**

``` python
import subprocess, json, pandas as pd, pathlib, os
pathlib.Path("reports").mkdir(exist_ok=True)
rows=[]
for ctx in [16,32,64]:
    subprocess.run(["python","scripts/train_lstm.py","--context",str(ctx)], check=True)
    m = json.load(open("reports/lstm_metrics.json"))
    rows.append({"context":ctx, **m})
pd.DataFrame(rows).to_csv("reports/lstm_ablation.csv", index=False)
print("Wrote reports/lstm_ablation.csv")
```

### Grading (pass/revise)

-   `scripts/train_lstm.py` runs; creates `reports/lstm_val_preds.csv`, `reports/lstm_metrics.json`, and `models/lstm_baseline.pt` (or `.json`).
-   `tests/` pass (`pytest` green).
-   `Makefile` has `train-lstm`.
-   **Ablation** table present with 3 contexts.

------------------------------------------------------------------------

## Instructor checklist (before class)

-   Dry‑run the lab on a fresh Colab runtime; ensure training finishes in \<10 minutes with the suggested settings.
-   Be ready to point out **where leakage could sneak in** (normalization, window/label alignment).
-   Have a slide showing tensor shapes: `[B, T, F] → LSTM → [B, 1]`.

## Emphasize while teaching

-   **Windowing** and **normalization-from-train** are the two biggest gotchas.
-   Keep models **small and fast**; measure improvements against naive baselines.
-   Save enough metadata (features, context, split dates) to make results **repeatable**.

Next time (Session 11): we'll implement **attention (tiny GPT)** on a toy task, then adapt it to our time‑series windows.
